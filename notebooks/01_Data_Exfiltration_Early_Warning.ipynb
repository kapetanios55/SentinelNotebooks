{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846a3219",
   "metadata": {},
   "source": [
    "# Data Staging & Exfiltration Early Warning\n",
    "\n",
    "This notebook surfaces early indicators of data staging and exfiltration by correlating endpoint, network, and storage telemetry from Microsoft Sentinel Data Lake.\n",
    "\n",
    "## Why it matters\n",
    "- Focuses on the *exfiltration* phase of the cyber kill chain, complementing identity and threat-hunting notebooks.\n",
    "- Helps SOC analysts validate whether suspicious file staging is progressing toward actual data loss.\n",
    "- Provides repeatable detections you can operationalize into scheduled jobs or Sentinel analytics rules.\n",
    "\n",
    "ℹ️ **Run the setup notebook first** to populate workspace names and attach to your Spark pool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8f8e8",
   "metadata": {},
   "source": [
    "## Detection focus\n",
    "- **Local staging**: creation of archive files or bulk copies into staging folders.\n",
    "- **Cloud/object uploads**: command-line tools pushing data to blob/object storage.\n",
    "- **Outbound spikes**: anomalous egress bursts toward rare destinations.\n",
    "- **Audit corroboration**: storage key/list operations that often precede exfiltration.\n",
    "\n",
    "| Table | Signal | Why it matters |\n",
    "| --- | --- | --- |\n",
    "| `DeviceFileEvents` | File writes, archive creation | Identify staging of data sets |\n",
    "| `DeviceProcessEvents` | Command-line telemetry | Catch compression tools and storage CLIs |\n",
    "| `DeviceNetworkEvents` | Network flows | Measure outbound volume and destinations |\n",
    "| `StorageBlobLogs` / `BlobServiceLogs` | Storage access | Confirm uploads to cloud storage |\n",
    "| `AuditLogs` | Azure activity trail | Highlight storage key/permission operations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and analyst configuration\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "from pyspark.sql.functions import (\n",
    "    col, lower, regexp_extract, sum as spark_sum, countDistinct,\n",
    "    lit, date_trunc\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "ANALYSIS_HOURS = 24\n",
    "PRIMARY_WORKSPACE = \"ak-SecOps\"  # replace with your workspace\n",
    "STORAGE_WORKSPACE = PRIMARY_WORKSPACE  # override if storage logs sit elsewhere\n",
    "\n",
    "WORKSPACE_MAPPING = {\n",
    "    'DeviceFileEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceProcessEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceNetworkEvents': PRIMARY_WORKSPACE,\n",
    "    'AuditLogs': PRIMARY_WORKSPACE,\n",
    "    'StorageBlobLogs': STORAGE_WORKSPACE,\n",
    "    'BlobServiceLogs': STORAGE_WORKSPACE\n",
    "}\n",
    "\n",
    "UTC_NOW = datetime.utcnow()\n",
    "START_TIME = UTC_NOW - timedelta(hours=ANALYSIS_HOURS)\n",
    "print(f\"Analysis window (UTC): {START_TIME} → {UTC_NOW}\")\n",
    "for table, workspace in WORKSPACE_MAPPING.items():\n",
    "    print(f\"• {table} → {workspace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper utilities\n",
    "def try_load(table_name: str, timestamp_col: str | None = None, workspace: str | None = None):\n",
    "    workspace = workspace or WORKSPACE_MAPPING.get(table_name, PRIMARY_WORKSPACE)\n",
    "    try:\n",
    "        df = data_provider.read_table(table_name, workspace)\n",
    "        if timestamp_col and timestamp_col in df.columns:\n",
    "            df = df.filter((col(timestamp_col) >= lit(START_TIME)) & (col(timestamp_col) <= lit(UTC_NOW)))\n",
    "        return df\n",
    "    except Exception as exc:\n",
    "        print(f\"⚠️ {table_name} not available: {exc}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def preview(df, label: str, sample: int = 5):\n",
    "    if df is None:\n",
    "        print(f\"⚠️ {label}: table unavailable\")\n",
    "        return\n",
    "    count = df.count()\n",
    "    print(f\"✅ {label}: {count} rows in window\")\n",
    "    sample_columns = df.columns[: min(len(df.columns), 8)]\n",
    "    if sample_columns:\n",
    "        print(\"   columns:\", \", \".join(sample_columns))\n",
    "\n",
    "\n",
    "device_files = try_load('DeviceFileEvents', 'Timestamp')\n",
    "device_processes = try_load('DeviceProcessEvents', 'Timestamp')\n",
    "device_network = try_load('DeviceNetworkEvents', 'Timestamp')\n",
    "storage_logs = try_load('StorageBlobLogs', 'TimeGenerated') or try_load('BlobServiceLogs', 'TimeGenerated')\n",
    "audit_logs = try_load('AuditLogs', 'TimeGenerated')\n",
    "\n",
    "preview(device_files, 'DeviceFileEvents')\n",
    "preview(device_processes, 'DeviceProcessEvents')\n",
    "preview(device_network, 'DeviceNetworkEvents')\n",
    "preview(storage_logs, 'Storage blob logs')\n",
    "preview(audit_logs, 'AuditLogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection 1 – Compression / staging behavior\n",
    "ALLOWED_COMMAND_PATTERNS = [\n",
    "    # Add fully qualified command lines or fragments for sanctioned compression jobs\n",
    "    r\".*\\\\backup_agent\\\\.*zip\",\n",
    "    r\"/usr/bin/tar .* /var/backups\"\n",
    "]\n",
    "ALLOWED_PATH_PATTERNS = [\n",
    "    # Add trusted staging directories used by backup tooling\n",
    "    r\".*\\\\CompanyBackup\\\\\",\n",
    "    r\"/var/backups/.*\"\n",
    "]\n",
    "ALLOWED_ACCOUNTS = {\"backup-svc\"}\n",
    "ALLOWED_DEVICES = {\"backup-server01\"}\n",
    "\n",
    "\n",
    "def _compile_or_pattern(patterns: list[str]) -> str:\n",
    "    return \"|\".join([f\"({p.lower()})\" for p in patterns if p])\n",
    "\n",
    "\n",
    "def detect_compression_activity(file_df, process_df):\n",
    "    if file_df is None or process_df is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    compression_procs = process_df.filter(\n",
    "        lower(col('ProcessCommandLine')).rlike('compress|zip|7z|rar|archive|tar')\n",
    "    )\n",
    "\n",
    "    command_allowlist = _compile_or_pattern(ALLOWED_COMMAND_PATTERNS)\n",
    "    if command_allowlist:\n",
    "        compression_procs = compression_procs.filter(~lower(col('ProcessCommandLine')).rlike(command_allowlist))\n",
    "\n",
    "    if ALLOWED_ACCOUNTS and 'InitiatingProcessAccountName' in compression_procs.columns:\n",
    "        allowed_accounts = [acct.lower() for acct in ALLOWED_ACCOUNTS]\n",
    "        compression_procs = compression_procs.filter(~lower(col('InitiatingProcessAccountName')).isin(allowed_accounts))\n",
    "\n",
    "    if ALLOWED_DEVICES and 'DeviceName' in compression_procs.columns:\n",
    "        allowed_devices = [device.lower() for device in ALLOWED_DEVICES]\n",
    "        compression_procs = compression_procs.filter(~lower(col('DeviceName')).isin(allowed_devices))\n",
    "\n",
    "    if compression_procs.rdd.isEmpty():\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    staging_candidates = file_df.filter(\n",
    "        lower(col('FolderPath')).rlike('temp|\\\\\\\\staging|\\\\\\\\backup') &\n",
    "        lower(col('FileName')).rlike('\\\\\\\\.(zip|7z|rar|tar|gz|bz2)$')\n",
    "    )\n",
    "\n",
    "    path_allowlist = _compile_or_pattern(ALLOWED_PATH_PATTERNS)\n",
    "    if path_allowlist:\n",
    "        staging_candidates = staging_candidates.filter(~lower(col('FolderPath')).rlike(path_allowlist))\n",
    "\n",
    "    staging_files = staging_candidates.select(\n",
    "        'DeviceName',\n",
    "        col('FileName').alias('StagedFileName'),\n",
    "        col('FolderPath').alias('StagedFolderPath'),\n",
    "        col('Timestamp').alias('StagedFileTimestamp')\n",
    "    )\n",
    "\n",
    "    joined = compression_procs.join(\n",
    "        staging_files,\n",
    "        on='DeviceName',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    selected_cols = [\n",
    "        'DeviceName',\n",
    "        'InitiatingProcessAccountName',\n",
    "        'ProcessCommandLine',\n",
    "        'Timestamp'\n",
    "    ]\n",
    "    if 'InitiatingProcessParentFileName' in compression_procs.columns:\n",
    "        selected_cols.append('InitiatingProcessParentFileName')\n",
    "\n",
    "    pandas_df = joined.select(*selected_cols,\n",
    "                              'StagedFileName',\n",
    "                              'StagedFolderPath',\n",
    "                              'StagedFileTimestamp').toPandas()\n",
    "\n",
    "    return pandas_df\n",
    "\n",
    "compression_findings = detect_compression_activity(device_files, device_processes)\n",
    "compression_findings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection 2 – Cloud storage uploads via CLI tools\n",
    "def detect_cli_storage_uploads(process_df, network_df):\n",
    "    if process_df is None or network_df is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cli_exec = process_df.filter(\n",
    "        lower(col('ProcessCommandLine')).rlike('azcopy|az storage|aws s3|gsutil|rclone|scp')\n",
    "    )\n",
    "    if cli_exec.count() == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    storage_net = network_df.filter(\n",
    "        lower(col('RemoteUrl')).rlike('blob.core.windows.net|amazonaws.com|storage.googleapis.com|digitaloceanspaces.com')\n",
    "    )\n",
    "\n",
    "    if 'InitiatingProcessId' in cli_exec.columns and 'InitiatingProcessId' in storage_net.columns:\n",
    "        joined = cli_exec.join(storage_net, ['DeviceName', 'InitiatingProcessId'], 'inner')\n",
    "    else:\n",
    "        joined = cli_exec.join(storage_net, ['DeviceName'], 'inner')\n",
    "\n",
    "    cols = [c for c in ['Timestamp', 'DeviceName', 'InitiatingProcessAccountName', 'ProcessCommandLine', 'RemoteUrl', 'BytesSent'] if c in joined.columns]\n",
    "    return joined.select(*cols).toPandas()\n",
    "\n",
    "cli_uploads = detect_cli_storage_uploads(device_processes, device_network)\n",
    "cli_uploads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection 3 – Anomalous egress spikes\n",
    "def detect_egress_spikes(network_df):\n",
    "    if network_df is None or 'BytesSent' not in network_df.columns:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    hourly = network_df.groupBy('DeviceName', date_trunc('hour', col('Timestamp')).alias('Hour')).agg(\n",
    "        spark_sum('BytesSent').alias('BytesSent')\n",
    "    )\n",
    "\n",
    "    window = Window.partitionBy('DeviceName').orderBy('Hour').rowsBetween(-23, 0)\n",
    "    enriched = hourly.withColumn('RollingAvg', spark_sum('BytesSent').over(window) / lit(24))\n",
    "    suspicious = enriched.filter(col('BytesSent') > col('RollingAvg') * lit(3))\n",
    "\n",
    "    return suspicious.orderBy(col('BytesSent').desc()).toPandas()\n",
    "\n",
    "egress_spikes = detect_egress_spikes(device_network)\n",
    "egress_spikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection 4 – Storage account audit anomalies\n",
    "def detect_storage_audit_events(audit_df):\n",
    "    if audit_df is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    storage_ops = audit_df.filter(\n",
    "        lower(col('OperationName')).rlike('storageaccounts|accesskeys|listkeys|setserviceproperties')\n",
    "    )\n",
    "\n",
    "    cols = [c for c in ['TimeGenerated', 'OperationName', 'ResultDescription', 'InitiatedBy', 'TargetResources'] if c in storage_ops.columns]\n",
    "    return storage_ops.select(*cols).toPandas()\n",
    "\n",
    "storage_audit_events = detect_storage_audit_events(audit_logs)\n",
    "storage_audit_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae8ab0",
   "metadata": {},
   "source": [
    "## Triage summary\n",
    "Correlate findings to decide whether to escalate. Tune thresholds and allow-lists for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ecbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and score findings\n",
    "def summarize_findings(**signals):\n",
    "    rows = []\n",
    "    score = 0\n",
    "    for name, df in signals.items():\n",
    "        if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "            rows.append({'signal': name, 'rows': len(df)})\n",
    "            score += min(len(df), 10)\n",
    "    summary = pd.DataFrame(rows)\n",
    "    if score >= 20:\n",
    "        level = 'HIGH'\n",
    "    elif score >= 10:\n",
    "        level = 'MEDIUM'\n",
    "    else:\n",
    "        level = 'LOW' if score > 0 else 'NONE'\n",
    "    return summary, score, level\n",
    "\n",
    "summary_table, risk_score, risk_level = summarize_findings(\n",
    "    compression=compression_findings,\n",
    "    cli_uploads=cli_uploads,\n",
    "    egress_spikes=egress_spikes,\n",
    "    storage_audit=storage_audit_events\n",
    ")\n",
    "\n",
    "print(f\"Risk score: {risk_score} ({risk_level})\")\n",
    "if not summary_table.empty:\n",
    "    display(summary_table)\n",
    "else:\n",
    "    print('No detections in current window.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d5043",
   "metadata": {},
   "source": [
    "## Next steps for defenders\n",
    "- Validate compression hosts with scheduled backup windows.\n",
    "- Investigate CLI uploads for credential or tool misuse.\n",
    "- Cross-reference egress hosts with identity anomalies or incident queues.\n",
    "- Convert high-confidence detections into Sentinel analytics rules or scheduled notebooks.\n",
    "- Iterate on thresholds and allow-lists after each hunting cycle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [01_Data_Exfiltration_Early_Warning]",
   "language": "Python",
   "name": "MSGMedium"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
