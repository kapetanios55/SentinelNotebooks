{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce72287a",
   "metadata": {},
   "source": [
    "# üß™ Microsoft Sentinel Data Lake Notebook Examples\n",
    "\n",
    "Practical walkthrough covering each scenario from the [Microsoft Sentinel data lake notebook examples](https://learn.microsoft.com/azure/sentinel/datalake/notebook-examples).\n",
    "\n",
    "## üéØ Scenarios covered\n",
    "- Explore Microsoft Entra ID groups\n",
    "- Filter sign-ins for a specific user\n",
    "- Examine sign-in location details\n",
    "- Detect sign-ins from unusual countries\n",
    "- Spot brute-force patterns\n",
    "- Identify lateral movement attempts\n",
    "- Surface credential dumping indicators\n",
    "\n",
    "Each section reuses the zero-config loader to keep the workflow consistent with the rest of this workbook collection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd656f",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start\n",
    "1. Attach the Sentinel kernel (medium pool recommended)\n",
    "2. Update any configuration variables marked with \"CHANGE_ME\"\n",
    "   - Set `PRIMARY_WORKSPACE` if you want to bypass system-table discovery\n",
    "   - Flip `AUTO_DISCOVER` to `False` if every table lives in that workspace\n",
    "3. Run cells in order ‚Äì each scenario will gracefully handle missing tables\n",
    "\n",
    "‚öôÔ∏è **Notebook defaults:** No manual workspace mapping required ‚Äì the helper functions auto-discover available workspaces just like the other notebooks in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9fec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Notebook parameters\n",
    "TARGET_USER_UPN = \"<add your UPN user here>\"\n",
    "print(f\"TARGET_USER_UPN set to: {TARGET_USER_UPN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports & zero-config setup\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "from pyspark.sql.functions import (\n",
    "    col, count as spark_count, countDistinct, expr, when,\n",
    "    lower, upper, lit, date_trunc, hour, to_date,\n",
    "    from_json, schema_of_json\n",
    ")\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, MapType, StringType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "print('‚úÖ Environment initialized (auto workspace detection enabled)')\n",
    "\n",
    "PRIMARY_WORKSPACE = 'ak-SecOps'  # Set to None to let auto-discovery handle everything\n",
    "AUTO_DISCOVER = True  # Set to False to skip probing System Tables\n",
    "FALLBACK_WORKSPACES = ['default']  # Additional workspaces to probe if needed\n",
    "ANALYSIS_HOURS = 72\n",
    "\n",
    "def try_read(table_name: str, workspace: str | None):\n",
    "    if workspace:\n",
    "        return data_provider.read_table(table_name, workspace)\n",
    "    return data_provider.read_table(table_name)\n",
    "\n",
    "def smart_load(table_name: str):\n",
    "    last_error = None\n",
    "\n",
    "    def attempt(workspace: str | None):\n",
    "        nonlocal last_error\n",
    "        try:\n",
    "            df = try_read(table_name, workspace)\n",
    "            return df, (workspace or 'auto'), None\n",
    "        except Exception as e:\n",
    "            last_error = str(e)\n",
    "            return None\n",
    "\n",
    "    if PRIMARY_WORKSPACE:\n",
    "        result = attempt(PRIMARY_WORKSPACE)\n",
    "        if result:\n",
    "            return result\n",
    "\n",
    "    if AUTO_DISCOVER:\n",
    "        result = attempt(None)\n",
    "        if result:\n",
    "            return result\n",
    "\n",
    "    for ws in FALLBACK_WORKSPACES:\n",
    "        if PRIMARY_WORKSPACE and ws == PRIMARY_WORKSPACE:\n",
    "            continue\n",
    "        result = attempt(ws)\n",
    "        if result:\n",
    "            return result\n",
    "\n",
    "    return None, None, last_error\n",
    "\n",
    "def summarize_table(name: str, df: DataFrame | None, workspace: str | None, err: str | None):\n",
    "    if df is None:\n",
    "        print(f'‚ùå {name} not available: {err}')\n",
    "        return False\n",
    "    rows = df.count()\n",
    "    print(f'‚úÖ {name} loaded from workspace={workspace} ({rows:,} rows)')\n",
    "    return rows > 0\n",
    "\n",
    "print('üéØ Ready to explore all notebook scenarios')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3faee",
   "metadata": {},
   "source": [
    "## 1. Microsoft Entra ID Group Inventory\n",
    "Discover group metadata (display name, types, descriptions) directly from the data lake tier (EntraGroups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3704ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "entra_groups, entra_workspace, entra_err = smart_load('EntraGroups')\n",
    "if summarize_table('EntraGroups', entra_groups, entra_workspace, entra_err):\n",
    "    sample_cols = ['displayName', 'groupTypes', 'mail', 'mailNickname', 'description', 'tenantId']\n",
    "    available_cols = [c for c in sample_cols if c in entra_groups.columns]\n",
    "    if available_cols:\n",
    "        entra_groups.select(*available_cols).show(25, truncate=False)\n",
    "    else:\n",
    "        print('‚ö†Ô∏è Expected columns not found in EntraGroups table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b8022",
   "metadata": {},
   "source": [
    "## 2. Sign-ins for a specific user\n",
    "Start with an overview of recent sign-ins, then drill into a specific account when you assign `TARGET_USER_UPN` (for example, by running `TARGET_USER_UPN = \"alice@contoso.com\"` in a cell above this one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35499dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_USER_UPN = globals().get('TARGET_USER_UPN')\n",
    "if TARGET_USER_UPN is not None:\n",
    "    TARGET_USER_UPN = str(TARGET_USER_UPN).strip() or None\n",
    "\n",
    "signin_df, signin_workspace, signin_err = smart_load('SigninLogs')\n",
    "\n",
    "if summarize_table('SigninLogs', signin_df, signin_workspace, signin_err):\n",
    "    if TARGET_USER_UPN:\n",
    "        focused = signin_df.filter(lower(col('UserPrincipalName')) == lower(lit(TARGET_USER_UPN)))\n",
    "        total = focused.count()\n",
    "        print(f'üìä Sign-ins for {TARGET_USER_UPN}: {total:,}')\n",
    "        if total > 0:\n",
    "            display_cols = ['TimeGenerated', 'ResultType', 'AppDisplayName', 'IpAddress', 'Location']\n",
    "            available = [c for c in display_cols if c in focused.columns]\n",
    "            focused.orderBy(col('TimeGenerated').desc()).select(*available).show(20, truncate=False)\n",
    "        else:\n",
    "            print('‚ÑπÔ∏è No sign-in records found for the specified user in the current retention window')\n",
    "    else:\n",
    "        if 'UserPrincipalName' not in signin_df.columns:\n",
    "            print('‚ö†Ô∏è UserPrincipalName column missing ‚Äì unable to summarise sign-ins by user')\n",
    "        else:\n",
    "            print('üìå Showing top sign-in activity. Set TARGET_USER_UPN = \"user@domain\" and rerun to drill into a specific account.')\n",
    "            user_counts = (\n",
    "                signin_df\n",
    "                .groupBy('UserPrincipalName')\n",
    "                .agg(spark_count('*').alias('SignIns'), countDistinct('AppDisplayName').alias('DistinctApps'))\n",
    "                .orderBy(col('SignIns').desc())\n",
    "                .limit(15)\n",
    "            )\n",
    "            user_counts.show(truncate=False)\n",
    "            if 'AppDisplayName' in signin_df.columns:\n",
    "                app_counts = (\n",
    "                    signin_df\n",
    "                    .groupBy('AppDisplayName')\n",
    "                    .agg(spark_count('*').alias('SignIns'))\n",
    "                    .orderBy(col('SignIns').desc())\n",
    "                    .limit(10)\n",
    "                )\n",
    "                print('\\nüéØ Top applications by sign-in volume:')\n",
    "                app_counts.show(truncate=False)\n",
    "            if 'ResultType' in signin_df.columns:\n",
    "                result_counts = (\n",
    "                    signin_df\n",
    "                    .groupBy('ResultType')\n",
    "                    .agg(spark_count('*').alias('Attempts'))\n",
    "                    .orderBy(col('Attempts').desc())\n",
    "                )\n",
    "                print('\\nüßæ Result type breakdown:')\n",
    "                result_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef225605",
   "metadata": {},
   "source": [
    "## 3. Sign-in location details\n",
    "Parse the LocationDetails JSON blob to extract city, state, and country information for visualization or enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ecfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'signin_df' not in globals():\n",
    "    signin_df, signin_workspace, signin_err = smart_load('SigninLogs')\n",
    "\n",
    "if summarize_table('SigninLogs', signin_df, signin_workspace, signin_err):\n",
    "    if 'LocationDetails' not in signin_df.columns:\n",
    "        print('‚ö†Ô∏è LocationDetails column absent ‚Äì skipping extraction')\n",
    "    else:\n",
    "        sample_row = signin_df.select('LocationDetails').dropna().limit(1).collect()\n",
    "        if not sample_row:\n",
    "            print('‚ÑπÔ∏è No LocationDetails data to parse')\n",
    "        else:\n",
    "            sample_json = sample_row[0]['LocationDetails']\n",
    "            schema_literal = spark.read.json(spark.sparkContext.parallelize([sample_json])).schema\n",
    "            location_expanded = signin_df.select(\n",
    "                'TimeGenerated',\n",
    "                'UserPrincipalName',\n",
    "                from_json(col('LocationDetails'), schema_literal).alias('Location'),\n",
    "                'AppDisplayName',\n",
    "                'IpAddress',\n",
    "                'ResultType'\n",
    "            )\n",
    "            expanded_cols = [\n",
    "                col('Location.city').alias('City'),\n",
    "                col('Location.state').alias('State'),\n",
    "                col('Location.countryOrRegion').alias('CountryOrRegion'),\n",
    "                col('Location.geoCoordinates.latitude').alias('Latitude'),\n",
    "                col('Location.geoCoordinates.longitude').alias('Longitude')\n",
    "            ]\n",
    "            location_expanded.select('TimeGenerated', 'UserPrincipalName', *expanded_cols).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a0370",
   "metadata": {},
   "source": [
    "## 4. Sign-ins from unusual countries\n",
    "Detect country deviations by comparing each user's distinct sign-in countries against a baseline threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29aff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNUSUAL_COUNTRY_THRESHOLD = 3\n",
    "if 'signin_df' not in globals():\n",
    "    signin_df, signin_workspace, signin_err = smart_load('SigninLogs')\n",
    "\n",
    "if summarize_table('SigninLogs', signin_df, signin_workspace, signin_err):\n",
    "    country_df = None\n",
    "\n",
    "    if 'Location' in signin_df.columns:\n",
    "        loc_field_type = signin_df.schema['Location'].dataType\n",
    "        if isinstance(loc_field_type, StructType):\n",
    "            country_df = signin_df.select(\n",
    "                col('UserPrincipalName'),\n",
    "                col('Location.countryOrRegion').alias('Country')\n",
    "            )\n",
    "        elif isinstance(loc_field_type, MapType):\n",
    "            country_df = signin_df.select(\n",
    "                col('UserPrincipalName'),\n",
    "                col(\"Location['countryOrRegion']\").alias('Country')\n",
    "            )\n",
    "        elif isinstance(loc_field_type, StringType):\n",
    "            sample_value = signin_df.select('Location').dropna().limit(1).collect()\n",
    "            if sample_value:\n",
    "                sample_json = sample_value[0]['Location']\n",
    "                trimmed = sample_json.strip()\n",
    "                if trimmed.startswith('{') and trimmed.endswith('}'):\n",
    "                    inferred_schema = spark.read.json(\n",
    "                        spark.sparkContext.parallelize([trimmed])\n",
    "                    ).schema\n",
    "                    country_df = signin_df.select(\n",
    "                        col('UserPrincipalName'),\n",
    "                        from_json(col('Location'), inferred_schema).alias('ParsedLocation')\n",
    "                    ).select(\n",
    "                        col('UserPrincipalName'),\n",
    "                        col('ParsedLocation.countryOrRegion').alias('Country')\n",
    "                    )\n",
    "                else:\n",
    "                    country_df = signin_df.select(\n",
    "                        col('UserPrincipalName'),\n",
    "                        col('Location').alias('Country')\n",
    "                    )\n",
    "    if country_df is None and 'LocationDetails' in signin_df.columns:\n",
    "        sample_value = signin_df.select('LocationDetails').dropna().limit(1).collect()\n",
    "        if sample_value:\n",
    "            sample_json = sample_value[0]['LocationDetails']\n",
    "            inferred_schema = spark.read.json(\n",
    "                spark.sparkContext.parallelize([sample_json])\n",
    "            ).schema\n",
    "            country_df = signin_df.select(\n",
    "                col('UserPrincipalName'),\n",
    "                from_json(col('LocationDetails'), inferred_schema).alias('LocationDetailsStruct')\n",
    "            ).select(\n",
    "                col('UserPrincipalName'),\n",
    "                col('LocationDetailsStruct.countryOrRegion').alias('Country')\n",
    "            )\n",
    "\n",
    "    if country_df is None:\n",
    "        print('‚ö†Ô∏è Unable to derive country information from Location/LocationDetails columns')\n",
    "    else:\n",
    "        country_counts = country_df.dropna()\n",
    "        per_user = country_counts.groupBy('UserPrincipalName').agg(countDistinct('Country').alias('DistinctCountries'))\n",
    "        unusual = per_user.filter(col('DistinctCountries') > UNUSUAL_COUNTRY_THRESHOLD)\n",
    "        total = unusual.count()\n",
    "        if total > 0:\n",
    "            print(f'üö® Users exceeding country threshold ({UNUSUAL_COUNTRY_THRESHOLD}): {total}')\n",
    "            unusual.orderBy(col('DistinctCountries').desc()).show(20, truncate=False)\n",
    "        else:\n",
    "            print('‚úÖ No users exceeded the unusual country threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b0f1b",
   "metadata": {},
   "source": [
    "## 5. Brute-force activity (multiple failed sign-ins)\n",
    "Flag accounts with repeated failures and large spreads across IPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82574ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIL_THRESHOLD = 20\n",
    "if 'signin_df' not in globals():\n",
    "    signin_df, signin_workspace, signin_err = smart_load('SigninLogs')\n",
    "\n",
    "if summarize_table('SigninLogs', signin_df, signin_workspace, signin_err):\n",
    "    required = {'ResultType', 'UserPrincipalName', 'IpAddress'}\n",
    "    if not required.issubset(signin_df.columns):\n",
    "        print('‚ö†Ô∏è Missing ResultType/UserPrincipalName/IpAddress columns')\n",
    "    else:\n",
    "        failures = signin_df.filter(~col('ResultType').isin(0, '0', 'Success', 'success'))\n",
    "        grouped = failures.groupBy('UserPrincipalName').agg(\n",
    "            spark_count('*').alias('FailedAttempts'),\n",
    "            countDistinct('IpAddress').alias('DistinctFailIPs')\n",
    "        )\n",
    "        suspects = grouped.filter(col('FailedAttempts') >= FAIL_THRESHOLD)\n",
    "        suspect_total = suspects.count()\n",
    "        if suspect_total > 0:\n",
    "            print(f'üö® Accounts with >= {FAIL_THRESHOLD} failures: {suspect_total}')\n",
    "            suspects.orderBy(col('FailedAttempts').desc()).show(20, truncate=False)\n",
    "        else:\n",
    "            print('‚úÖ No accounts crossed the brute-force threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a571e60",
   "metadata": {},
   "source": [
    "## 6. Lateral movement attempts\n",
    "Inspect DeviceNetworkEvents for unusual internal connections such as unexpected SMB/RDP cross-host traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df, network_ws, network_err = smart_load('DeviceNetworkEvents')\n",
    "if summarize_table('DeviceNetworkEvents', network_df, network_ws, network_err):\n",
    "    required_cols = {'InitiatingProcessAccountName', 'RemoteUrl', 'RemotePort', 'Protocol', 'ReportId'}\n",
    "    available = set(network_df.columns)\n",
    "    if not required_cols.issubset(available):\n",
    "        print('‚ö†Ô∏è Required columns missing ‚Äì adjust schema or ingestion settings')\n",
    "    else:\n",
    "        suspicious_ports = [3389, 5985, 5986, 445]\n",
    "        lateral = network_df.filter(col('RemotePort').isin(suspicious_ports))\n",
    "        aggregated = lateral.groupBy('InitiatingProcessAccountName', 'RemoteUrl', 'RemotePort').agg(\n",
    "            spark_count('*').alias('Events'),\n",
    "            countDistinct('ReportId').alias('DistinctReports')\n",
    "        )\n",
    "        flagged = aggregated.filter((col('Events') >= 10) | (col('DistinctReports') >= 5))\n",
    "        count_flagged = flagged.count()\n",
    "        if count_flagged > 0:\n",
    "            print(f'üö® Potential lateral movement connections: {count_flagged}')\n",
    "            flagged.orderBy(col('Events').desc()).show(20, truncate=False)\n",
    "        else:\n",
    "            print('‚úÖ No high-volume suspicious lateral connections detected (threshold: >=10 events or >=5 reports)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a0918c",
   "metadata": {},
   "source": [
    "## 7. Credential dumping indicators\n",
    "Search DeviceProcessEvents for known credential dumping tools and suspicious access to LSASS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd105ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df, process_ws, process_err = smart_load('DeviceProcessEvents')\n",
    "if summarize_table('DeviceProcessEvents', process_df, process_ws, process_err):\n",
    "    required_cols = {'FileName', 'ProcessCommandLine', 'InitiatingProcessFileName', 'DeviceName', 'AccountName'}\n",
    "    if not required_cols.issubset(process_df.columns):\n",
    "        print('‚ö†Ô∏è DeviceProcessEvents missing required fields for this heuristic')\n",
    "    else:\n",
    "        pattern_terms = [\n",
    "            'mimikatz',\n",
    "            'procdump',\n",
    "            'lsass',\n",
    "            'comsvcs\\\\.dll',\n",
    "            'rundll32\\\\.exe.*comsvcs\\\\.dll.*minidump'\n",
    "        ]\n",
    "        pattern_regex = '(' + '|'.join(pattern_terms) + ')'\n",
    "        indicators = process_df.filter(\n",
    "            lower(col('FileName')).rlike(pattern_regex) |\n",
    "            lower(col('ProcessCommandLine')).rlike(pattern_regex)\n",
    "        )\n",
    "        total_hits = indicators.count()\n",
    "        if total_hits > 0:\n",
    "            print(f'üö® Credential dumping indicators detected: {total_hits}')\n",
    "            indicators.select('TimeGenerated', 'DeviceName', 'AccountName', 'FileName', 'ProcessCommandLine').orderBy(col('TimeGenerated').desc()).show(25, truncate=False)\n",
    "        else:\n",
    "            print('‚úÖ No credential dumping indicators detected with the current heuristics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295c346",
   "metadata": {},
   "source": [
    "## üìã Wrap-up\n",
    "- Update thresholds to fit your organization's baseline\n",
    "- Promote interesting findings to analytics tier for detections\n",
    "- Convert cells into scheduled jobs where recurring monitoring is required\n",
    "\n",
    "üõ†Ô∏è This notebook follows the same pattern as the rest of the collection ‚Äì feel free to adapt or extend each scenario into a dedicated hunting playbook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [07_Notebook_Examples_Scenarios]",
   "language": "Python",
   "name": "MSGMedium"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
