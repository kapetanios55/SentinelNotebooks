{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72f7222",
   "metadata": {},
   "source": [
    "# Anomalous Sign-In Detection and Enrichment\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook analyzes sign-in logs to detect anomalous authentication patterns, enriches them with threat intelligence and geo-location data, and saves findings to a managed table for alerting and investigation.\n",
    "\n",
    "### How to Run Notebook\n",
    "\n",
    "Reference the general [Sentinel Notebook Readme](../README.md) for guidance on installing and running notebooks.\n",
    "\n",
    "For this job specifically there is a job yaml file included. Action required by users on that job yaml:\n",
    "- **StartTime**: What day and time the job should start running.\n",
    "- **EndTime**: What day and time the job should stop running.\n",
    "- **JobName(Optional)**: If you decide to change the jobname, prefix the name with 'Anomalous-SignIn-Detection'.\n",
    "\n",
    "### Key Features:\n",
    "- **Anomaly Detection**: Identifies suspicious sign-in patterns including:\n",
    "  - Multiple failed attempts followed by success\n",
    "  - Impossible travel scenarios (geographically distant logins)\n",
    "  - Sign-ins from risky IPs or locations\n",
    "  - Unusual sign-in times or locations for users\n",
    "- **Threat Intelligence Enrichment**: Correlates suspicious IPs with known threat indicators\n",
    "- **Geo-Location Analysis**: Detects travel velocity anomalies\n",
    "- **Risk Scoring**: Assigns risk scores based on multiple factors\n",
    "- **Incremental Updates**: Avoids duplicate alerts on subsequent runs\n",
    "\n",
    "### Data Sources:\n",
    "- **SigninLogs**: Standard user sign-in activities\n",
    "- **AADNonInteractiveUserSignInLogs**: Non-interactive authentication\n",
    "- **ThreatIntelIndicators**: IP threat intelligence data\n",
    "\n",
    "### Required Customer Input:\n",
    "- **WORKSPACE_NAME**: Customer Log Analytics workspace name. If 'None', auto-detects first available workspace.\n",
    "- **LOOKBACK_DAYS**: 1-90. Analysis period for sign-in logs. Default 7.\n",
    "- **MIN_FAILED_ATTEMPTS**: Minimum failed logins before success to flag as anomalous. Default 3.\n",
    "- **IMPOSSIBLE_TRAVEL_KM_PER_HOUR**: Speed threshold for impossible travel detection. Default 800.\n",
    "\n",
    "### Output Schema:\n",
    "Results are saved to the `AnomalousSignInFindings_SPRK_CL` custom table with the following schema:\n",
    "\n",
    "| Column Name | Type | Description |\n",
    "|-------------|------|-------------|\n",
    "| FindingId | string | Unique identifier for the finding |\n",
    "| JobId | string | Identifier for the analysis job execution |\n",
    "| JobStartTime | datetime | Timestamp when the job started |\n",
    "| JobEndTime | datetime | Timestamp when the job completed |\n",
    "| AnomalyType | string | Type of anomaly detected (e.g., \"FailedThenSuccess\", \"ImpossibleTravel\", \"ThreatIP\") |\n",
    "| UserPrincipalName | string | User account involved in anomalous activity |\n",
    "| IPAddress | string | Source IP address |\n",
    "| Location | string | Sign-in location |\n",
    "| RiskScore | int | Calculated risk score (0-100) |\n",
    "| FailedAttempts | int | Number of failed attempts (for FailedThenSuccess type) |\n",
    "| TravelDistanceKm | double | Distance traveled (for ImpossibleTravel type) |\n",
    "| TravelVelocityKmh | double | Travel velocity (for ImpossibleTravel type) |\n",
    "| ThreatIntelMatch | boolean | Whether IP matched threat intelligence |\n",
    "| ThreatActors | dynamic | Threat actors associated with IP (if any) |\n",
    "| EventReferences | dynamic | Array of related sign-in event IDs |\n",
    "| FirstSeen | datetime | First occurrence of this anomaly pattern |\n",
    "| LastSeen | datetime | Most recent occurrence |\n",
    "| OccurrenceCount | int | Number of times this pattern occurred |\n",
    "| TenantId | string | Azure tenant identifier |\n",
    "| TimeGenerated | datetime | Timestamp of record creation |\n",
    "\n",
    "### Version\n",
    "1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b66fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# PARAMETERS AND CONFIGURATION\n",
    "# ===============================================================================\n",
    "\n",
    "# Workspace Configuration\n",
    "WORKSPACE_NAME = None  # Set to your workspace name or leave as None for auto-detection\n",
    "LOOKBACK_DAYS = 7  # Days to look back for analysis (1-90)\n",
    "\n",
    "# Anomaly Detection Thresholds\n",
    "MIN_FAILED_ATTEMPTS = 3  # Minimum failed logins before success to flag\n",
    "IMPOSSIBLE_TRAVEL_KM_PER_HOUR = 800  # Speed threshold for impossible travel (km/h)\n",
    "MAX_TRAVEL_TIME_HOURS = 1  # Maximum time between logins to check for impossible travel\n",
    "\n",
    "# Risk Scoring Weights (total should equal 100)\n",
    "WEIGHT_FAILED_ATTEMPTS = 30\n",
    "WEIGHT_IMPOSSIBLE_TRAVEL = 40\n",
    "WEIGHT_THREAT_INTEL = 30\n",
    "\n",
    "# Table Names\n",
    "THREAT_INTEL_TABLE = \"ThreatIntelIndicators\"\n",
    "RESULTS_TABLE = \"AnomalousSignInFindings_SPRK_CL\"\n",
    "\n",
    "# Debug Settings\n",
    "SHOW_DEBUG_LOGS = False\n",
    "SHOW_STATS = True\n",
    "\n",
    "# Version\n",
    "VERSION = \"1.0.0\"\n",
    "\n",
    "# ===============================================================================\n",
    "# PARAMETER VALIDATION\n",
    "# ===============================================================================\n",
    "if not (1 <= LOOKBACK_DAYS <= 90):\n",
    "    raise ValueError(\"LOOKBACK_DAYS must be between 1 and 90\")\n",
    "\n",
    "if MIN_FAILED_ATTEMPTS < 1:\n",
    "    raise ValueError(\"MIN_FAILED_ATTEMPTS must be at least 1\")\n",
    "\n",
    "if IMPOSSIBLE_TRAVEL_KM_PER_HOUR <= 0:\n",
    "    raise ValueError(\"IMPOSSIBLE_TRAVEL_KM_PER_HOUR must be positive\")\n",
    "\n",
    "if not RESULTS_TABLE.endswith('_SPRK_CL'):\n",
    "    RESULTS_TABLE = f\"{RESULTS_TABLE}_SPRK_CL\"\n",
    "\n",
    "print(f\"Notebook version: {VERSION}\")\n",
    "print(f\"Configuration loaded: Workspace={WORKSPACE_NAME}, Lookback={LOOKBACK_DAYS} days\")\n",
    "print(f\"Thresholds: Failed attempts>={MIN_FAILED_ATTEMPTS}, Travel speed>{IMPOSSIBLE_TRAVEL_KM_PER_HOUR} km/h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5dfb1",
   "metadata": {},
   "source": [
    "## Imports, Sentinel Provider, and Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ===============================================================================\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, array, struct, when, count, row_number,\n",
    "    first, last, collect_list, collect_set, size, explode, concat_ws, concat,\n",
    "    array_distinct, coalesce, sum as spark_sum, count_distinct, avg,\n",
    "    min as spark_min, max as spark_max, lag, lead, unix_timestamp,\n",
    "    from_json, to_json, get_json_object, broadcast, expr, udf, greatest, least, split, flatten\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, DoubleType, BooleanType, ArrayType,\n",
    "    StructType, StructField, TimestampType, LongType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Spark configuration for performance\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", str(150 * 1024 * 1024))\n",
    "\n",
    "# Initialize provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "print(\"✓ Microsoft Sentinel data provider initialized\")\n",
    "\n",
    "# Job tracking\n",
    "job_id = str(uuid.uuid4())\n",
    "job_start_time = current_timestamp()\n",
    "print(f\"✓ Job ID: {job_id}\")\n",
    "\n",
    "# Auto-select workspace if not provided\n",
    "if WORKSPACE_NAME is None or WORKSPACE_NAME.strip() == \"\":\n",
    "    print(\"Auto-selecting workspace...\")\n",
    "    databases = data_provider.list_databases()\n",
    "    for db in databases:\n",
    "        if db.lower() not in [\"default\", \"system tables\"]:\n",
    "            WORKSPACE_NAME = db\n",
    "            print(f\"✓ Auto-selected workspace: {WORKSPACE_NAME}\")\n",
    "            break\n",
    "\n",
    "if WORKSPACE_NAME is None:\n",
    "    raise ValueError(\"No workspace available. Please specify WORKSPACE_NAME.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e12bb6c",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d343c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ===============================================================================\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points on Earth.\n",
    "    Returns distance in kilometers.\n",
    "    \"\"\"\n",
    "    if None in [lat1, lon1, lat2, lon2]:\n",
    "        return None\n",
    "    \n",
    "    # Convert to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    \n",
    "    # Radius of Earth in kilometers\n",
    "    r = 6371\n",
    "    return c * r\n",
    "\n",
    "# Register UDF for distance calculation\n",
    "haversine_udf = udf(haversine_distance, DoubleType())\n",
    "\n",
    "def calculate_risk_score(failed_attempts, travel_velocity, has_threat_intel):\n",
    "    \"\"\"\n",
    "    Calculate risk score (0-100) based on multiple factors.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Failed attempts component (0-30 points)\n",
    "    if failed_attempts:\n",
    "        score += min(WEIGHT_FAILED_ATTEMPTS, (failed_attempts / 10) * WEIGHT_FAILED_ATTEMPTS)\n",
    "    \n",
    "    # Impossible travel component (0-40 points)\n",
    "    if travel_velocity:\n",
    "        if travel_velocity > IMPOSSIBLE_TRAVEL_KM_PER_HOUR:\n",
    "            excess = (travel_velocity - IMPOSSIBLE_TRAVEL_KM_PER_HOUR) / IMPOSSIBLE_TRAVEL_KM_PER_HOUR\n",
    "            score += min(WEIGHT_IMPOSSIBLE_TRAVEL, WEIGHT_IMPOSSIBLE_TRAVEL * (1 + excess * 0.5))\n",
    "    \n",
    "    # Threat intel component (0-30 points)\n",
    "    if has_threat_intel:\n",
    "        score += WEIGHT_THREAT_INTEL\n",
    "    \n",
    "    return min(100, int(score))\n",
    "\n",
    "# Register UDF for risk scoring\n",
    "calculate_risk_udf = udf(calculate_risk_score, IntegerType())\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7519de7",
   "metadata": {},
   "source": [
    "## Load Sign-In Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeff527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# LOAD SIGN-IN LOGS\n",
    "# ===============================================================================\n",
    "\n",
    "# Calculate date range\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=LOOKBACK_DAYS)\n",
    "\n",
    "print(f\"Loading sign-in logs from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}...\")\n",
    "\n",
    "# Load interactive sign-in logs\n",
    "try:\n",
    "    signin_logs = data_provider.read_table(\"SigninLogs\", WORKSPACE_NAME)\n",
    "    signin_logs = signin_logs.filter(\n",
    "        (col(\"TimeGenerated\") >= lit(start_date)) &\n",
    "        (col(\"TimeGenerated\") <= lit(end_date))\n",
    "    ).select(\n",
    "        col(\"Id\").alias(\"EventId\"),\n",
    "        col(\"TimeGenerated\"),\n",
    "        col(\"UserPrincipalName\"),\n",
    "        col(\"IPAddress\"),\n",
    "        col(\"Location\"),\n",
    "        col(\"ResultType\"),\n",
    "        col(\"ResultDescription\"),\n",
    "        when(col(\"ResultType\") == \"0\", lit(True)).otherwise(lit(False)).alias(\"IsSuccess\"),\n",
    "        col(\"TenantId\")\n",
    "    )\n",
    "    print(f\"✓ Loaded SigninLogs: {signin_logs.count():,} records\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load SigninLogs: {e}\")\n",
    "    signin_logs = None\n",
    "\n",
    "# Load non-interactive sign-in logs\n",
    "try:\n",
    "    noninteractive_logs = data_provider.read_table(\"AADNonInteractiveUserSignInLogs\", WORKSPACE_NAME)\n",
    "    noninteractive_logs = noninteractive_logs.filter(\n",
    "        (col(\"TimeGenerated\") >= lit(start_date)) &\n",
    "        (col(\"TimeGenerated\") <= lit(end_date))\n",
    "    ).select(\n",
    "        col(\"Id\").alias(\"EventId\"),\n",
    "        col(\"TimeGenerated\"),\n",
    "        col(\"UserPrincipalName\"),\n",
    "        col(\"IPAddress\"),\n",
    "        col(\"Location\"),\n",
    "        col(\"ResultType\"),\n",
    "        col(\"ResultDescription\"),\n",
    "        when(col(\"ResultType\") == \"0\", lit(True)).otherwise(lit(False)).alias(\"IsSuccess\"),\n",
    "        col(\"TenantId\")\n",
    "    )\n",
    "    print(f\"✓ Loaded AADNonInteractiveUserSignInLogs: {noninteractive_logs.count():,} records\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load AADNonInteractiveUserSignInLogs: {e}\")\n",
    "    noninteractive_logs = None\n",
    "\n",
    "# Combine logs\n",
    "if signin_logs is not None and noninteractive_logs is not None:\n",
    "    all_signin_logs = signin_logs.union(noninteractive_logs)\n",
    "elif signin_logs is not None:\n",
    "    all_signin_logs = signin_logs\n",
    "elif noninteractive_logs is not None:\n",
    "    all_signin_logs = noninteractive_logs\n",
    "else:\n",
    "    raise RuntimeError(\"No sign-in logs available\")\n",
    "\n",
    "all_signin_logs = all_signin_logs.cache()\n",
    "total_signin_count = all_signin_logs.count()\n",
    "print(f\"✓ Total sign-in events: {total_signin_count:,}\")\n",
    "\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    print(\"\\nSample sign-in logs:\")\n",
    "    all_signin_logs.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9da8a3",
   "metadata": {},
   "source": [
    "## Detect Anomaly: Failed Attempts Followed by Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# ANOMALY DETECTION: FAILED THEN SUCCESS\n",
    "# ===============================================================================\n",
    "\n",
    "print(\"\\nDetecting failed-then-success patterns...\")\n",
    "\n",
    "# Window for ordering events by user and time\n",
    "user_window = Window.partitionBy(\"UserPrincipalName\", \"IPAddress\").orderBy(\"TimeGenerated\")\n",
    "\n",
    "# Identify sequences where failures are followed by success\n",
    "signin_with_next = all_signin_logs.withColumn(\n",
    "    \"NextSuccess\", \n",
    "    lead(\"IsSuccess\", 1).over(user_window)\n",
    ").withColumn(\n",
    "    \"NextEventId\",\n",
    "    lead(\"EventId\", 1).over(user_window)\n",
    ").withColumn(\n",
    "    \"NextTime\",\n",
    "    lead(\"TimeGenerated\", 1).over(user_window)\n",
    ")\n",
    "\n",
    "# Find failed attempts that are followed by success within reasonable time (1 hour)\n",
    "failed_then_success = signin_with_next.filter(\n",
    "    (col(\"IsSuccess\") == False) &\n",
    "    (col(\"NextSuccess\") == True) &\n",
    "    ((unix_timestamp(\"NextTime\") - unix_timestamp(\"TimeGenerated\")) <= 3600)\n",
    ")\n",
    "\n",
    "# Group by user/IP and count consecutive failures before success\n",
    "failed_counts = failed_then_success.groupBy(\n",
    "    \"UserPrincipalName\", \n",
    "    \"IPAddress\",\n",
    "    \"NextEventId\"\n",
    ").agg(\n",
    "    count(\"*\").alias(\"FailedAttempts\"),\n",
    "    spark_min(\"TimeGenerated\").alias(\"FirstFailure\"),\n",
    "    spark_max(\"NextTime\").alias(\"SuccessTime\"),\n",
    "    first(\"Location\").alias(\"Location\"),\n",
    "    first(\"TenantId\").alias(\"TenantId\"),\n",
    "    collect_list(\"EventId\").alias(\"FailedEventIds\")\n",
    ").filter(col(\"FailedAttempts\") >= MIN_FAILED_ATTEMPTS)\n",
    "\n",
    "# Create findings with properly structured EventReferences array\n",
    "failed_then_success_findings = failed_counts.withColumn(\n",
    "    \"FindingId\",\n",
    "    concat_ws(\"-\", lit(\"FTS\"), col(\"UserPrincipalName\"), col(\"IPAddress\"), col(\"NextEventId\"))\n",
    ").withColumn(\n",
    "    \"AnomalyType\", lit(\"FailedThenSuccess\")\n",
    ").withColumn(\n",
    "    \"EventReferences\",\n",
    "    array_distinct(concat(col(\"FailedEventIds\"), array(col(\"NextEventId\"))))\n",
    ").select(\n",
    "    \"FindingId\",\n",
    "    \"AnomalyType\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"IPAddress\",\n",
    "    \"Location\",\n",
    "    \"FailedAttempts\",\n",
    "    lit(None).cast(DoubleType()).alias(\"TravelDistanceKm\"),\n",
    "    lit(None).cast(DoubleType()).alias(\"TravelVelocityKmh\"),\n",
    "    \"FirstFailure\",\n",
    "    \"SuccessTime\",\n",
    "    \"EventReferences\",\n",
    "    \"TenantId\"\n",
    ")\n",
    "\n",
    "failed_then_success_count = failed_then_success_findings.count()\n",
    "print(f\"✓ Detected {failed_then_success_count:,} failed-then-success anomalies\")\n",
    "\n",
    "if SHOW_DEBUG_LOGS and failed_then_success_count > 0:\n",
    "    print(\"\\nSample findings:\")\n",
    "    failed_then_success_findings.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ac09c",
   "metadata": {},
   "source": [
    "## Detect Anomaly: Impossible Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185655e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# ANOMALY DETECTION: IMPOSSIBLE TRAVEL\n",
    "# ===============================================================================\n",
    "\n",
    "print(\"\\nDetecting impossible travel patterns...\")\n",
    "\n",
    "# Parse location to extract city, state, country, lat, lon\n",
    "# Location format typically: \"City, State, Country, Lat:XX.XX, Long:YY.YY\"\n",
    "signin_with_geo = all_signin_logs.filter(col(\"IsSuccess\") == True).withColumn(\n",
    "    \"LocationParts\", expr(\"split(Location, ',')\")  \n",
    ").withColumn(\n",
    "    \"Latitude\",\n",
    "    expr(\"cast(regexp_extract(Location, 'Lat:([\\\\-0-9\\\\.]+)', 1) as double)\")\n",
    ").withColumn(\n",
    "    \"Longitude\",\n",
    "    expr(\"cast(regexp_extract(Location, 'Long:([\\\\-0-9\\\\.]+)', 1) as double)\")\n",
    ").filter(\n",
    "    col(\"Latitude\").isNotNull() & col(\"Longitude\").isNotNull()\n",
    ")\n",
    "\n",
    "# Get consecutive logins per user\n",
    "user_time_window = Window.partitionBy(\"UserPrincipalName\").orderBy(\"TimeGenerated\")\n",
    "\n",
    "signin_with_prev = signin_with_geo.withColumn(\n",
    "    \"PrevTime\", lag(\"TimeGenerated\", 1).over(user_time_window)\n",
    ").withColumn(\n",
    "    \"PrevLat\", lag(\"Latitude\", 1).over(user_time_window)\n",
    ").withColumn(\n",
    "    \"PrevLon\", lag(\"Longitude\", 1).over(user_time_window)\n",
    ").withColumn(\n",
    "    \"PrevLocation\", lag(\"Location\", 1).over(user_time_window)\n",
    ").withColumn(\n",
    "    \"PrevEventId\", lag(\"EventId\", 1).over(user_time_window)\n",
    ").filter(\n",
    "    col(\"PrevTime\").isNotNull()\n",
    ")\n",
    "\n",
    "# Calculate distance and velocity\n",
    "travel_analysis = signin_with_prev.withColumn(\n",
    "    \"TimeDiffHours\",\n",
    "    (unix_timestamp(\"TimeGenerated\") - unix_timestamp(\"PrevTime\")) / 3600.0\n",
    ").withColumn(\n",
    "    \"DistanceKm\",\n",
    "    haversine_udf(col(\"PrevLat\"), col(\"PrevLon\"), col(\"Latitude\"), col(\"Longitude\"))\n",
    ").withColumn(\n",
    "    \"VelocityKmh\",\n",
    "    when(col(\"TimeDiffHours\") > 0, col(\"DistanceKm\") / col(\"TimeDiffHours\")).otherwise(lit(None))\n",
    ")\n",
    "\n",
    "# Identify impossible travel (velocity exceeds threshold and within time window)\n",
    "impossible_travel = travel_analysis.filter(\n",
    "    (col(\"VelocityKmh\") > IMPOSSIBLE_TRAVEL_KM_PER_HOUR) &\n",
    "    (col(\"TimeDiffHours\") <= MAX_TRAVEL_TIME_HOURS)\n",
    ")\n",
    "\n",
    "# Create findings\n",
    "impossible_travel_findings = impossible_travel.withColumn(\n",
    "    \"FindingId\",\n",
    "    concat_ws(\"-\", lit(\"IT\"), col(\"UserPrincipalName\"), col(\"EventId\"))\n",
    ").withColumn(\n",
    "    \"AnomalyType\", lit(\"ImpossibleTravel\")\n",
    ").withColumn(\n",
    "    \"EventReferences\",\n",
    "    array(col(\"PrevEventId\"), col(\"EventId\"))\n",
    ").select(\n",
    "    \"FindingId\",\n",
    "    \"AnomalyType\",\n",
    "    \"UserPrincipalName\",\n",
    "    \"IPAddress\",\n",
    "    concat_ws(\" -> \", col(\"PrevLocation\"), col(\"Location\")).alias(\"Location\"),\n",
    "    lit(None).cast(IntegerType()).alias(\"FailedAttempts\"),\n",
    "    col(\"DistanceKm\").alias(\"TravelDistanceKm\"),\n",
    "    col(\"VelocityKmh\").alias(\"TravelVelocityKmh\"),\n",
    "    col(\"PrevTime\").alias(\"FirstFailure\"),\n",
    "    col(\"TimeGenerated\").alias(\"SuccessTime\"),\n",
    "    \"EventReferences\",\n",
    "    \"TenantId\"\n",
    ")\n",
    "\n",
    "impossible_travel_count = impossible_travel_findings.count()\n",
    "print(f\"✓ Detected {impossible_travel_count:,} impossible travel anomalies\")\n",
    "\n",
    "if SHOW_DEBUG_LOGS and impossible_travel_count > 0:\n",
    "    print(\"\\nSample findings:\")\n",
    "    impossible_travel_findings.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5285a4c",
   "metadata": {},
   "source": [
    "## Enrich with Threat Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# THREAT INTELLIGENCE ENRICHMENT\n",
    "# ===============================================================================\n",
    "\n",
    "print(\"\\nEnriching findings with threat intelligence...\")\n",
    "\n",
    "# Combine all findings\n",
    "all_findings = failed_then_success_findings.union(impossible_travel_findings)\n",
    "\n",
    "if all_findings.count() == 0:\n",
    "    print(\"⚠ No anomalies detected. Skipping threat intelligence enrichment.\")\n",
    "    enriched_findings = all_findings.withColumn(\n",
    "        \"ThreatIntelMatch\", lit(False)\n",
    "    ).withColumn(\n",
    "        \"ThreatActors\", array()\n",
    "    )\n",
    "else:\n",
    "    # Load threat intelligence for IPs\n",
    "    try:\n",
    "        threat_intel = data_provider.read_table(THREAT_INTEL_TABLE, WORKSPACE_NAME)\n",
    "        \n",
    "        # Filter for IP-related indicators that are currently valid\n",
    "        current_time = current_timestamp()\n",
    "        ip_threat_intel = threat_intel.filter(\n",
    "            (col(\"ObservableKey\").isin(\n",
    "                \"ipv4-addr:value\", \n",
    "                \"ipv6-addr:value\",\n",
    "                \"network-traffic:src_ref.value\",\n",
    "                \"network-traffic:dst_ref.value\"\n",
    "            )) &\n",
    "            (col(\"IsActive\") == True) &\n",
    "            (col(\"ValidFrom\") <= current_time) &\n",
    "            (col(\"ValidUntil\") >= current_time)\n",
    "        ).select(\n",
    "            col(\"ObservableValue\").alias(\"ThreatIP\"),\n",
    "            get_json_object(col(\"Data\"), \"$.threat_actors\").alias(\"ThreatActorsJson\"),\n",
    "            col(\"ThreatType\"),\n",
    "            col(\"Confidence\")\n",
    "        ).withColumn(\n",
    "            \"ThreatActors\",\n",
    "            from_json(col(\"ThreatActorsJson\"), ArrayType(StringType()))\n",
    "        ).drop(\"ThreatActorsJson\")\n",
    "        \n",
    "        ti_count = ip_threat_intel.count()\n",
    "        print(f\"✓ Loaded {ti_count:,} IP threat intelligence indicators\")\n",
    "        \n",
    "        # Join findings with threat intel\n",
    "        enriched_findings = all_findings.join(\n",
    "            broadcast(ip_threat_intel),\n",
    "            all_findings.IPAddress == ip_threat_intel.ThreatIP,\n",
    "            \"left\"\n",
    "        ).withColumn(\n",
    "            \"ThreatIntelMatch\",\n",
    "            when(col(\"ThreatIP\").isNotNull(), lit(True)).otherwise(lit(False))\n",
    "        ).select(\n",
    "            all_findings[\"*\"],\n",
    "            \"ThreatIntelMatch\",\n",
    "            coalesce(col(\"ThreatActors\"), array()).alias(\"ThreatActors\")\n",
    "        )\n",
    "        \n",
    "        matched_count = enriched_findings.filter(col(\"ThreatIntelMatch\") == True).count()\n",
    "        print(f\"✓ Matched {matched_count:,} findings with threat intelligence\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not load threat intelligence: {e}\")\n",
    "        enriched_findings = all_findings.withColumn(\n",
    "            \"ThreatIntelMatch\", lit(False)\n",
    "        ).withColumn(\n",
    "            \"ThreatActors\", array()\n",
    "        )\n",
    "\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    print(\"\\nSample enriched findings:\")\n",
    "    enriched_findings.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e38300",
   "metadata": {},
   "source": [
    "## Calculate Risk Scores and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# RISK SCORING AND AGGREGATION\n",
    "# ===============================================================================\n",
    "\n",
    "print(\"\\nCalculating risk scores...\")\n",
    "\n",
    "# Calculate risk score\n",
    "findings_with_risk = enriched_findings.withColumn(\n",
    "    \"RiskScore\",\n",
    "    calculate_risk_udf(\n",
    "        col(\"FailedAttempts\"),\n",
    "        col(\"TravelVelocityKmh\"),\n",
    "        col(\"ThreatIntelMatch\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Aggregate by finding (in case of duplicates)\n",
    "final_findings = findings_with_risk.groupBy(\"FindingId\").agg(\n",
    "    first(\"AnomalyType\").alias(\"AnomalyType\"),\n",
    "    first(\"UserPrincipalName\").alias(\"UserPrincipalName\"),\n",
    "    first(\"IPAddress\").alias(\"IPAddress\"),\n",
    "    first(\"Location\").alias(\"Location\"),\n",
    "    first(\"RiskScore\").alias(\"RiskScore\"),\n",
    "    first(\"FailedAttempts\").alias(\"FailedAttempts\"),\n",
    "    first(\"TravelDistanceKm\").alias(\"TravelDistanceKm\"),\n",
    "    first(\"TravelVelocityKmh\").alias(\"TravelVelocityKmh\"),\n",
    "    first(\"ThreatIntelMatch\").alias(\"ThreatIntelMatch\"),\n",
    "    first(\"ThreatActors\").alias(\"ThreatActors\"),\n",
    "    flatten(collect_list(\"EventReferences\")).alias(\"EventReferences\"),\n",
    "    spark_min(\"FirstFailure\").alias(\"FirstSeen\"),\n",
    "    spark_max(\"SuccessTime\").alias(\"LastSeen\"),\n",
    "    count(\"*\").alias(\"OccurrenceCount\"),\n",
    "    first(\"TenantId\").alias(\"TenantId\")\n",
    ").withColumn(\n",
    "    \"JobId\", lit(job_id)\n",
    ").withColumn(\n",
    "    \"JobStartTime\", job_start_time\n",
    ").withColumn(\n",
    "    \"JobEndTime\", current_timestamp()\n",
    ").withColumn(\n",
    "    \"TimeGenerated\", current_timestamp()\n",
    ")\n",
    "\n",
    "# Convert arrays to JSON for storage\n",
    "final_findings = final_findings.withColumn(\n",
    "    \"ThreatActors\", to_json(col(\"ThreatActors\"))\n",
    ").withColumn(\n",
    "    \"EventReferences\", to_json(col(\"EventReferences\"))\n",
    ")\n",
    "\n",
    "final_count = final_findings.count()\n",
    "print(f\"✓ Prepared {final_count:,} findings for ingestion\")\n",
    "\n",
    "if SHOW_STATS and final_count > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINDINGS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Risk distribution\n",
    "    print(\"\\n📊 Risk Score Distribution:\")\n",
    "    final_findings.withColumn(\n",
    "        \"RiskBucket\",\n",
    "        when(col(\"RiskScore\") >= 80, lit(\"Critical (80-100)\"))\n",
    "        .when(col(\"RiskScore\") >= 60, lit(\"High (60-79)\"))\n",
    "        .when(col(\"RiskScore\") >= 40, lit(\"Medium (40-59)\"))\n",
    "        .otherwise(lit(\"Low (0-39)\"))\n",
    "    ).groupBy(\"RiskBucket\").count().orderBy(\"RiskBucket\").show()\n",
    "    \n",
    "    # Anomaly type breakdown\n",
    "    print(\"\\n📊 Findings by Anomaly Type:\")\n",
    "    final_findings.groupBy(\"AnomalyType\").agg(\n",
    "        count(\"*\").alias(\"Count\"),\n",
    "        avg(\"RiskScore\").alias(\"AvgRiskScore\")\n",
    "    ).show()\n",
    "    \n",
    "    # Top users\n",
    "    print(\"\\n👤 Top 10 Users by Finding Count:\")\n",
    "    final_findings.groupBy(\"UserPrincipalName\").count().orderBy(\n",
    "        col(\"count\").desc()\n",
    "    ).limit(10).show(truncate=False)\n",
    "    \n",
    "    # Threat intel matches\n",
    "    threat_matches = final_findings.filter(col(\"ThreatIntelMatch\") == True).count()\n",
    "    print(f\"\\n🎯 Threat Intelligence Matches: {threat_matches:,} / {final_count:,} ({threat_matches/final_count*100:.1f}%)\")\n",
    "\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    print(\"\\nSample final findings:\")\n",
    "    final_findings.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98200d9c",
   "metadata": {},
   "source": [
    "## Save Results to Log Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00352f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# SAVE RESULTS TO LOG ANALYTICS\n",
    "# ===============================================================================\n",
    "\n",
    "if final_count == 0:\n",
    "    print(\"\\n✓ No anomalies detected in this run. No data to save.\")\n",
    "else:\n",
    "    print(f\"\\nSaving {final_count:,} findings to {RESULTS_TABLE}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if table exists\n",
    "        existing_df = None\n",
    "        table_exists = False\n",
    "        \n",
    "        try:\n",
    "            existing_df = data_provider.read_table(RESULTS_TABLE, WORKSPACE_NAME)\n",
    "            table_exists = existing_df.count() > 0\n",
    "        except Exception:\n",
    "            table_exists = False\n",
    "        \n",
    "        if not table_exists:\n",
    "            # Create new table\n",
    "            print(f\"📁 Creating new results table: {RESULTS_TABLE}\")\n",
    "            data_provider.save_as_table(final_findings, RESULTS_TABLE, WORKSPACE_NAME)\n",
    "            print(f\"✓ Created table with {final_count:,} findings\")\n",
    "        else:\n",
    "            # Table exists - deduplicate by FindingId\n",
    "            existing_count = existing_df.count()\n",
    "            print(f\"📁 Found existing results table with {existing_count:,} records\")\n",
    "            \n",
    "            # Anti-join to find new findings only\n",
    "            new_findings = final_findings.join(\n",
    "                existing_df.select(\"FindingId\"),\n",
    "                \"FindingId\",\n",
    "                \"leftanti\"\n",
    "            )\n",
    "            \n",
    "            new_count = new_findings.count()\n",
    "            duplicate_count = final_count - new_count\n",
    "            \n",
    "            print(f\"\\n📈 Deduplication results:\")\n",
    "            print(f\"  • Total findings: {final_count:,}\")\n",
    "            print(f\"  • Duplicate findings (already exist): {duplicate_count:,}\")\n",
    "            print(f\"  • New findings to add: {new_count:,}\")\n",
    "            \n",
    "            if new_count > 0:\n",
    "                # Append new findings\n",
    "                data_provider.save_as_table(\n",
    "                    new_findings,\n",
    "                    RESULTS_TABLE,\n",
    "                    WORKSPACE_NAME,\n",
    "                    mode=\"append\"\n",
    "                )\n",
    "                print(f\"✓ Appended {new_count:,} new findings to table\")\n",
    "            else:\n",
    "                print(\"✓ All findings already exist in table. No updates needed.\")\n",
    "        \n",
    "        print(f\"\\n✅ Results saved successfully to workspace '{WORKSPACE_NAME}'\")\n",
    "        print(f\"   Query your findings: {RESULTS_TABLE}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error saving results: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7aafc1",
   "metadata": {},
   "source": [
    "## Summary and Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# SUMMARY AND METRICS\n",
    "# ===============================================================================\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_seconds = end_time - start_time\n",
    "elapsed_minutes = elapsed_seconds / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JOB SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Notebook version: {VERSION}\")\n",
    "print(f\"Job ID: {job_id}\")\n",
    "print(f\"Workspace: {WORKSPACE_NAME}\")\n",
    "print(f\"Analysis period: {LOOKBACK_DAYS} days\")\n",
    "print(f\"Runtime: {elapsed_minutes:.2f} minutes\")\n",
    "print(\"\\n📊 Processing Summary:\")\n",
    "print(f\"  • Sign-in events analyzed: {total_signin_count:,}\")\n",
    "print(f\"  • Failed-then-success anomalies: {failed_then_success_count:,}\")\n",
    "print(f\"  • Impossible travel anomalies: {impossible_travel_count:,}\")\n",
    "print(f\"  • Total anomalies detected: {final_count:,}\")\n",
    "print(f\"\\n📁 Output:\")\n",
    "print(f\"  • Table: {RESULTS_TABLE}\")\n",
    "print(f\"  • Workspace: {WORKSPACE_NAME}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "large pool (16 vCores) [08_Anomalous_SignIn_Detection]",
   "language": "Python",
   "name": "MSGLarge"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
