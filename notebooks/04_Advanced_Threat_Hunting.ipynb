{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0060b03",
   "metadata": {},
   "source": [
    "# Advanced Threat Hunting - Microsoft Sentinel Data Lake\n",
    "\n",
    "This notebook provides advanced threat hunting capabilities using Microsoft Sentinel Data Lake, combining multiple data sources for sophisticated threat detection.\n",
    "\n",
    "## üéØ **SIMPLE SETUP: Just Update the Workspace Names!**\n",
    "This notebook uses a simple manual configuration system. \n",
    "**Just update the workspace names in the configuration cell and run!**\n",
    "\n",
    "## Advanced Use Cases Covered\n",
    "1. **Command and Control (C2) Detection** - Identify beacon behavior and C2 communications\n",
    "2. **Living off the Land** - Detect abuse of legitimate tools for malicious purposes\n",
    "3. **Data Exfiltration Patterns** - Multi-stage data theft detection\n",
    "4. **Advanced Persistent Threat (APT) Indicators** - Long-term compromise detection\n",
    "5. **User Behavior Analytics** - Detect anomalous user activities\n",
    "6. **Behavioral Analytics** - Advanced statistical analysis for threat detection\n",
    "\n",
    "## Prerequisites ‚úÖ\n",
    "- ‚úÖ **Update workspace names in the configuration cell below** (see the simple setup instructions)\n",
    "- ‚úÖ **Multiple data sources available** (SignInLogs, DeviceEvents, NetworkEvents)\n",
    "- ‚úÖ **Microsoft Sentinel Data Lake enabled** in your environment\n",
    "\n",
    "## üöÄ **Publication-Ready Features:**\n",
    "- ‚úÖ **Simple manual configuration** - just update workspace names\n",
    "- ‚úÖ **Works in any environment** with any workspace names\n",
    "- ‚úÖ **Adapts to available data** - uses whatever data sources you have\n",
    "- ‚úÖ **No hardcoded values** - completely portable once configured\n",
    "- ‚úÖ **Advanced analytics** - sophisticated threat detection algorithms\n",
    "- ‚úÖ **Clear error handling** - helpful messages if data isn't available\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56d137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import advanced libraries for threat hunting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Sentinel Data Lake libraries\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "from pyspark.sql.functions import (\n",
    "    col, count as spark_count, desc, asc, when, from_json, \n",
    "    countDistinct, sum as spark_sum, avg, stddev,\n",
    "    date_trunc, hour, dayofweek, minute,\n",
    "    regexp_extract, lower, upper, split, concat,\n",
    "    to_timestamp, datediff, current_timestamp,\n",
    "    substring, length,\n",
    "    expr, lit, coalesce, isnan, isnull,\n",
    "    collect_list, collect_set, array_contains,\n",
    "    unix_timestamp, from_unixtime, lag, lead\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "\n",
    "# Initialize data provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "print(\"‚úÖ Advanced threat hunting libraries imported\")\n",
    "\n",
    "# üîÑ WORKSPACE CONFIGURATION\n",
    "# ===========================================\n",
    "# üéØ SIMPLE SETUP: Copy the workspace names from your setup notebook!\n",
    "\n",
    "PRIMARY_WORKSPACE = \"ak-SecOps\"      # üè¢ Copy your primary workspace name here\n",
    "ENTRA_WORKSPACE = \"default\"          # üîµ Copy your Entra workspace name here\n",
    "\n",
    "# Analysis configuration\n",
    "ANALYSIS_HOURS = 24                  # üìÖ Hours of data to analyze\n",
    "SENTINEL_ENVIRONMENT = True          # üîç Enable advanced analysis features\n",
    "\n",
    "# Advanced settings (optional - can leave as defaults)\n",
    "WORKSPACE_MAPPING = {\n",
    "    'SigninLogs': PRIMARY_WORKSPACE,\n",
    "    'DeviceEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceProcessEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceNetworkEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceFileEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceInfo': PRIMARY_WORKSPACE,\n",
    "    'SecurityEvent': PRIMARY_WORKSPACE,\n",
    "    'CommonSecurityLog': PRIMARY_WORKSPACE,\n",
    "    'AADNonInteractiveUserSignInLogs': PRIMARY_WORKSPACE,\n",
    "    'AADServicePrincipalSignInLogs': PRIMARY_WORKSPACE,\n",
    "    'AADManagedIdentitySignInLogs': PRIMARY_WORKSPACE,\n",
    "    'AuditLogs': PRIMARY_WORKSPACE,\n",
    "    'EntraUsers': ENTRA_WORKSPACE,\n",
    "    'EntraGroups': ENTRA_WORKSPACE,\n",
    "    'EntraApplications': ENTRA_WORKSPACE,\n",
    "    'EntraServicePrincipals': ENTRA_WORKSPACE,\n",
    "    'EntraGroupMemberships': ENTRA_WORKSPACE,\n",
    "    'EntraMembers': ENTRA_WORKSPACE,\n",
    "    'EntraOrganizations': ENTRA_WORKSPACE\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ ADVANCED THREAT HUNTING CONFIGURATION:\")\n",
    "print(f\"üè¢ Primary workspace: '{PRIMARY_WORKSPACE}'\")\n",
    "print(f\"üîµ Entra workspace: '{ENTRA_WORKSPACE}'\")\n",
    "print(f\"\udcc5 Analysis window: {ANALYSIS_HOURS} hours\")\n",
    "\n",
    "# Configuration validation\n",
    "if PRIMARY_WORKSPACE == \"YOUR_WORKSPACE_NAME_HERE\":\n",
    "    print(f\"\\n‚ö†Ô∏è  CONFIGURATION NEEDED!\")\n",
    "    print(f\"üìù Please update the workspace names above:\")\n",
    "    print(f\"   1. Run 01_Setup_and_Configuration.ipynb first\")\n",
    "    print(f\"   2. Copy the discovered workspace names\")\n",
    "    print(f\"   3. Update PRIMARY_WORKSPACE and ENTRA_WORKSPACE above\")\n",
    "    print(f\"   4. Re-run this cell\")\n",
    "elif PRIMARY_WORKSPACE == \"test-workspace\":\n",
    "    print(f\"\\n‚úÖ DEMO MODE: Using test configuration\")\n",
    "    print(f\"üí° Configuration system is working correctly!\")\n",
    "    print(f\"üìù For real analysis, replace with your actual workspace names\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Configuration looks good!\")\n",
    "    print(f\"\ude80 Ready for advanced threat hunting in your environment\")\n",
    "    \n",
    "    # Show workspace mapping\n",
    "    print(f\"\\nüìä Table-to-workspace mapping:\")\n",
    "    for table, workspace in WORKSPACE_MAPPING.items():\n",
    "        print(f\"   ‚Ä¢ {table} ‚Üí {workspace}\")\n",
    "\n",
    "print(f\"\\nüéØ ADVANCED THREAT HUNTING READY!\")\n",
    "print(f\"üîç This notebook will perform sophisticated multi-source threat detection\")\n",
    "\n",
    "# Helper function for safe table checking using discovered mapping\n",
    "def safe_table_check(table_name, workspace_name=None):\n",
    "    \"\"\"Safely check table availability using workspace mapping\"\"\"\n",
    "    try:\n",
    "        # Use workspace mapping from configuration\n",
    "        if workspace_name is None:\n",
    "            workspace_name = WORKSPACE_MAPPING.get(table_name, PRIMARY_WORKSPACE)\n",
    "        \n",
    "        df = data_provider.read_table(table_name, workspace_name)\n",
    "        \n",
    "        # Get basic stats with small sample for performance\n",
    "        sample_count = df.limit(100).count()\n",
    "        columns = df.columns\n",
    "        \n",
    "        return {\n",
    "            'available': True,\n",
    "            'sample_rows': sample_count,\n",
    "            'total_columns': len(columns),\n",
    "            'columns': columns[:5],  # Show first 5 columns\n",
    "            'workspace': workspace_name,\n",
    "            'error': None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'available': False,\n",
    "            'sample_rows': 0,\n",
    "            'total_columns': 0,\n",
    "            'columns': [],\n",
    "            'workspace': workspace_name,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Quick verification of key threat hunting data sources\n",
    "print(f\"\\nüîç VERIFYING THREAT HUNTING DATA SOURCES...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test key tables across different data source types\n",
    "threat_hunting_tables = [\n",
    "    \"SigninLogs\",           # Identity data\n",
    "    \"DeviceEvents\",         # Endpoint data  \n",
    "    \"DeviceProcessEvents\",  # Process data\n",
    "    \"DeviceNetworkEvents\",  # Network data\n",
    "    \"SecurityEvent\",        # Windows events\n",
    "    \"CommonSecurityLog\"     # Network security devices\n",
    "]\n",
    "\n",
    "accessible_hunting_tables = []\n",
    "\n",
    "for table in threat_hunting_tables:\n",
    "    result = safe_table_check(table)\n",
    "    if result['available']:\n",
    "        accessible_hunting_tables.append(table)\n",
    "        print(f\"‚úÖ {table}: {result['sample_rows']} sample rows in '{result['workspace']}'\")\n",
    "    else:\n",
    "        print(f\"‚ùå {table}: Not accessible ({result['error'][:50]}...)\")\n",
    "\n",
    "print(f\"\\nüìä THREAT HUNTING DATA AVAILABILITY:\")\n",
    "print(f\"   ‚úÖ Accessible data sources: {len(accessible_hunting_tables)}/{len(threat_hunting_tables)}\")\n",
    "\n",
    "if accessible_hunting_tables:\n",
    "    print(f\"   üìã Ready for hunting: {', '.join(accessible_hunting_tables)}\")\n",
    "    print(f\"\\nüöÄ Ready for advanced threat hunting!\")\n",
    "    \n",
    "    # Categorize available data types\n",
    "    identity_data = [t for t in accessible_hunting_tables if 'signin' in t.lower()]\n",
    "    endpoint_data = [t for t in accessible_hunting_tables if 'device' in t.lower()]\n",
    "    security_data = [t for t in accessible_hunting_tables if 'security' in t.lower()]\n",
    "    \n",
    "    print(f\"\\nüìà DATA SOURCE CATEGORIES:\")\n",
    "    if identity_data:\n",
    "        print(f\"   üîê Identity: {', '.join(identity_data)}\")\n",
    "    if endpoint_data:\n",
    "        print(f\"   üíª Endpoint: {', '.join(endpoint_data)}\")\n",
    "    if security_data:\n",
    "        print(f\"   üõ°Ô∏è  Security: {', '.join(security_data)}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Limited threat hunting data available\")\n",
    "    print(f\"   üí° Advanced threat hunting works best with multiple data sources\")\n",
    "    print(f\"   üìù The analysis sections will adapt to available data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5080e2",
   "metadata": {},
   "source": [
    "## 1. Command and Control (C2) Beacon Detection\n",
    "\n",
    "Advanced detection of C2 communications using statistical analysis of network patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c20ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced C2 beacon detection\n",
    "def detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detect potential C2 beacons using advanced statistical analysis\n",
    "    \"\"\"\n",
    "    print(\"üîç ANALYZING C2 BEACON PATTERNS...\\n\")\n",
    "    \n",
    "    # First, let's see what columns are available\n",
    "    print(\"üìä Available columns in DeviceNetworkEvents:\")\n",
    "    print(network_df.columns)\n",
    "    \n",
    "    # Group connections by source and destination with time bucketing\n",
    "    # Note: Using connection frequency instead of byte counts for beacon detection\n",
    "    beacon_analysis = network_df.withColumn(\n",
    "        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n",
    "    ).groupBy(\n",
    "        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n",
    "    ).agg(\n",
    "        spark_count(\"*\").alias(\"ConnectionCount\")\n",
    "    )\n",
    "    \n",
    "    # Calculate beacon characteristics per connection pair\n",
    "    beacon_stats = beacon_analysis.groupBy(\n",
    "        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n",
    "    ).agg(\n",
    "        spark_count(\"*\").alias(\"TimeWindows\"),\n",
    "        avg(\"ConnectionCount\").alias(\"AvgConnectionsPerHour\"),\n",
    "        stddev(\"ConnectionCount\").alias(\"StdDevConnections\")\n",
    "    ).filter(\n",
    "        col(\"TimeWindows\") >= min_connections\n",
    "    )\n",
    "    \n",
    "    # Identify potential beacons (low jitter, consistent timing)\n",
    "    potential_beacons = beacon_stats.withColumn(\n",
    "        \"ConnectionJitter\", \n",
    "        coalesce(col(\"StdDevConnections\") / col(\"AvgConnectionsPerHour\"), lit(999))\n",
    "    ).withColumn(\n",
    "        \"BeaconScore\",\n",
    "        when(col(\"ConnectionJitter\") <= jitter_threshold, 100)\n",
    "        .when(col(\"ConnectionJitter\") <= 0.3, 75)\n",
    "        .when(col(\"ConnectionJitter\") <= 0.5, 50)\n",
    "        .otherwise(25)\n",
    "    ).filter(\n",
    "        col(\"BeaconScore\") >= 50\n",
    "    ).orderBy(desc(\"BeaconScore\"), desc(\"TimeWindows\"))\n",
    "    \n",
    "    return potential_beacons\n",
    "\n",
    "# Load network data for C2 analysis\n",
    "try:\n",
    "    if SENTINEL_ENVIRONMENT:\n",
    "        # Use safe_table_check to get proper workspace\n",
    "        table_info = safe_table_check(\"DeviceNetworkEvents\")\n",
    "        if table_info['available']:\n",
    "            network_events = data_provider.read_table(\"DeviceNetworkEvents\", table_info['workspace'])\n",
    "            print(f\"‚úÖ Loaded DeviceNetworkEvents from {table_info['workspace']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå DeviceNetworkEvents not available: {table_info['error']}\")\n",
    "            network_events = None\n",
    "    else:\n",
    "        network_events = None\n",
    "    \n",
    "    # Filter to analysis window and external connections\n",
    "    if network_events is not None:\n",
    "        network_filtered = network_events.filter(\n",
    "            col(\"Timestamp\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "        ).filter(\n",
    "            # Focus on external connections (not internal RFC1918)\n",
    "            ~col(\"RemoteIP\").rlike(r\"^(10\\.|192\\.168\\.|172\\.(1[6-9]|2[0-9]|3[0-1])\\.).*$\")\n",
    "        ).filter(\n",
    "            # Filter out common legitimate traffic\n",
    "            ~col(\"RemotePort\").isin([80, 443, 53, 123])  # HTTP, HTTPS, DNS, NTP\n",
    "        )\n",
    "        \n",
    "        # Detect C2 beacons\n",
    "        c2_beacons = detect_c2_beacons(network_filtered)\n",
    "        \n",
    "        beacon_count = c2_beacons.count()\n",
    "    else:\n",
    "        print(\"‚ùå Network events not available - skipping C2 analysis\")\n",
    "        beacon_count = 0\n",
    "        network_filtered = None\n",
    "    \n",
    "    if beacon_count > 0:\n",
    "        print(f\"üö® POTENTIAL C2 BEACONS DETECTED: {beacon_count}\\n\")\n",
    "        \n",
    "        c2_beacons.select(\n",
    "            \"DeviceName\", \"RemoteIP\", \"RemotePort\", \n",
    "            \"BeaconScore\", \"TimeWindows\", \"AvgConnectionsPerHour\", \"ConnectionJitter\"\n",
    "        ).show(20, truncate=False)\n",
    "        \n",
    "        # Analyze beacon timing patterns\n",
    "        high_confidence_beacons = c2_beacons.filter(col(\"BeaconScore\") >= 75)\n",
    "        \n",
    "        if high_confidence_beacons.count() > 0:\n",
    "            print(\"\\nüî• HIGH CONFIDENCE C2 BEACONS:\")\n",
    "            high_confidence_beacons.show(10, truncate=False)\n",
    "            \n",
    "            print(\"\\nüö® IMMEDIATE ACTIONS REQUIRED:\")\n",
    "            print(\"   1. Block identified C2 IPs at firewall\")\n",
    "            print(\"   2. Isolate affected devices immediately\")\n",
    "            print(\"   3. Analyze malware samples on affected systems\")\n",
    "            print(\"   4. Search for related IOCs across environment\")\n",
    "            print(\"   5. Review user accounts on affected devices\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚úÖ No obvious C2 beacon patterns detected\")\n",
    "        \n",
    "        # Show some network statistics (only if network_filtered is available)\n",
    "        if 'network_filtered' in locals() and network_filtered is not None:\n",
    "            external_connections = network_filtered.count()\n",
    "            unique_destinations = network_filtered.select(\"RemoteIP\").distinct().count()\n",
    "            \n",
    "            print(f\"üìä External Network Analysis Summary:\")\n",
    "            print(f\"   External Connections: {external_connections:,}\")\n",
    "            print(f\"   Unique Destinations: {unique_destinations:,}\")\n",
    "        else:\n",
    "            print(\"üìä Network statistics not available\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error analyzing network data: {str(e)}\")\n",
    "    print(\"   DeviceNetworkEvents may not be available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fc38e",
   "metadata": {},
   "source": [
    "## 2. Living off the Land (LotL) Detection\n",
    "\n",
    "Detect abuse of legitimate system tools for malicious purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de6a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Living off the Land detection\n",
    "def detect_lotl_abuse(process_events):\n",
    "    \"\"\"\n",
    "    Detect abuse of legitimate tools (Living off the Land techniques)\n",
    "    \"\"\"\n",
    "    print(\"üé≠ DETECTING LIVING OFF THE LAND TECHNIQUES...\\n\")\n",
    "    \n",
    "    # Define suspicious usage patterns for legitimate tools\n",
    "    lotl_patterns = {\n",
    "        \"PowerShell Abuse\": {\n",
    "            \"process\": \"powershell|pwsh\",\n",
    "            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n",
    "        },\n",
    "        \"WMI Abuse\": {\n",
    "            \"process\": \"wmic|wmiprvse\",\n",
    "            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n",
    "        },\n",
    "        \"Certificate Abuse\": {\n",
    "            \"process\": \"certutil\",\n",
    "            \"cmdline\": \"urlcache|decode|encode|-f\"\n",
    "        },\n",
    "        \"BitsAdmin Abuse\": {\n",
    "            \"process\": \"bitsadmin\",\n",
    "            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n",
    "        },\n",
    "        \"RegSvr32 Abuse\": {\n",
    "            \"process\": \"regsvr32\",\n",
    "            \"cmdline\": \"/s.*http|/u.*http|scrobj\\.dll\"\n",
    "        },\n",
    "        \"MSBuild Abuse\": {\n",
    "            \"process\": \"msbuild\",\n",
    "            \"cmdline\": \"\\.xml|inline\"\n",
    "        },\n",
    "        \"WMIC Process Creation\": {\n",
    "            \"process\": \"wmic\",\n",
    "            \"cmdline\": \"process.*call.*create|/node:\"\n",
    "        },\n",
    "        \"Rundll32 Abuse\": {\n",
    "            \"process\": \"rundll32\",\n",
    "            \"cmdline\": \"javascript|vbscript|url\\.dll|shell32.*control_rundll\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    lotl_results = {}\n",
    "    \n",
    "    for technique, pattern in lotl_patterns.items():\n",
    "        lotl_detections = process_events.filter(\n",
    "            lower(col(\"FileName\")).rlike(pattern[\"process\"]) &\n",
    "            lower(col(\"ProcessCommandLine\")).rlike(pattern[\"cmdline\"])\n",
    "        )\n",
    "        \n",
    "        detection_count = lotl_detections.count()\n",
    "        \n",
    "        if detection_count > 0:\n",
    "            lotl_results[technique] = {\n",
    "                'count': detection_count,\n",
    "                'data': lotl_detections\n",
    "            }\n",
    "    \n",
    "    return lotl_results\n",
    "\n",
    "# Load and analyze process events for LotL\n",
    "try:\n",
    "    if SENTINEL_ENVIRONMENT:\n",
    "        # Use safe_table_check to get proper workspace\n",
    "        table_info = safe_table_check(\"DeviceProcessEvents\")\n",
    "        if table_info['available']:\n",
    "            process_events = data_provider.read_table(\"DeviceProcessEvents\", table_info['workspace'])\n",
    "            print(f\"‚úÖ Loaded DeviceProcessEvents from {table_info['workspace']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå DeviceProcessEvents not available: {table_info['error']}\")\n",
    "            process_events = None\n",
    "    else:\n",
    "        process_events = None\n",
    "    \n",
    "    # Filter to analysis window\n",
    "    if process_events is not None:\n",
    "        process_filtered = process_events.filter(\n",
    "            col(\"Timestamp\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "        )\n",
    "        \n",
    "        # Detect LotL techniques\n",
    "        lotl_results = detect_lotl_abuse(process_filtered)\n",
    "    else:\n",
    "        print(\"‚ùå Process events not available - skipping LotL analysis\")\n",
    "        lotl_results = {}\n",
    "    \n",
    "    if lotl_results:\n",
    "        print(f\"üé≠ LIVING OFF THE LAND TECHNIQUES DETECTED: {len(lotl_results)}\\n\")\n",
    "        \n",
    "        for technique, result in lotl_results.items():\n",
    "            print(f\"üîç {technique}: {result['count']} instances\")\n",
    "            \n",
    "            # Show top examples\n",
    "            technique_summary = result['data'].groupBy(\n",
    "                \"ProcessCommandLine\", \"AccountName\"\n",
    "            ).agg(\n",
    "                spark_count(\"*\").alias(\"Count\"),\n",
    "                countDistinct(\"DeviceName\").alias(\"UniqueDevices\")\n",
    "            ).orderBy(desc(\"Count\"))\n",
    "            \n",
    "            print(f\"   Top command patterns:\")\n",
    "            technique_summary.show(5, truncate=False)\n",
    "            print()\n",
    "        \n",
    "        # Overall LotL summary\n",
    "        total_lotl_events = sum(result['count'] for result in lotl_results.values())\n",
    "        \n",
    "        print(f\"üìä LIVING OFF THE LAND SUMMARY:\")\n",
    "        print(f\"   Total Suspicious Events: {total_lotl_events:,}\")\n",
    "        print(f\"   Techniques Detected: {len(lotl_results)}\")\n",
    "        \n",
    "        # Get affected users and devices\n",
    "        all_lotl_events = None\n",
    "        for result in lotl_results.values():\n",
    "            if all_lotl_events is None:\n",
    "                all_lotl_events = result['data']\n",
    "            else:\n",
    "                all_lotl_events = all_lotl_events.union(result['data'])\n",
    "        \n",
    "        affected_users = all_lotl_events.select(\"AccountName\").distinct().count()\n",
    "        affected_devices = all_lotl_events.select(\"DeviceName\").distinct().count()\n",
    "        \n",
    "        print(f\"   Affected Users: {affected_users}\")\n",
    "        print(f\"   Affected Devices: {affected_devices}\")\n",
    "        \n",
    "        print(\"\\nüõ°Ô∏è  LOTL MITIGATION RECOMMENDATIONS:\")\n",
    "        print(\"   1. Implement PowerShell logging and monitoring\")\n",
    "        print(\"   2. Restrict administrative tools usage\")\n",
    "        print(\"   3. Enable application whitelisting\")\n",
    "        print(\"   4. Monitor process creation events\")\n",
    "        print(\"   5. Train users on social engineering tactics\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚úÖ No Living off the Land techniques detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error analyzing process events: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992e3ea",
   "metadata": {},
   "source": [
    "## 3. Advanced Persistence Detection\n",
    "\n",
    "Detect sophisticated persistence mechanisms used by advanced threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998f22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced persistence detection\n",
    "def detect_persistence_mechanisms(process_events, registry_events=None):\n",
    "    \"\"\"\n",
    "    Detect various persistence mechanisms\n",
    "    \"\"\"\n",
    "    print(\"üîÑ DETECTING PERSISTENCE MECHANISMS...\\n\")\n",
    "    \n",
    "    persistence_indicators = []\n",
    "    \n",
    "    # 1. Scheduled task creation/modification\n",
    "    schtasks_persistence = process_events.filter(\n",
    "        lower(col(\"FileName\")).rlike(\"schtasks|at\\\\.exe\") &\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n",
    "    )\n",
    "    \n",
    "    schtasks_count = schtasks_persistence.count()\n",
    "    if schtasks_count > 0:\n",
    "        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n",
    "    \n",
    "    # 2. Service creation/modification\n",
    "    service_persistence = process_events.filter(\n",
    "        lower(col(\"FileName\")).rlike(\"sc\\\\.exe|net\\\\.exe\") &\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n",
    "    )\n",
    "    \n",
    "    service_count = service_persistence.count()\n",
    "    if service_count > 0:\n",
    "        persistence_indicators.append((\"Service Persistence\", service_count, service_persistence))\n",
    "    \n",
    "    # 3. Registry-based persistence (RunKey modifications)\n",
    "    run_key_persistence = process_events.filter(\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"reg.*add.*run|reg.*add.*runonce\")\n",
    "    )\n",
    "    \n",
    "    run_key_count = run_key_persistence.count()\n",
    "    if run_key_count > 0:\n",
    "        persistence_indicators.append((\"Registry Run Keys\", run_key_count, run_key_persistence))\n",
    "    \n",
    "    # 4. DLL hijacking indicators (unusual DLL loads)\n",
    "    dll_hijacking = process_events.filter(\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"rundll32|regsvr32\") &\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"appdata|temp|public\")\n",
    "    )\n",
    "    \n",
    "    dll_count = dll_hijacking.count()\n",
    "    if dll_count > 0:\n",
    "        persistence_indicators.append((\"DLL Hijacking\", dll_count, dll_hijacking))\n",
    "    \n",
    "    # 5. WMI event subscription persistence\n",
    "    wmi_persistence = process_events.filter(\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"wmic.*eventfilter|wmic.*consumer|wmic.*subscription\")\n",
    "    )\n",
    "    \n",
    "    wmi_count = wmi_persistence.count()\n",
    "    if wmi_count > 0:\n",
    "        persistence_indicators.append((\"WMI Persistence\", wmi_count, wmi_persistence))\n",
    "    \n",
    "    return persistence_indicators\n",
    "\n",
    "# Analyze persistence mechanisms\n",
    "persistence_results = detect_persistence_mechanisms(process_filtered) if 'process_filtered' in locals() else []\n",
    "\n",
    "if persistence_results:\n",
    "    print(f\"üîÑ PERSISTENCE MECHANISMS DETECTED: {len(persistence_results)}\\n\")\n",
    "    \n",
    "    for mechanism, mechanism_count, data in persistence_results:\n",
    "        print(f\"üéØ {mechanism}: {mechanism_count} instances\")\n",
    "        \n",
    "        # Show details for each mechanism\n",
    "        mechanism_details = data.groupBy(\n",
    "            \"ProcessCommandLine\", \"AccountName\", \"DeviceName\"\n",
    "        ).agg(\n",
    "            spark_count(\"*\").alias(\"Count\")\n",
    "        ).orderBy(desc(\"Count\"))\n",
    "        \n",
    "        print(\"   Command examples:\")\n",
    "        mechanism_details.show(3, truncate=False)\n",
    "        print()\n",
    "    \n",
    "    # Risk assessment\n",
    "    total_persistence = sum(mechanism_count for _, mechanism_count, _ in persistence_results)\n",
    "    risk_level = \"HIGH\" if total_persistence >= 10 else \"MEDIUM\" if total_persistence >= 5 else \"LOW\"\n",
    "    \n",
    "    print(f\"üõ°Ô∏è  PERSISTENCE RISK LEVEL: {risk_level}\")\n",
    "    print(f\"   Total Indicators: {total_persistence}\")\n",
    "    \n",
    "    if risk_level in [\"HIGH\", \"MEDIUM\"]:\n",
    "        print(\"\\nüö® IMMEDIATE ACTIONS REQUIRED:\")\n",
    "        print(\"   1. Review all scheduled tasks and services\")\n",
    "        print(\"   2. Check registry run keys for unauthorized entries\")\n",
    "        print(\"   3. Validate DLL authenticity and locations\")\n",
    "        print(\"   4. Audit WMI subscriptions and filters\")\n",
    "        print(\"   5. Implement application allowlisting\")\n",
    "        print(\"   6. Monitor for lateral movement patterns\")\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ No obvious persistence mechanisms detected\")\n",
    "    print(\"   Continue monitoring for sophisticated techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36dabf",
   "metadata": {},
   "source": [
    "## 4. Data Exfiltration Pattern Analysis\n",
    "\n",
    "Detect sophisticated data exfiltration patterns combining multiple indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fc7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced data exfiltration detection\n",
    "def detect_data_exfiltration_patterns():\n",
    "    \"\"\"\n",
    "    Detect sophisticated data exfiltration using multiple data sources\n",
    "    \"\"\"\n",
    "    print(\"üì§ ANALYZING DATA EXFILTRATION PATTERNS...\\n\")\n",
    "    \n",
    "    exfiltration_indicators = []\n",
    "    \n",
    "    try:\n",
    "        # 1. Large external data transfers - NOTE: Adapted for available columns\n",
    "        if 'network_filtered' in locals() and network_filtered is not None:\n",
    "            # Focus on connection frequency to external IPs as a proxy for data transfers\n",
    "            external_connections = network_filtered.groupBy(\n",
    "                \"DeviceName\", \"RemoteIP\"\n",
    "            ).agg(\n",
    "                spark_count(\"*\").alias(\"ConnectionCount\")\n",
    "            ).filter(col(\"ConnectionCount\") > 100)  # High connection frequency\n",
    "            \n",
    "            high_connection_count = external_connections.count()\n",
    "            if high_connection_count > 0:\n",
    "                exfiltration_indicators.append((\"High External Connection Frequency\", high_connection_count, external_connections))\n",
    "        \n",
    "        # 2. Archive creation before external transfers\n",
    "        if 'process_filtered' in locals():\n",
    "            archive_creation = process_filtered.filter(\n",
    "                lower(col(\"ProcessCommandLine\")).rlike(\n",
    "                    \"7z.*a |winrar.*a |zip.*-r|tar.*-czf|makecab\"\n",
    "                ) &\n",
    "                lower(col(\"ProcessCommandLine\")).rlike(\n",
    "                    \"documents|desktop|users|programdata|temp\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            archive_count = archive_creation.count()\n",
    "            if archive_count > 0:\n",
    "                exfiltration_indicators.append((\"Suspicious Archive Creation\", archive_count, archive_creation))\n",
    "        \n",
    "        # 3. Cloud storage tool usage\n",
    "        if 'process_filtered' in locals():\n",
    "            cloud_tools = process_filtered.filter(\n",
    "                lower(col(\"FileName\")).rlike(\n",
    "                    \"rclone|aws|gsutil|azcopy|dropbox|googledrive\"\n",
    "                ) |\n",
    "                lower(col(\"ProcessCommandLine\")).rlike(\n",
    "                    \"s3.*cp|blob.*upload|drive.*upload|dropbox.*upload\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            cloud_count = cloud_tools.count()\n",
    "            if cloud_count > 0:\n",
    "                exfiltration_indicators.append((\"Cloud Storage Tools\", cloud_count, cloud_tools))\n",
    "        \n",
    "        # 4. Encoded/encrypted data preparation\n",
    "        if 'process_filtered' in locals():\n",
    "            encoding_activities = process_filtered.filter(\n",
    "                lower(col(\"ProcessCommandLine\")).rlike(\n",
    "                    \"base64|certutil.*encode|openssl.*enc|gpg.*encrypt\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            encoding_count = encoding_activities.count()\n",
    "            if encoding_count > 0:\n",
    "                exfiltration_indicators.append((\"Data Encoding/Encryption\", encoding_count, encoding_activities))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error in exfiltration analysis: {str(e)}\")\n",
    "    \n",
    "    return exfiltration_indicators\n",
    "\n",
    "# Detect data exfiltration patterns\n",
    "exfiltration_results = detect_data_exfiltration_patterns()\n",
    "\n",
    "if exfiltration_results:\n",
    "    print(f\"üì§ DATA EXFILTRATION INDICATORS DETECTED: {len(exfiltration_results)}\\n\")\n",
    "    \n",
    "    for indicator, count, data in exfiltration_results:\n",
    "        print(f\"üö® {indicator}: {count} instances\")\n",
    "        \n",
    "        # Show relevant details based on indicator type\n",
    "        if \"Connection\" in indicator:\n",
    "            data.select(\n",
    "                \"DeviceName\", \"RemoteIP\", \"ConnectionCount\"\n",
    "            ).show(5, truncate=False)\n",
    "        else:\n",
    "            summary = data.groupBy(\"ProcessCommandLine\", \"AccountName\").agg(\n",
    "                spark_count(\"*\").alias(\"Count\"),\n",
    "                countDistinct(\"DeviceName\").alias(\"UniqueDevices\")\n",
    "            ).orderBy(desc(\"Count\"))\n",
    "            \n",
    "            summary.show(3, truncate=False)\n",
    "        print()\n",
    "    \n",
    "    # Risk assessment\n",
    "    total_indicators = len(exfiltration_results)\n",
    "    risk_level = \"HIGH\" if total_indicators >= 3 else \"MEDIUM\" if total_indicators >= 2 else \"LOW\"\n",
    "    \n",
    "    print(f\"üéØ EXFILTRATION RISK LEVEL: {risk_level}\")\n",
    "    print(f\"   Total Indicators: {total_indicators}\")\n",
    "    \n",
    "    if risk_level in [\"HIGH\", \"MEDIUM\"]:\n",
    "        print(\"\\nüö® IMMEDIATE ACTIONS REQUIRED:\")\n",
    "        print(\"   1. Investigate all flagged activities immediately\")\n",
    "        print(\"   2. Review data access logs for affected users\")\n",
    "        print(\"   3. Check for unauthorized cloud storage usage\")\n",
    "        print(\"   4. Validate legitimate business purposes\")\n",
    "        print(\"   5. Consider temporary network restrictions\")\n",
    "        print(\"   6. Implement DLP policies if not already present\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚úÖ No obvious data exfiltration patterns detected\")\n",
    "    print(\"   Continue monitoring for suspicious data movement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0348d9e",
   "metadata": {},
   "source": [
    "## 5. User Behavior Analytics (UBA)\n",
    "\n",
    "Advanced behavioral analysis to detect anomalous user activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a759c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Behavior Analytics\n",
    "def perform_user_behavior_analysis():\n",
    "    \"\"\"\n",
    "    Perform advanced user behavior analysis across multiple data sources\n",
    "    \"\"\"\n",
    "    print(\"üë§ USER BEHAVIOR ANALYTICS...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load sign-in data for behavioral analysis using smart table checking\n",
    "        if SENTINEL_ENVIRONMENT:\n",
    "            table_info = safe_table_check(\"SigninLogs\")\n",
    "            if table_info['available']:\n",
    "                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n",
    "                print(f\"‚úÖ Loaded SigninLogs from {table_info['workspace']}\")\n",
    "            else:\n",
    "                print(f\"‚ùå SigninLogs not available: {table_info['error']}\")\n",
    "                signin_logs = None\n",
    "        else:\n",
    "            signin_logs = None\n",
    "        \n",
    "        if signin_logs is None:\n",
    "            print(\"‚ö†Ô∏è SigninLogs not available - skipping user behavior analysis\")\n",
    "            return\n",
    "        \n",
    "        # Filter to analysis window\n",
    "        signin_filtered = signin_logs.filter(\n",
    "            col(\"CreatedDateTime\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS*2} HOURS\"))  # Longer window for baseline\n",
    "        )\n",
    "        \n",
    "        # 1. Unusual time-based patterns\n",
    "        user_time_patterns = signin_filtered.withColumn(\n",
    "            \"HourOfDay\", hour(col(\"CreatedDateTime\"))\n",
    "        ).withColumn(\n",
    "            \"DayOfWeek\", dayofweek(col(\"CreatedDateTime\"))\n",
    "        ).groupBy(\n",
    "            \"UserPrincipalName\", \"HourOfDay\", \"DayOfWeek\"\n",
    "        ).agg(\n",
    "            spark_count(\"*\").alias(\"SignInCount\")\n",
    "        )\n",
    "        \n",
    "        # Calculate user's normal hours (hours with >10% of their activity)\n",
    "        user_total_signins = user_time_patterns.groupBy(\"UserPrincipalName\").agg(\n",
    "            spark_sum(\"SignInCount\").alias(\"TotalSignIns\")\n",
    "        )\n",
    "        \n",
    "        user_normal_hours = user_time_patterns.join(\n",
    "            user_total_signins, \"UserPrincipalName\"\n",
    "        ).withColumn(\n",
    "            \"ActivityPercentage\", col(\"SignInCount\") / col(\"TotalSignIns\")\n",
    "        ).filter(\n",
    "            col(\"ActivityPercentage\") > 0.05  # Hours with >5% of activity\n",
    "        )\n",
    "        \n",
    "        # Find recent sign-ins outside normal patterns\n",
    "        recent_signins = signin_filtered.filter(\n",
    "            col(\"CreatedDateTime\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "        ).withColumn(\n",
    "            \"HourOfDay\", hour(col(\"CreatedDateTime\"))\n",
    "        ).withColumn(\n",
    "            \"DayOfWeek\", dayofweek(col(\"CreatedDateTime\"))\n",
    "        )\n",
    "        \n",
    "        # Anti-join to find sign-ins outside normal patterns\n",
    "        anomalous_time_signins = recent_signins.join(\n",
    "            user_normal_hours.select(\"UserPrincipalName\", \"HourOfDay\", \"DayOfWeek\"),\n",
    "            [\"UserPrincipalName\", \"HourOfDay\", \"DayOfWeek\"],\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        \n",
    "        anomalous_time_count = anomalous_time_signins.count()\n",
    "        \n",
    "        if anomalous_time_count > 0:\n",
    "            print(f\"‚è∞ ANOMALOUS TIME-BASED SIGN-INS: {anomalous_time_count}\")\n",
    "            \n",
    "            time_anomalies = anomalous_time_signins.groupBy(\n",
    "                \"UserPrincipalName\", \"UserDisplayName\"\n",
    "            ).agg(\n",
    "                spark_count(\"*\").alias(\"AnomalousSignIns\"),\n",
    "                countDistinct(\"IPAddress\").alias(\"UniqueIPs\")\n",
    "            ).orderBy(desc(\"AnomalousSignIns\"))\n",
    "            \n",
    "            time_anomalies.show(10, truncate=False)\n",
    "        \n",
    "        # 2. Geographic anomalies\n",
    "        if signin_filtered.filter(col(\"LocationDetails\").isNotNull()).count() > 0:\n",
    "            location_schema = StructType([\n",
    "                StructField(\"countryOrRegion\", StringType(), True)\n",
    "            ])\n",
    "            \n",
    "            user_locations = signin_filtered.filter(\n",
    "                col(\"LocationDetails\").isNotNull()\n",
    "            ).withColumn(\n",
    "                \"Country\", from_json(col(\"LocationDetails\"), location_schema).getField(\"countryOrRegion\")\n",
    "            )\n",
    "            \n",
    "            # Baseline countries for each user\n",
    "            baseline_countries = user_locations.filter(\n",
    "                col(\"CreatedDateTime\") < (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "            ).select(\"UserPrincipalName\", \"Country\", \"UserDisplayName\").distinct()\n",
    "            \n",
    "            # Recent countries\n",
    "            recent_countries = user_locations.filter(\n",
    "                col(\"CreatedDateTime\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "            ).select(\"UserPrincipalName\", \"Country\", \"UserDisplayName\").distinct()\n",
    "            \n",
    "            new_country_signins = recent_countries.join(\n",
    "                baseline_countries,\n",
    "                [\"UserPrincipalName\", \"Country\"],\n",
    "                \"left_anti\"\n",
    "            )\n",
    "            \n",
    "            new_country_count = new_country_signins.count()\n",
    "            \n",
    "            if new_country_count > 0:\n",
    "                print(f\"\\nüåç NEW COUNTRY SIGN-INS: {new_country_count}\")\n",
    "                new_country_signins.show(10, truncate=False)\n",
    "        \n",
    "        # 3. Application usage anomalies\n",
    "        app_anomalies = recent_signins.filter(\n",
    "            col(\"AppDisplayName\").isNotNull()\n",
    "        ).groupBy(\n",
    "            \"UserPrincipalName\", \"AppDisplayName\"\n",
    "        ).agg(\n",
    "            spark_count(\"*\").alias(\"RecentUsage\")\n",
    "        )\n",
    "        \n",
    "        # Find apps used in baseline period\n",
    "        baseline_app_usage = signin_filtered.filter(\n",
    "            col(\"CreatedDateTime\") < (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "        ).filter(\n",
    "            col(\"AppDisplayName\").isNotNull()\n",
    "        ).groupBy(\n",
    "            \"UserPrincipalName\", \"AppDisplayName\"\n",
    "        ).agg(\n",
    "            spark_count(\"*\").alias(\"BaselineUsage\")\n",
    "        )\n",
    "        \n",
    "        app_usage_anomalies = app_anomalies.join(\n",
    "            baseline_app_usage,\n",
    "            [\"UserPrincipalName\", \"AppDisplayName\"],\n",
    "            \"left\"\n",
    "        ).filter(\n",
    "            col(\"RecentUsage\") > (col(\"BaselineUsage\") * 3)  # 3x normal usage\n",
    "        ).filter(\n",
    "            col(\"RecentUsage\") > 5  # At least 5 sign-ins\n",
    "        )\n",
    "        \n",
    "        app_anomaly_count = app_usage_anomalies.count()\n",
    "        \n",
    "        if app_anomaly_count > 0:\n",
    "            print(f\"\\nüì± APPLICATION USAGE ANOMALIES: {app_anomaly_count}\")\n",
    "            app_usage_anomalies.orderBy(desc(\"RecentUsage\")).show(10, truncate=False)\n",
    "        \n",
    "        # Summary\n",
    "        total_anomalies = sum([\n",
    "            anomalous_time_count if 'anomalous_time_count' in locals() else 0,\n",
    "            new_country_count if 'new_country_count' in locals() else 0,\n",
    "            app_anomaly_count if 'app_anomaly_count' in locals() else 0\n",
    "        ])\n",
    "        \n",
    "        if total_anomalies > 0:\n",
    "            print(f\"\\nüéØ USER BEHAVIOR ANALYSIS SUMMARY:\")\n",
    "            print(f\"   Total Behavioral Anomalies: {total_anomalies}\")\n",
    "            print(\"\\nüîç INVESTIGATION PRIORITIES:\")\n",
    "            print(\"   1. Review users with multiple anomaly types\")\n",
    "            print(\"   2. Correlate with recent security events\")\n",
    "            print(\"   3. Verify account compromise indicators\")\n",
    "            print(\"   4. Check for privilege escalation attempts\")\n",
    "            print(\"   5. Monitor for additional suspicious activities\")\n",
    "        else:\n",
    "            print(\"‚úÖ No significant user behavior anomalies detected\")\n",
    "        \n",
    "        # Return total anomalies for summary\n",
    "        return total_anomalies\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error in user behavior analysis: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# Perform user behavior analysis\n",
    "total_anomalies = perform_user_behavior_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940c568",
   "metadata": {},
   "source": [
    "## 6. Advanced Threat Hunting Summary\n",
    "\n",
    "Comprehensive summary and threat assessment based on all analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e026ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive threat hunting summary\n",
    "print(\"üéØ ADVANCED THREAT HUNTING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all findings\n",
    "threat_findings = {}\n",
    "\n",
    "if 'beacon_count' in locals():\n",
    "    threat_findings['C2 Beacons'] = beacon_count\n",
    "if 'lotl_results' in locals():\n",
    "    threat_findings['Living off the Land'] = len(lotl_results)\n",
    "if 'persistence_results' in locals():\n",
    "    threat_findings['Persistence Mechanisms'] = len(persistence_results)\n",
    "if 'exfiltration_results' in locals():\n",
    "    threat_findings['Data Exfiltration Indicators'] = len(exfiltration_results)\n",
    "if 'total_anomalies' in locals():\n",
    "    threat_findings['Behavioral Anomalies'] = total_anomalies\n",
    "\n",
    "# Display findings\n",
    "print(f\"üìä THREAT HUNTING RESULTS ({ANALYSIS_HOURS}-hour analysis):\")\n",
    "for finding, count in threat_findings.items():\n",
    "    status = \"üö®\" if count > 0 else \"‚úÖ\"\n",
    "    print(f\"   {status} {finding}: {count}\")\n",
    "\n",
    "# Calculate overall threat score\n",
    "threat_score = 0\n",
    "critical_findings = 0\n",
    "\n",
    "# Weight different finding types\n",
    "if threat_findings.get('C2 Beacons', 0) > 0:\n",
    "    threat_score += 40\n",
    "    critical_findings += 1\n",
    "if threat_findings.get('Data Exfiltration Indicators', 0) >= 2:\n",
    "    threat_score += 30\n",
    "    critical_findings += 1\n",
    "if threat_findings.get('Living off the Land', 0) >= 3:\n",
    "    threat_score += 20\n",
    "if threat_findings.get('Persistence Mechanisms', 0) >= 2:\n",
    "    threat_score += 15\n",
    "if threat_findings.get('Behavioral Anomalies', 0) >= 5:\n",
    "    threat_score += 10\n",
    "\n",
    "# Determine threat level\n",
    "if threat_score >= 50:\n",
    "    threat_level = \"CRITICAL\"\n",
    "    color = \"üî¥\"\n",
    "elif threat_score >= 30:\n",
    "    threat_level = \"HIGH\"\n",
    "    color = \"üü†\"\n",
    "elif threat_score >= 15:\n",
    "    threat_level = \"MEDIUM\"\n",
    "    color = \"üü°\"\n",
    "else:\n",
    "    threat_level = \"LOW\"\n",
    "    color = \"üü¢\"\n",
    "\n",
    "print(f\"\\n{color} OVERALL THREAT LEVEL: {threat_level} (Score: {threat_score}/100)\")\n",
    "\n",
    "# Provide specific recommendations based on findings\n",
    "print(f\"\\nüéØ THREAT-SPECIFIC RECOMMENDATIONS:\")\n",
    "\n",
    "if threat_level == \"CRITICAL\":\n",
    "    print(\"   üö® IMMEDIATE INCIDENT RESPONSE REQUIRED\")\n",
    "    print(\"   1. Activate incident response team\")\n",
    "    print(\"   2. Isolate affected systems immediately\")\n",
    "    print(\"   3. Preserve evidence for forensic analysis\")\n",
    "    print(\"   4. Reset credentials for affected accounts\")\n",
    "    print(\"   5. Implement emergency containment measures\")\n",
    "    print(\"   6. Contact legal and compliance teams\")\n",
    "    print(\"   7. Prepare external communications if needed\")\n",
    "\n",
    "elif threat_level == \"HIGH\":\n",
    "    print(\"   ‚ö†Ô∏è  ELEVATED THREAT - IMMEDIATE INVESTIGATION\")\n",
    "    print(\"   1. Begin detailed investigation of all findings\")\n",
    "    print(\"   2. Implement enhanced monitoring\")\n",
    "    print(\"   3. Consider network segmentation\")\n",
    "    print(\"   4. Review and update security policies\")\n",
    "    print(\"   5. Increase security team alertness\")\n",
    "    print(\"   6. Prepare for potential escalation\")\n",
    "\n",
    "elif threat_level == \"MEDIUM\":\n",
    "    print(\"   üîç ACTIVE MONITORING AND INVESTIGATION\")\n",
    "    print(\"   1. Investigate flagged activities systematically\")\n",
    "    print(\"   2. Validate findings with additional context\")\n",
    "    print(\"   3. Implement targeted monitoring\")\n",
    "    print(\"   4. Review security controls effectiveness\")\n",
    "    print(\"   5. Update threat hunting playbooks\")\n",
    "\n",
    "else:\n",
    "    print(\"   ‚úÖ BASELINE SECURITY POSTURE MAINTAINED\")\n",
    "    print(\"   1. Continue regular monitoring\")\n",
    "    print(\"   2. Maintain current security controls\")\n",
    "    print(\"   3. Schedule next threat hunting cycle\")\n",
    "    print(\"   4. Review and update hunting queries\")\n",
    "    print(\"   5. Train team on new techniques\")\n",
    "\n",
    "# Next steps and continuous improvement\n",
    "print(f\"\\nüîÑ CONTINUOUS IMPROVEMENT:\")\n",
    "print(\"   üìä Update baselines with new data\")\n",
    "print(\"   üéì Train analysts on identified techniques\")\n",
    "print(\"   üîß Tune detection rules based on findings\")\n",
    "print(\"   üìù Document lessons learned\")\n",
    "print(\"   üïí Schedule regular threat hunting cycles\")\n",
    "\n",
    "print(f\"\\nüìÖ Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üéØ Next recommended analysis: {(datetime.now() + timedelta(days=7)).strftime('%Y-%m-%d')}\")\n",
    "print(\"‚ú® Advanced threat hunting cycle complete!\")\n",
    "\n",
    "# Generate hunting report summary\n",
    "if threat_level in [\"CRITICAL\", \"HIGH\"]:\n",
    "    print(\"\\nüìã EXECUTIVE SUMMARY FOR STAKEHOLDERS:\")\n",
    "    print(f\"   ‚Ä¢ Threat Level: {threat_level}\")\n",
    "    print(f\"   ‚Ä¢ Critical Findings: {critical_findings}\")\n",
    "    print(f\"   ‚Ä¢ Analysis Period: {ANALYSIS_HOURS} hours\")\n",
    "    print(f\"   ‚Ä¢ Total Indicators: {sum(threat_findings.values())}\")\n",
    "    print(\"   ‚Ä¢ Immediate action required for security posture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [04_Advanced_Threat_Hunting]",
   "language": "Python",
   "name": "MSGMedium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
