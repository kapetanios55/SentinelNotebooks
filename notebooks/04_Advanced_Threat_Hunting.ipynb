{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0060b03",
   "metadata": {},
   "source": [
    "# Advanced Threat Hunting - Microsoft Sentinel Data Lake\n",
    "\n",
    "This notebook provides advanced threat hunting capabilities using Microsoft Sentinel Data Lake, combining multiple data sources for sophisticated threat detection.\n",
    "\n",
    "## 🎯 **SIMPLE SETUP: Just Update the Workspace Names!**\n",
    "This notebook uses a simple manual configuration system. \n",
    "**Just update the workspace names in the configuration cell and run!**\n",
    "\n",
    "## Advanced Use Cases Covered\n",
    "1. **Command and Control (C2) Detection** - Identify beacon behavior and C2 communications\n",
    "2. **Living off the Land** - Detect abuse of legitimate tools for malicious purposes\n",
    "3. **Data Exfiltration Patterns** - Multi-stage data theft detection\n",
    "4. **Advanced Persistent Threat (APT) Indicators** - Long-term compromise detection\n",
    "5. **User Behavior Analytics** - Detect anomalous user activities\n",
    "6. **Behavioral Analytics** - Advanced statistical analysis for threat detection\n",
    "\n",
    "## Prerequisites ✅\n",
    "- ✅ **Update workspace names in the configuration cell below** (see the simple setup instructions)\n",
    "- ✅ **Multiple data sources available** (SignInLogs, DeviceEvents, NetworkEvents)\n",
    "- ✅ **Microsoft Sentinel Data Lake enabled** in your environment\n",
    "\n",
    "## 🚀 **Publication-Ready Features:**\n",
    "- ✅ **Simple manual configuration** - just update workspace names\n",
    "- ✅ **Works in any environment** with any workspace names\n",
    "- ✅ **Adapts to available data** - uses whatever data sources you have\n",
    "- ✅ **No hardcoded values** - completely portable once configured\n",
    "- ✅ **Advanced analytics** - sophisticated threat detection algorithms\n",
    "- ✅ **Clear error handling** - helpful messages if data isn't available\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56d137e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-03T09:29:23.4728193Z",
       "execution_start_time": "2025-09-03T09:27:18.2622664Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c7032baa-db95-4e4c-8a4d-ae3de0a78899",
       "queued_time": "2025-09-03T09:27:18.0061623Z",
       "session_id": "39",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2025-09-03T09:29:19.879GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "5",
          "jobId": 48,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 4,
          "rowCount": 0,
          "stageIds": [
           84,
           83
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:19.852GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:19.834GMT",
          "dataRead": 53492,
          "dataWritten": 159,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "5",
          "jobId": 47,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 268,
          "stageIds": [
           82
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:19.670GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:19.585GMT",
          "dataRead": 10434,
          "dataWritten": 0,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 46,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 16,
          "numTasks": 66,
          "rowCount": 17,
          "stageIds": [
           81,
           80
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:19.521GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:19.402GMT",
          "dataRead": 6229,
          "dataWritten": 0,
          "description": "Delta: Delta: Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...: Filtering files for query: Compute snapshot for version: 15",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 45,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 66,
          "numTasks": 67,
          "rowCount": 50,
          "stageIds": [
           78,
           79,
           77
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:19.353GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:19.328GMT",
          "dataRead": 10434,
          "dataWritten": 6229,
          "description": "Delta: Delta: Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...: Filtering files for query: Compute snapshot for version: 15",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 44,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 16,
          "numTasks": 66,
          "rowCount": 67,
          "stageIds": [
           75,
           76
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:19.191GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:19.125GMT",
          "dataRead": 15574,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 43,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 16,
          "numTasks": 66,
          "rowCount": 33,
          "stageIds": [
           74,
           73
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:18.723GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:18.358GMT",
          "dataRead": 27692,
          "dataWritten": 15574,
          "description": "Delta: Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 42,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 16,
          "numCompletedStages": 1,
          "numCompletedTasks": 16,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 16,
          "rowCount": 66,
          "stageIds": [
           72
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:18.054GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:17.618GMT",
          "dataRead": 27692,
          "dataWritten": 0,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "toString at String.java:2951",
          "jobGroup": "5",
          "jobId": 41,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "toString at String.java:2951",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 16,
          "numCompletedStages": 1,
          "numCompletedTasks": 16,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 16,
          "rowCount": 33,
          "stageIds": [
           71
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:12.700GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:06.309GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "5",
          "jobId": 40,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 3,
          "numTasks": 4,
          "rowCount": 0,
          "stageIds": [
           70,
           69
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:06.279GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:06.254GMT",
          "dataRead": 71368,
          "dataWritten": 160,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "5",
          "jobId": 39,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 3,
          "numCompletedStages": 1,
          "numCompletedTasks": 3,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 3,
          "rowCount": 226,
          "stageIds": [
           68
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:06.106GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:06.018GMT",
          "dataRead": 1486387,
          "dataWritten": 0,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 38,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 12,
          "numTasks": 62,
          "rowCount": 7011,
          "stageIds": [
           66,
           67
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:05.920GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:05.814GMT",
          "dataRead": 7716,
          "dataWritten": 0,
          "description": "Delta: Delta: Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...: Filtering files for query: Compute snapshot for version: 7011",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 37,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 62,
          "numTasks": 63,
          "rowCount": 50,
          "stageIds": [
           63,
           64,
           65
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:05.738GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:05.690GMT",
          "dataRead": 1486387,
          "dataWritten": 7716,
          "description": "Delta: Delta: Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...: Filtering files for query: Compute snapshot for version: 7011",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 36,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 12,
          "numTasks": 62,
          "rowCount": 7061,
          "stageIds": [
           61,
           62
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:05.557GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:05.484GMT",
          "dataRead": 1580729,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 35,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 12,
          "numTasks": 62,
          "rowCount": 7022,
          "stageIds": [
           60,
           59
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:05.121GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:04.827GMT",
          "dataRead": 738250,
          "dataWritten": 1580729,
          "description": "Delta: Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 34,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 12,
          "numCompletedStages": 1,
          "numCompletedTasks": 12,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 12,
          "rowCount": 14044,
          "stageIds": [
           58
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:04.597GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:03.647GMT",
          "dataRead": 18718,
          "dataWritten": 0,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "toString at String.java:2951",
          "jobGroup": "5",
          "jobId": 33,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "toString at String.java:2951",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 12,
          "numCompletedStages": 1,
          "numCompletedTasks": 12,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 12,
          "rowCount": 7022,
          "stageIds": [
           57
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:28:59.002GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:28:52.706GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "5",
          "jobId": 32,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 21,
          "numTasks": 22,
          "rowCount": 0,
          "stageIds": [
           56,
           55
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:28:52.678GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:28:52.659GMT",
          "dataRead": 335175,
          "dataWritten": 1092,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "5",
          "jobId": 31,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 21,
          "numCompletedStages": 1,
          "numCompletedTasks": 21,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 21,
          "rowCount": 214,
          "stageIds": [
           54
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:28:52.234GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:28:52.123GMT",
          "dataRead": 855081,
          "dataWritten": 0,
          "description": "Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 30,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 4073,
          "stageIds": [
           52,
           53
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:28:52.006GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:28:51.874GMT",
          "dataRead": 5750,
          "dataWritten": 0,
          "description": "Delta: Delta: Job group for statement 5:\n# Import advanced libraries for threat hunting\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import Sentinel Data Lake libraries\nfrom sentinel_lake.providers import MicrosoftSentinelProvider\nfrom pyspark.sql.functions import (\n    col, count as spark_count, desc, asc, when, from_json, \n    countDistinct, sum as spark_sum, avg, stddev,\n    date_trunc, hour, dayofweek, minute,\n    regexp_extract, lower, upper, split, concat,\n    to_timestamp, datediff, current_timestamp,\n    substring, length,\n    expr, lit, coalesce, isnan, isnull,\n    collect_list, collect_set, array_contains,\n    unix_timestamp, from_unixtime, lag, lead\n)\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom pyspark.sql.window import Window\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set visualization style\ntry:\n    plt.style.use('seaborn-v0_8')\nexcept:\n    plt.style.use('default')\n\n# Initialize data p...: Filtering files for query: Compute snapshot for version: 4073",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "5",
          "jobId": 29,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 68,
          "numTasks": 69,
          "rowCount": 50,
          "stageIds": [
           51,
           49,
           50
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:28:51.798GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 48,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "MSGMedium",
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(MSGMedium, 39, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Advanced threat hunting libraries imported\n",
      "\n",
      "🎯 ADVANCED THREAT HUNTING CONFIGURATION:\n",
      "🏢 Primary workspace: 'ak-SecOps'\n",
      "🔵 Entra workspace: 'default'\n",
      "� Analysis window: 24 hours\n",
      "\n",
      "✅ Configuration looks good!\n",
      "� Ready for advanced threat hunting in your environment\n",
      "\n",
      "📊 Table-to-workspace mapping:\n",
      "   • SigninLogs → ak-SecOps\n",
      "   • DeviceEvents → ak-SecOps\n",
      "   • DeviceProcessEvents → ak-SecOps\n",
      "   • DeviceNetworkEvents → ak-SecOps\n",
      "   • DeviceFileEvents → ak-SecOps\n",
      "   • DeviceInfo → ak-SecOps\n",
      "   • SecurityEvent → ak-SecOps\n",
      "   • CommonSecurityLog → ak-SecOps\n",
      "   • AADNonInteractiveUserSignInLogs → ak-SecOps\n",
      "   • AADServicePrincipalSignInLogs → ak-SecOps\n",
      "   • AADManagedIdentitySignInLogs → ak-SecOps\n",
      "   • AuditLogs → ak-SecOps\n",
      "   • EntraUsers → default\n",
      "   • EntraGroups → default\n",
      "   • EntraApplications → default\n",
      "   • EntraServicePrincipals → default\n",
      "   • EntraGroupMemberships → default\n",
      "   • EntraMembers → default\n",
      "   • EntraOrganizations → default\n",
      "\n",
      "🎯 ADVANCED THREAT HUNTING READY!\n",
      "🔍 This notebook will perform sophisticated multi-source threat detection\n",
      "\n",
      "🔍 VERIFYING THREAT HUNTING DATA SOURCES...\n",
      "==================================================\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: SigninLogs\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table SigninLogs\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table SigninLogs\"}\n",
      "✅ SigninLogs: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceEvents\"}\n",
      "✅ SigninLogs: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceEvents\"}\n",
      "✅ DeviceEvents: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceProcessEvents\"}\n",
      "✅ DeviceEvents: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceProcessEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceProcessEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceProcessEvents\"}\n",
      "✅ DeviceProcessEvents: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceNetworkEvents\"}\n",
      "✅ DeviceProcessEvents: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceNetworkEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceNetworkEvents\"}\n",
      "✅ DeviceNetworkEvents: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: SecurityEvent\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceNetworkEvents\"}\n",
      "✅ DeviceNetworkEvents: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: SecurityEvent\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table SecurityEvent\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table SecurityEvent\"}\n",
      "✅ SecurityEvent: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: CommonSecurityLog\"}\n",
      "✅ SecurityEvent: 100 sample rows in 'ak-SecOps'\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: CommonSecurityLog\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table CommonSecurityLog\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table CommonSecurityLog\"}\n",
      "✅ CommonSecurityLog: 100 sample rows in 'ak-SecOps'\n",
      "\n",
      "📊 THREAT HUNTING DATA AVAILABILITY:\n",
      "   ✅ Accessible data sources: 6/6\n",
      "   📋 Ready for hunting: SigninLogs, DeviceEvents, DeviceProcessEvents, DeviceNetworkEvents, SecurityEvent, CommonSecurityLog\n",
      "\n",
      "🚀 Ready for advanced threat hunting!\n",
      "\n",
      "📈 DATA SOURCE CATEGORIES:\n",
      "   🔐 Identity: SigninLogs\n",
      "   💻 Endpoint: DeviceEvents, DeviceProcessEvents, DeviceNetworkEvents\n",
      "   🛡️  Security: SecurityEvent, CommonSecurityLog\n",
      "\n",
      "============================================================\n",
      "✅ CommonSecurityLog: 100 sample rows in 'ak-SecOps'\n",
      "\n",
      "📊 THREAT HUNTING DATA AVAILABILITY:\n",
      "   ✅ Accessible data sources: 6/6\n",
      "   📋 Ready for hunting: SigninLogs, DeviceEvents, DeviceProcessEvents, DeviceNetworkEvents, SecurityEvent, CommonSecurityLog\n",
      "\n",
      "🚀 Ready for advanced threat hunting!\n",
      "\n",
      "📈 DATA SOURCE CATEGORIES:\n",
      "   🔐 Identity: SigninLogs\n",
      "   💻 Endpoint: DeviceEvents, DeviceProcessEvents, DeviceNetworkEvents\n",
      "   🛡️  Security: SecurityEvent, CommonSecurityLog\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import advanced libraries for threat hunting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Sentinel Data Lake libraries\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "from pyspark.sql.functions import (\n",
    "    col, count as spark_count, desc, asc, when, from_json, \n",
    "    countDistinct, sum as spark_sum, avg, stddev,\n",
    "    date_trunc, hour, dayofweek, minute,\n",
    "    regexp_extract, lower, upper, split, concat,\n",
    "    to_timestamp, datediff, current_timestamp,\n",
    "    substring, length,\n",
    "    expr, lit, coalesce, isnan, isnull,\n",
    "    collect_list, collect_set, array_contains,\n",
    "    unix_timestamp, from_unixtime, lag, lead\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "\n",
    "# Initialize data provider\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "\n",
    "print(\"✅ Advanced threat hunting libraries imported\")\n",
    "\n",
    "# 🔄 WORKSPACE CONFIGURATION\n",
    "# ===========================================\n",
    "# 🎯 SIMPLE SETUP: Copy the workspace names from your setup notebook!\n",
    "\n",
    "PRIMARY_WORKSPACE = \"ak-SecOps\"      # 🏢 Copy your primary workspace name here\n",
    "ENTRA_WORKSPACE = \"default\"          # 🔵 Copy your Entra workspace name here\n",
    "\n",
    "# Analysis configuration\n",
    "ANALYSIS_HOURS = 24                  # 📅 Hours of data to analyze\n",
    "SENTINEL_ENVIRONMENT = True          # 🔍 Enable advanced analysis features\n",
    "\n",
    "# Advanced settings (optional - can leave as defaults)\n",
    "WORKSPACE_MAPPING = {\n",
    "    'SigninLogs': PRIMARY_WORKSPACE,\n",
    "    'DeviceEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceProcessEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceNetworkEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceFileEvents': PRIMARY_WORKSPACE,\n",
    "    'DeviceInfo': PRIMARY_WORKSPACE,\n",
    "    'SecurityEvent': PRIMARY_WORKSPACE,\n",
    "    'CommonSecurityLog': PRIMARY_WORKSPACE,\n",
    "    'AADNonInteractiveUserSignInLogs': PRIMARY_WORKSPACE,\n",
    "    'AADServicePrincipalSignInLogs': PRIMARY_WORKSPACE,\n",
    "    'AADManagedIdentitySignInLogs': PRIMARY_WORKSPACE,\n",
    "    'AuditLogs': PRIMARY_WORKSPACE,\n",
    "    'EntraUsers': ENTRA_WORKSPACE,\n",
    "    'EntraGroups': ENTRA_WORKSPACE,\n",
    "    'EntraApplications': ENTRA_WORKSPACE,\n",
    "    'EntraServicePrincipals': ENTRA_WORKSPACE,\n",
    "    'EntraGroupMemberships': ENTRA_WORKSPACE,\n",
    "    'EntraMembers': ENTRA_WORKSPACE,\n",
    "    'EntraOrganizations': ENTRA_WORKSPACE\n",
    "}\n",
    "\n",
    "print(f\"\\n🎯 ADVANCED THREAT HUNTING CONFIGURATION:\")\n",
    "print(f\"🏢 Primary workspace: '{PRIMARY_WORKSPACE}'\")\n",
    "print(f\"🔵 Entra workspace: '{ENTRA_WORKSPACE}'\")\n",
    "print(f\"\udcc5 Analysis window: {ANALYSIS_HOURS} hours\")\n",
    "\n",
    "# Configuration validation\n",
    "if PRIMARY_WORKSPACE == \"YOUR_WORKSPACE_NAME_HERE\":\n",
    "    print(f\"\\n⚠️  CONFIGURATION NEEDED!\")\n",
    "    print(f\"📝 Please update the workspace names above:\")\n",
    "    print(f\"   1. Run 01_Setup_and_Configuration.ipynb first\")\n",
    "    print(f\"   2. Copy the discovered workspace names\")\n",
    "    print(f\"   3. Update PRIMARY_WORKSPACE and ENTRA_WORKSPACE above\")\n",
    "    print(f\"   4. Re-run this cell\")\n",
    "elif PRIMARY_WORKSPACE == \"test-workspace\":\n",
    "    print(f\"\\n✅ DEMO MODE: Using test configuration\")\n",
    "    print(f\"💡 Configuration system is working correctly!\")\n",
    "    print(f\"📝 For real analysis, replace with your actual workspace names\")\n",
    "else:\n",
    "    print(f\"\\n✅ Configuration looks good!\")\n",
    "    print(f\"\ude80 Ready for advanced threat hunting in your environment\")\n",
    "    \n",
    "    # Show workspace mapping\n",
    "    print(f\"\\n📊 Table-to-workspace mapping:\")\n",
    "    for table, workspace in WORKSPACE_MAPPING.items():\n",
    "        print(f\"   • {table} → {workspace}\")\n",
    "\n",
    "print(f\"\\n🎯 ADVANCED THREAT HUNTING READY!\")\n",
    "print(f\"🔍 This notebook will perform sophisticated multi-source threat detection\")\n",
    "\n",
    "# Helper function for safe table checking using discovered mapping\n",
    "def safe_table_check(table_name, workspace_name=None):\n",
    "    \"\"\"Safely check table availability using workspace mapping\"\"\"\n",
    "    try:\n",
    "        # Use workspace mapping from configuration\n",
    "        if workspace_name is None:\n",
    "            workspace_name = WORKSPACE_MAPPING.get(table_name, PRIMARY_WORKSPACE)\n",
    "        \n",
    "        df = data_provider.read_table(table_name, workspace_name)\n",
    "        \n",
    "        # Get basic stats with small sample for performance\n",
    "        sample_count = df.limit(100).count()\n",
    "        columns = df.columns\n",
    "        \n",
    "        return {\n",
    "            'available': True,\n",
    "            'sample_rows': sample_count,\n",
    "            'total_columns': len(columns),\n",
    "            'columns': columns[:5],  # Show first 5 columns\n",
    "            'workspace': workspace_name,\n",
    "            'error': None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'available': False,\n",
    "            'sample_rows': 0,\n",
    "            'total_columns': 0,\n",
    "            'columns': [],\n",
    "            'workspace': workspace_name,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Quick verification of key threat hunting data sources\n",
    "print(f\"\\n🔍 VERIFYING THREAT HUNTING DATA SOURCES...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test key tables across different data source types\n",
    "threat_hunting_tables = [\n",
    "    \"SigninLogs\",           # Identity data\n",
    "    \"DeviceEvents\",         # Endpoint data  \n",
    "    \"DeviceProcessEvents\",  # Process data\n",
    "    \"DeviceNetworkEvents\",  # Network data\n",
    "    \"SecurityEvent\",        # Windows events\n",
    "    \"CommonSecurityLog\"     # Network security devices\n",
    "]\n",
    "\n",
    "accessible_hunting_tables = []\n",
    "\n",
    "for table in threat_hunting_tables:\n",
    "    result = safe_table_check(table)\n",
    "    if result['available']:\n",
    "        accessible_hunting_tables.append(table)\n",
    "        print(f\"✅ {table}: {result['sample_rows']} sample rows in '{result['workspace']}'\")\n",
    "    else:\n",
    "        print(f\"❌ {table}: Not accessible ({result['error'][:50]}...)\")\n",
    "\n",
    "print(f\"\\n📊 THREAT HUNTING DATA AVAILABILITY:\")\n",
    "print(f\"   ✅ Accessible data sources: {len(accessible_hunting_tables)}/{len(threat_hunting_tables)}\")\n",
    "\n",
    "if accessible_hunting_tables:\n",
    "    print(f\"   📋 Ready for hunting: {', '.join(accessible_hunting_tables)}\")\n",
    "    print(f\"\\n🚀 Ready for advanced threat hunting!\")\n",
    "    \n",
    "    # Categorize available data types\n",
    "    identity_data = [t for t in accessible_hunting_tables if 'signin' in t.lower()]\n",
    "    endpoint_data = [t for t in accessible_hunting_tables if 'device' in t.lower()]\n",
    "    security_data = [t for t in accessible_hunting_tables if 'security' in t.lower()]\n",
    "    \n",
    "    print(f\"\\n📈 DATA SOURCE CATEGORIES:\")\n",
    "    if identity_data:\n",
    "        print(f\"   🔐 Identity: {', '.join(identity_data)}\")\n",
    "    if endpoint_data:\n",
    "        print(f\"   💻 Endpoint: {', '.join(endpoint_data)}\")\n",
    "    if security_data:\n",
    "        print(f\"   🛡️  Security: {', '.join(security_data)}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"   ⚠️ Limited threat hunting data available\")\n",
    "    print(f\"   💡 Advanced threat hunting works best with multiple data sources\")\n",
    "    print(f\"   📝 The analysis sections will adapt to available data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5080e2",
   "metadata": {},
   "source": [
    "## 1. Command and Control (C2) Beacon Detection\n",
    "\n",
    "Advanced detection of C2 communications using statistical analysis of network patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c20ff36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-03T09:30:18.9855126Z",
       "execution_start_time": "2025-09-03T09:29:23.8976045Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f8c023f6-5c71-47ea-a7bc-4558c57c1ef5",
       "queued_time": "2025-09-03T09:29:23.8961723Z",
       "session_id": "39",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2025-09-03T09:30:18.805GMT",
          "dataRead": 2639,
          "dataWritten": 0,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 68,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 129,
          "numTasks": 130,
          "rowCount": 20,
          "stageIds": [
           125,
           126,
           127
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:30:18.719GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:30:18.658GMT",
          "dataRead": 4705,
          "dataWritten": 2639,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 67,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 128,
          "numTasks": 129,
          "rowCount": 58,
          "stageIds": [
           123,
           124
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:30:18.516GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:30:18.481GMT",
          "dataRead": 13984559,
          "dataWritten": 4705,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 66,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 128,
          "numCompletedStages": 1,
          "numCompletedTasks": 128,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 128,
          "rowCount": 152,
          "stageIds": [
           122
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:30:08.760GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:30:08.534GMT",
          "dataRead": 569536,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "7",
          "jobId": 65,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 4071,
          "stageIds": [
           121,
           120
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:30:08.433GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:30:07.908GMT",
          "dataRead": 59,
          "dataWritten": 0,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 64,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 3,
          "numSkippedTasks": 130,
          "numTasks": 131,
          "rowCount": 1,
          "stageIds": [
           117,
           118,
           119,
           116
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:30:07.881GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:30:07.865GMT",
          "dataRead": 2639,
          "dataWritten": 59,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 63,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 129,
          "numTasks": 130,
          "rowCount": 21,
          "stageIds": [
           114,
           115,
           113
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:30:07.800GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:30:07.739GMT",
          "dataRead": 4705,
          "dataWritten": 2639,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 62,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 128,
          "numTasks": 129,
          "rowCount": 58,
          "stageIds": [
           111,
           112
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:30:07.611GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:30:07.581GMT",
          "dataRead": 10890442,
          "dataWritten": 4705,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 61,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 128,
          "numCompletedStages": 1,
          "numCompletedTasks": 128,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 128,
          "rowCount": 152,
          "stageIds": [
           110
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:57.318GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:57.104GMT",
          "dataRead": 569536,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "7",
          "jobId": 60,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 4071,
          "stageIds": [
           108,
           109
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:57.004GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:56.380GMT",
          "dataRead": 2636,
          "dataWritten": 0,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 59,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 129,
          "numTasks": 130,
          "rowCount": 20,
          "stageIds": [
           107,
           105,
           106
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:56.301GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:56.207GMT",
          "dataRead": 4705,
          "dataWritten": 2636,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 58,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 128,
          "numTasks": 129,
          "rowCount": 58,
          "stageIds": [
           103,
           104
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:56.166GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:56.128GMT",
          "dataRead": 22248859,
          "dataWritten": 4705,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 57,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 128,
          "numCompletedStages": 1,
          "numCompletedTasks": 128,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 128,
          "rowCount": 152,
          "stageIds": [
           102
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:45.017GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:44.766GMT",
          "dataRead": 569536,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "7",
          "jobId": 56,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 4071,
          "stageIds": [
           100,
           101
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:44.636GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:44.095GMT",
          "dataRead": 59,
          "dataWritten": 0,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 55,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 3,
          "numSkippedTasks": 130,
          "numTasks": 131,
          "rowCount": 1,
          "stageIds": [
           99,
           96,
           97,
           98
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:44.064GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:44.045GMT",
          "dataRead": 2637,
          "dataWritten": 59,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 54,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 129,
          "numTasks": 130,
          "rowCount": 21,
          "stageIds": [
           93,
           94,
           95
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:43.978GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:43.921GMT",
          "dataRead": 4705,
          "dataWritten": 2637,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 53,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 128,
          "numTasks": 129,
          "rowCount": 58,
          "stageIds": [
           91,
           92
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:43.758GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:43.655GMT",
          "dataRead": 31841230,
          "dataWritten": 4705,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 52,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 128,
          "numCompletedStages": 1,
          "numCompletedTasks": 128,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 128,
          "rowCount": 152,
          "stageIds": [
           90
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:28.244GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:27.762GMT",
          "dataRead": 569536,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "7",
          "jobId": 51,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 4071,
          "stageIds": [
           88,
           89
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:27.515GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:26.080GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 50,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 21,
          "numTasks": 22,
          "rowCount": 0,
          "stageIds": [
           86,
           87
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:26.039GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:29:26.019GMT",
          "dataRead": 28934,
          "dataWritten": 1092,
          "description": "Job group for statement 7:\n# Advanced C2 beacon detection\ndef detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n    \"\"\"\n    Detect potential C2 beacons using advanced statistical analysis\n    \"\"\"\n    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\n\")\n    \n    # First, let's see what columns are available\n    print(\"📊 Available columns in DeviceNetworkEvents:\")\n    print(network_df.columns)\n    \n    # Group connections by source and destination with time bucketing\n    # Note: Using connection frequency instead of byte counts for beacon detection\n    beacon_analysis = network_df.withColumn(\n        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n    ).groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n    ).agg(\n        spark_count(\"*\").alias(\"ConnectionCount\")\n    )\n    \n    # Calculate beacon characteristics per connection pair\n    beacon_stats = beacon_analysis.groupBy(\n        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n    ).agg(\n        spark_count(\"*\").alias(\"TimeWindows\"),...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "7",
          "jobId": 49,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 21,
          "numCompletedStages": 1,
          "numCompletedTasks": 21,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 21,
          "rowCount": 214,
          "stageIds": [
           85
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:29:25.636GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 20,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "MSGMedium",
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(MSGMedium, 39, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceNetworkEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceNetworkEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceNetworkEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceNetworkEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceNetworkEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceNetworkEvents\"}\n",
      "✅ Loaded DeviceNetworkEvents from ak-SecOps\n",
      "🔍 ANALYZING C2 BEACON PATTERNS...\n",
      "\n",
      "📊 Available columns in DeviceNetworkEvents:\n",
      "['TenantId', 'ActionType', 'AdditionalFields', 'AppGuardContainerId', 'DeviceId', 'DeviceName', 'InitiatingProcessAccountDomain', 'InitiatingProcessAccountName', 'InitiatingProcessAccountObjectId', 'InitiatingProcessAccountSid', 'InitiatingProcessAccountUpn', 'InitiatingProcessCommandLine', 'InitiatingProcessFileName', 'InitiatingProcessFolderPath', 'InitiatingProcessId', 'InitiatingProcessIntegrityLevel', 'InitiatingProcessMD5', 'InitiatingProcessParentFileName', 'InitiatingProcessParentId', 'InitiatingProcessSHA1', 'InitiatingProcessSHA256', 'InitiatingProcessTokenElevation', 'InitiatingProcessFileSize', 'InitiatingProcessVersionInfoCompanyName', 'InitiatingProcessVersionInfoProductName', 'InitiatingProcessVersionInfoProductVersion', 'InitiatingProcessVersionInfoInternalFileName', 'InitiatingProcessVersionInfoOriginalFileName', 'InitiatingProcessVersionInfoFileDescription', 'LocalIP', 'LocalIPType', 'LocalPort', 'MachineGroup', 'Protocol', 'RemoteIP', 'RemoteIPType', 'RemotePort', 'RemoteUrl', 'ReportId', 'TimeGenerated', 'Timestamp', 'InitiatingProcessParentCreationTime', 'InitiatingProcessCreationTime', 'InitiatingProcessSessionId', 'IsInitiatingProcessRemoteSession', 'InitiatingProcessRemoteSessionDeviceName', 'InitiatingProcessRemoteSessionIP', 'InitiatingProcessUniqueId', 'SourceSystem', '_ReceivedTime', '_IsBillable', '_BilledSize']\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceNetworkEvents\"}\n",
      "✅ Loaded DeviceNetworkEvents from ak-SecOps\n",
      "🔍 ANALYZING C2 BEACON PATTERNS...\n",
      "\n",
      "📊 Available columns in DeviceNetworkEvents:\n",
      "['TenantId', 'ActionType', 'AdditionalFields', 'AppGuardContainerId', 'DeviceId', 'DeviceName', 'InitiatingProcessAccountDomain', 'InitiatingProcessAccountName', 'InitiatingProcessAccountObjectId', 'InitiatingProcessAccountSid', 'InitiatingProcessAccountUpn', 'InitiatingProcessCommandLine', 'InitiatingProcessFileName', 'InitiatingProcessFolderPath', 'InitiatingProcessId', 'InitiatingProcessIntegrityLevel', 'InitiatingProcessMD5', 'InitiatingProcessParentFileName', 'InitiatingProcessParentId', 'InitiatingProcessSHA1', 'InitiatingProcessSHA256', 'InitiatingProcessTokenElevation', 'InitiatingProcessFileSize', 'InitiatingProcessVersionInfoCompanyName', 'InitiatingProcessVersionInfoProductName', 'InitiatingProcessVersionInfoProductVersion', 'InitiatingProcessVersionInfoInternalFileName', 'InitiatingProcessVersionInfoOriginalFileName', 'InitiatingProcessVersionInfoFileDescription', 'LocalIP', 'LocalIPType', 'LocalPort', 'MachineGroup', 'Protocol', 'RemoteIP', 'RemoteIPType', 'RemotePort', 'RemoteUrl', 'ReportId', 'TimeGenerated', 'Timestamp', 'InitiatingProcessParentCreationTime', 'InitiatingProcessCreationTime', 'InitiatingProcessSessionId', 'IsInitiatingProcessRemoteSession', 'InitiatingProcessRemoteSessionDeviceName', 'InitiatingProcessRemoteSessionIP', 'InitiatingProcessUniqueId', 'SourceSystem', '_ReceivedTime', '_IsBillable', '_BilledSize']\n",
      "🚨 POTENTIAL C2 BEACONS DETECTED: 1\n",
      "\n",
      "🚨 POTENTIAL C2 BEACONS DETECTED: 1\n",
      "\n",
      "+-------------+--------+----------+-----------+-----------+---------------------+-------------------+\n",
      "|DeviceName   |RemoteIP|RemotePort|BeaconScore|TimeWindows|AvgConnectionsPerHour|ConnectionJitter   |\n",
      "+-------------+--------+----------+-----------+-----------+---------------------+-------------------+\n",
      "|atomicredteam|        |0         |75         |16         |1.0625               |0.23529411764705882|\n",
      "+-------------+--------+----------+-----------+-----------+---------------------+-------------------+\n",
      "\n",
      "+-------------+--------+----------+-----------+-----------+---------------------+-------------------+\n",
      "|DeviceName   |RemoteIP|RemotePort|BeaconScore|TimeWindows|AvgConnectionsPerHour|ConnectionJitter   |\n",
      "+-------------+--------+----------+-----------+-----------+---------------------+-------------------+\n",
      "|atomicredteam|        |0         |75         |16         |1.0625               |0.23529411764705882|\n",
      "+-------------+--------+----------+-----------+-----------+---------------------+-------------------+\n",
      "\n",
      "\n",
      "🔥 HIGH CONFIDENCE C2 BEACONS:\n",
      "\n",
      "🔥 HIGH CONFIDENCE C2 BEACONS:\n",
      "+-------------+---------+--------+----------+-----------+---------------------+-------------------+------------------+-----------+\n",
      "|DeviceName   |LocalIP  |RemoteIP|RemotePort|TimeWindows|AvgConnectionsPerHour|StdDevConnections  |ConnectionJitter  |BeaconScore|\n",
      "+-------------+---------+--------+----------+-----------+---------------------+-------------------+------------------+-----------+\n",
      "|atomicredteam|127.0.0.1|        |0         |16         |1.0625               |0.24999999999999997|0.2352941176470588|75         |\n",
      "+-------------+---------+--------+----------+-----------+---------------------+-------------------+------------------+-----------+\n",
      "\n",
      "\n",
      "🚨 IMMEDIATE ACTIONS REQUIRED:\n",
      "   1. Block identified C2 IPs at firewall\n",
      "   2. Isolate affected devices immediately\n",
      "   3. Analyze malware samples on affected systems\n",
      "   4. Search for related IOCs across environment\n",
      "   5. Review user accounts on affected devices\n",
      "+-------------+---------+--------+----------+-----------+---------------------+-------------------+------------------+-----------+\n",
      "|DeviceName   |LocalIP  |RemoteIP|RemotePort|TimeWindows|AvgConnectionsPerHour|StdDevConnections  |ConnectionJitter  |BeaconScore|\n",
      "+-------------+---------+--------+----------+-----------+---------------------+-------------------+------------------+-----------+\n",
      "|atomicredteam|127.0.0.1|        |0         |16         |1.0625               |0.24999999999999997|0.2352941176470588|75         |\n",
      "+-------------+---------+--------+----------+-----------+---------------------+-------------------+------------------+-----------+\n",
      "\n",
      "\n",
      "🚨 IMMEDIATE ACTIONS REQUIRED:\n",
      "   1. Block identified C2 IPs at firewall\n",
      "   2. Isolate affected devices immediately\n",
      "   3. Analyze malware samples on affected systems\n",
      "   4. Search for related IOCs across environment\n",
      "   5. Review user accounts on affected devices\n"
     ]
    }
   ],
   "source": [
    "# Advanced C2 beacon detection\n",
    "def detect_c2_beacons(network_df, min_connections=10, jitter_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detect potential C2 beacons using advanced statistical analysis\n",
    "    \"\"\"\n",
    "    print(\"🔍 ANALYZING C2 BEACON PATTERNS...\\n\")\n",
    "    \n",
    "    # First, let's see what columns are available\n",
    "    print(\"📊 Available columns in DeviceNetworkEvents:\")\n",
    "    print(network_df.columns)\n",
    "    \n",
    "    # Group connections by source and destination with time bucketing\n",
    "    # Note: Using connection frequency instead of byte counts for beacon detection\n",
    "    beacon_analysis = network_df.withColumn(\n",
    "        \"TimeWindow\", date_trunc(\"hour\", col(\"Timestamp\"))\n",
    "    ).groupBy(\n",
    "        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\", \"TimeWindow\"\n",
    "    ).agg(\n",
    "        spark_count(\"*\").alias(\"ConnectionCount\")\n",
    "    )\n",
    "    \n",
    "    # Calculate beacon characteristics per connection pair\n",
    "    beacon_stats = beacon_analysis.groupBy(\n",
    "        \"DeviceName\", \"LocalIP\", \"RemoteIP\", \"RemotePort\"\n",
    "    ).agg(\n",
    "        spark_count(\"*\").alias(\"TimeWindows\"),\n",
    "        avg(\"ConnectionCount\").alias(\"AvgConnectionsPerHour\"),\n",
    "        stddev(\"ConnectionCount\").alias(\"StdDevConnections\")\n",
    "    ).filter(\n",
    "        col(\"TimeWindows\") >= min_connections\n",
    "    )\n",
    "    \n",
    "    # Identify potential beacons (low jitter, consistent timing)\n",
    "    potential_beacons = beacon_stats.withColumn(\n",
    "        \"ConnectionJitter\", \n",
    "        coalesce(col(\"StdDevConnections\") / col(\"AvgConnectionsPerHour\"), lit(999))\n",
    "    ).withColumn(\n",
    "        \"BeaconScore\",\n",
    "        when(col(\"ConnectionJitter\") <= jitter_threshold, 100)\n",
    "        .when(col(\"ConnectionJitter\") <= 0.3, 75)\n",
    "        .when(col(\"ConnectionJitter\") <= 0.5, 50)\n",
    "        .otherwise(25)\n",
    "    ).filter(\n",
    "        col(\"BeaconScore\") >= 50\n",
    "    ).orderBy(desc(\"BeaconScore\"), desc(\"TimeWindows\"))\n",
    "    \n",
    "    return potential_beacons\n",
    "\n",
    "# Load network data for C2 analysis\n",
    "try:\n",
    "    if SENTINEL_ENVIRONMENT:\n",
    "        # Use safe_table_check to get proper workspace\n",
    "        table_info = safe_table_check(\"DeviceNetworkEvents\")\n",
    "        if table_info['available']:\n",
    "            network_events = data_provider.read_table(\"DeviceNetworkEvents\", table_info['workspace'])\n",
    "            print(f\"✅ Loaded DeviceNetworkEvents from {table_info['workspace']}\")\n",
    "        else:\n",
    "            print(f\"❌ DeviceNetworkEvents not available: {table_info['error']}\")\n",
    "            network_events = None\n",
    "    else:\n",
    "        network_events = None\n",
    "    \n",
    "    # Filter to analysis window and external connections\n",
    "    if network_events is not None:\n",
    "        network_filtered = network_events.filter(\n",
    "            col(\"Timestamp\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "        ).filter(\n",
    "            # Focus on external connections (not internal RFC1918)\n",
    "            ~col(\"RemoteIP\").rlike(r\"^(10\\.|192\\.168\\.|172\\.(1[6-9]|2[0-9]|3[0-1])\\.).*$\")\n",
    "        ).filter(\n",
    "            # Filter out common legitimate traffic\n",
    "            ~col(\"RemotePort\").isin([80, 443, 53, 123])  # HTTP, HTTPS, DNS, NTP\n",
    "        )\n",
    "        \n",
    "        # Detect C2 beacons\n",
    "        c2_beacons = detect_c2_beacons(network_filtered)\n",
    "        \n",
    "        beacon_count = c2_beacons.count()\n",
    "    else:\n",
    "        print(\"❌ Network events not available - skipping C2 analysis\")\n",
    "        beacon_count = 0\n",
    "        network_filtered = None\n",
    "    \n",
    "    if beacon_count > 0:\n",
    "        print(f\"🚨 POTENTIAL C2 BEACONS DETECTED: {beacon_count}\\n\")\n",
    "        \n",
    "        c2_beacons.select(\n",
    "            \"DeviceName\", \"RemoteIP\", \"RemotePort\", \n",
    "            \"BeaconScore\", \"TimeWindows\", \"AvgConnectionsPerHour\", \"ConnectionJitter\"\n",
    "        ).show(20, truncate=False)\n",
    "        \n",
    "        # Analyze beacon timing patterns\n",
    "        high_confidence_beacons = c2_beacons.filter(col(\"BeaconScore\") >= 75)\n",
    "        \n",
    "        if high_confidence_beacons.count() > 0:\n",
    "            print(\"\\n🔥 HIGH CONFIDENCE C2 BEACONS:\")\n",
    "            high_confidence_beacons.show(10, truncate=False)\n",
    "            \n",
    "            print(\"\\n🚨 IMMEDIATE ACTIONS REQUIRED:\")\n",
    "            print(\"   1. Block identified C2 IPs at firewall\")\n",
    "            print(\"   2. Isolate affected devices immediately\")\n",
    "            print(\"   3. Analyze malware samples on affected systems\")\n",
    "            print(\"   4. Search for related IOCs across environment\")\n",
    "            print(\"   5. Review user accounts on affected devices\")\n",
    "        \n",
    "    else:\n",
    "        print(\"✅ No obvious C2 beacon patterns detected\")\n",
    "        \n",
    "        # Show some network statistics (only if network_filtered is available)\n",
    "        if 'network_filtered' in locals() and network_filtered is not None:\n",
    "            external_connections = network_filtered.count()\n",
    "            unique_destinations = network_filtered.select(\"RemoteIP\").distinct().count()\n",
    "            \n",
    "            print(f\"📊 External Network Analysis Summary:\")\n",
    "            print(f\"   External Connections: {external_connections:,}\")\n",
    "            print(f\"   Unique Destinations: {unique_destinations:,}\")\n",
    "        else:\n",
    "            print(\"📊 Network statistics not available\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Error analyzing network data: {str(e)}\")\n",
    "    print(\"   DeviceNetworkEvents may not be available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fc38e",
   "metadata": {},
   "source": [
    "## 2. Living off the Land (LotL) Detection\n",
    "\n",
    "Detect abuse of legitimate system tools for malicious purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de6a52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-03T09:33:51.7868613Z",
       "execution_start_time": "2025-09-03T09:30:19.3645573Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1cf59969-78c2-4c4f-a09f-7b66a2cc69d6",
       "queued_time": "2025-09-03T09:30:19.3630249Z",
       "session_id": "39",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2025-09-03T09:33:49.876GMT",
          "dataRead": 59,
          "dataWritten": 0,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 106,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 262,
          "numTasks": 263,
          "rowCount": 1,
          "stageIds": [
           194,
           192,
           193
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:49.852GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:49.837GMT",
          "dataRead": 380,
          "dataWritten": 59,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 105,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 6,
          "stageIds": [
           190,
           191
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:49.803GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:49.785GMT",
          "dataRead": 1629794,
          "dataWritten": 380,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 104,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9153,
          "stageIds": [
           189
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:36.136GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:35.856GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "8",
          "jobId": 103,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           187,
           188
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:35.748GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:34.822GMT",
          "dataRead": 59,
          "dataWritten": 0,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 102,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 262,
          "numTasks": 263,
          "rowCount": 1,
          "stageIds": [
           186,
           184,
           185
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:34.797GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:34.780GMT",
          "dataRead": 340,
          "dataWritten": 59,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 101,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 6,
          "stageIds": [
           183,
           182
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:34.736GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:34.711GMT",
          "dataRead": 3143710,
          "dataWritten": 340,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 100,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9583,
          "stageIds": [
           181
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:20.641GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:20.338GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "8",
          "jobId": 99,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           179,
           180
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:20.239GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:19.255GMT",
          "dataRead": 256,
          "dataWritten": 0,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "8",
          "jobId": 98,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 2,
          "numSkippedTasks": 262,
          "numTasks": 263,
          "rowCount": 1,
          "stageIds": [
           176,
           177,
           178
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:19.212GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:19.171GMT",
          "dataRead": 1380,
          "dataWritten": 256,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "8",
          "jobId": 97,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 6,
          "stageIds": [
           174,
           175
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:19.055GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:19.013GMT",
          "dataRead": 4800613,
          "dataWritten": 1380,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "showString at NativeMethodAccessorImpl.java:0",
          "jobGroup": "8",
          "jobId": 96,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "showString at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9583,
          "stageIds": [
           173
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:04.584GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:04.267GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "8",
          "jobId": 95,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           171,
           172
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:04.148GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:03.047GMT",
          "dataRead": 14616,
          "dataWritten": 0,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 94,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 261,
          "stageIds": [
           169,
           170
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:03.011GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:02.997GMT",
          "dataRead": 6861813,
          "dataWritten": 14616,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 93,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9839,
          "stageIds": [
           168
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:32:44.320GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:32:44.035GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "8",
          "jobId": 92,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           166,
           167
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:32:43.944GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:32:43.068GMT",
          "dataRead": 14616,
          "dataWritten": 0,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 91,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 261,
          "stageIds": [
           165,
           164
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:32:43.017GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:32:43.001GMT",
          "dataRead": 7266767,
          "dataWritten": 14616,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "8",
          "jobId": 90,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9839,
          "stageIds": [
           163
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:32:27.456GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:32:27.192GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "8",
          "jobId": 89,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           161,
           162
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:32:27.087GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:32:26.241GMT",
          "dataRead": 14616,
          "dataWritten": 0,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "8",
          "jobId": 88,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 261,
          "stageIds": [
           159,
           160
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:32:26.201GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:32:26.188GMT",
          "dataRead": 9642996,
          "dataWritten": 14616,
          "description": "Job group for statement 8:\n# Living off the Land detection\ndef detect_lotl_abuse(process_events):\n    \"\"\"\n    Detect abuse of legitimate tools (Living off the Land techniques)\n    \"\"\"\n    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n\")\n    \n    # Define suspicious usage patterns for legitimate tools\n    lotl_patterns = {\n        \"PowerShell Abuse\": {\n            \"process\": \"powershell|pwsh\",\n            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n        },\n        \"WMI Abuse\": {\n            \"process\": \"wmic|wmiprvse\",\n            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n        },\n        \"Certificate Abuse\": {\n            \"process\": \"certutil\",\n            \"cmdline\": \"urlcache|decode|encode|-f\"\n        },\n        \"BitsAdmin Abuse\": {\n            \"process\": \"bitsadmin\",\n            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n        },\n        \"RegSvr32 Abuse\": {\n            \"process\": \"regsvr32\",\n            \"cmdline\": \"/s.*ht...",
          "displayName": "count at NativeMethodAccessorImpl.java:0",
          "jobGroup": "8",
          "jobId": 87,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at NativeMethodAccessorImpl.java:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9839,
          "stageIds": [
           158
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:32:10.728GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 38,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "MSGMedium",
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(MSGMedium, 39, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceProcessEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceProcessEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceProcessEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceProcessEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: DeviceProcessEvents\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceProcessEvents\"}\n",
      "✅ Loaded DeviceProcessEvents from ak-SecOps\n",
      "🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n",
      "\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table DeviceProcessEvents\"}\n",
      "✅ Loaded DeviceProcessEvents from ak-SecOps\n",
      "🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\n",
      "\n",
      "🎭 LIVING OFF THE LAND TECHNIQUES DETECTED: 1\n",
      "\n",
      "🔍 PowerShell Abuse: 10 instances\n",
      "   Top command patterns:\n",
      "🎭 LIVING OFF THE LAND TECHNIQUES DETECTED: 1\n",
      "\n",
      "🔍 PowerShell Abuse: 10 instances\n",
      "   Top command patterns:\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----+-------------+\n",
      "|ProcessCommandLine                                                                                                                                                                              |AccountName|Count|UniqueDevices|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----+-------------+\n",
      "|\"powershell.exe\" -noninteractive -outputFormat xml -NonInteractive -encodedCommand IABbAEUAbgB2AGkAcgBvAG4AbQBlAG4AdABdADoAOgBPAFMAVgBlAHIAcwBpAG8AbgAuAFYAZQByAHMAaQBvAG4AIAA= -inputFormat xml|system     |10   |1            |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----+-------------+\n",
      "\n",
      "\n",
      "📊 LIVING OFF THE LAND SUMMARY:\n",
      "   Total Suspicious Events: 10\n",
      "   Techniques Detected: 1\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----+-------------+\n",
      "|ProcessCommandLine                                                                                                                                                                              |AccountName|Count|UniqueDevices|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----+-------------+\n",
      "|\"powershell.exe\" -noninteractive -outputFormat xml -NonInteractive -encodedCommand IABbAEUAbgB2AGkAcgBvAG4AbQBlAG4AdABdADoAOgBPAFMAVgBlAHIAcwBpAG8AbgAuAFYAZQByAHMAaQBvAG4AIAA= -inputFormat xml|system     |10   |1            |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-----+-------------+\n",
      "\n",
      "\n",
      "📊 LIVING OFF THE LAND SUMMARY:\n",
      "   Total Suspicious Events: 10\n",
      "   Techniques Detected: 1\n",
      "   Affected Users: 1\n",
      "   Affected Devices: 1\n",
      "\n",
      "🛡️  LOTL MITIGATION RECOMMENDATIONS:\n",
      "   1. Implement PowerShell logging and monitoring\n",
      "   2. Restrict administrative tools usage\n",
      "   3. Enable application whitelisting\n",
      "   4. Monitor process creation events\n",
      "   5. Train users on social engineering tactics\n",
      "   Affected Users: 1\n",
      "   Affected Devices: 1\n",
      "\n",
      "🛡️  LOTL MITIGATION RECOMMENDATIONS:\n",
      "   1. Implement PowerShell logging and monitoring\n",
      "   2. Restrict administrative tools usage\n",
      "   3. Enable application whitelisting\n",
      "   4. Monitor process creation events\n",
      "   5. Train users on social engineering tactics\n"
     ]
    }
   ],
   "source": [
    "# Living off the Land detection\n",
    "def detect_lotl_abuse(process_events):\n",
    "    \"\"\"\n",
    "    Detect abuse of legitimate tools (Living off the Land techniques)\n",
    "    \"\"\"\n",
    "    print(\"🎭 DETECTING LIVING OFF THE LAND TECHNIQUES...\\n\")\n",
    "    \n",
    "    # Define suspicious usage patterns for legitimate tools\n",
    "    lotl_patterns = {\n",
    "        \"PowerShell Abuse\": {\n",
    "            \"process\": \"powershell|pwsh\",\n",
    "            \"cmdline\": \"encodedcommand|bypass|unrestricted|hidden|downloadstring|iex|invoke-expression|reflection\\.assembly\"\n",
    "        },\n",
    "        \"WMI Abuse\": {\n",
    "            \"process\": \"wmic|wmiprvse\",\n",
    "            \"cmdline\": \"process.*call.*create|shadowcopy.*delete|service.*create\"\n",
    "        },\n",
    "        \"Certificate Abuse\": {\n",
    "            \"process\": \"certutil\",\n",
    "            \"cmdline\": \"urlcache|decode|encode|-f\"\n",
    "        },\n",
    "        \"BitsAdmin Abuse\": {\n",
    "            \"process\": \"bitsadmin\",\n",
    "            \"cmdline\": \"transfer|addfile|setnotifyflags\"\n",
    "        },\n",
    "        \"RegSvr32 Abuse\": {\n",
    "            \"process\": \"regsvr32\",\n",
    "            \"cmdline\": \"/s.*http|/u.*http|scrobj\\.dll\"\n",
    "        },\n",
    "        \"MSBuild Abuse\": {\n",
    "            \"process\": \"msbuild\",\n",
    "            \"cmdline\": \"\\.xml|inline\"\n",
    "        },\n",
    "        \"WMIC Process Creation\": {\n",
    "            \"process\": \"wmic\",\n",
    "            \"cmdline\": \"process.*call.*create|/node:\"\n",
    "        },\n",
    "        \"Rundll32 Abuse\": {\n",
    "            \"process\": \"rundll32\",\n",
    "            \"cmdline\": \"javascript|vbscript|url\\.dll|shell32.*control_rundll\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    lotl_results = {}\n",
    "    \n",
    "    for technique, pattern in lotl_patterns.items():\n",
    "        lotl_detections = process_events.filter(\n",
    "            lower(col(\"FileName\")).rlike(pattern[\"process\"]) &\n",
    "            lower(col(\"ProcessCommandLine\")).rlike(pattern[\"cmdline\"])\n",
    "        )\n",
    "        \n",
    "        detection_count = lotl_detections.count()\n",
    "        \n",
    "        if detection_count > 0:\n",
    "            lotl_results[technique] = {\n",
    "                'count': detection_count,\n",
    "                'data': lotl_detections\n",
    "            }\n",
    "    \n",
    "    return lotl_results\n",
    "\n",
    "# Load and analyze process events for LotL\n",
    "try:\n",
    "    if SENTINEL_ENVIRONMENT:\n",
    "        # Use safe_table_check to get proper workspace\n",
    "        table_info = safe_table_check(\"DeviceProcessEvents\")\n",
    "        if table_info['available']:\n",
    "            process_events = data_provider.read_table(\"DeviceProcessEvents\", table_info['workspace'])\n",
    "            print(f\"✅ Loaded DeviceProcessEvents from {table_info['workspace']}\")\n",
    "        else:\n",
    "            print(f\"❌ DeviceProcessEvents not available: {table_info['error']}\")\n",
    "            process_events = None\n",
    "    else:\n",
    "        process_events = None\n",
    "    \n",
    "    # Filter to analysis window\n",
    "    if process_events is not None:\n",
    "        process_filtered = process_events.filter(\n",
    "            col(\"Timestamp\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "        )\n",
    "        \n",
    "        # Detect LotL techniques\n",
    "        lotl_results = detect_lotl_abuse(process_filtered)\n",
    "    else:\n",
    "        print(\"❌ Process events not available - skipping LotL analysis\")\n",
    "        lotl_results = {}\n",
    "    \n",
    "    if lotl_results:\n",
    "        print(f\"🎭 LIVING OFF THE LAND TECHNIQUES DETECTED: {len(lotl_results)}\\n\")\n",
    "        \n",
    "        for technique, result in lotl_results.items():\n",
    "            print(f\"🔍 {technique}: {result['count']} instances\")\n",
    "            \n",
    "            # Show top examples\n",
    "            technique_summary = result['data'].groupBy(\n",
    "                \"ProcessCommandLine\", \"AccountName\"\n",
    "            ).agg(\n",
    "                spark_count(\"*\").alias(\"Count\"),\n",
    "                countDistinct(\"DeviceName\").alias(\"UniqueDevices\")\n",
    "            ).orderBy(desc(\"Count\"))\n",
    "            \n",
    "            print(f\"   Top command patterns:\")\n",
    "            technique_summary.show(5, truncate=False)\n",
    "            print()\n",
    "        \n",
    "        # Overall LotL summary\n",
    "        total_lotl_events = sum(result['count'] for result in lotl_results.values())\n",
    "        \n",
    "        print(f\"📊 LIVING OFF THE LAND SUMMARY:\")\n",
    "        print(f\"   Total Suspicious Events: {total_lotl_events:,}\")\n",
    "        print(f\"   Techniques Detected: {len(lotl_results)}\")\n",
    "        \n",
    "        # Get affected users and devices\n",
    "        all_lotl_events = None\n",
    "        for result in lotl_results.values():\n",
    "            if all_lotl_events is None:\n",
    "                all_lotl_events = result['data']\n",
    "            else:\n",
    "                all_lotl_events = all_lotl_events.union(result['data'])\n",
    "        \n",
    "        affected_users = all_lotl_events.select(\"AccountName\").distinct().count()\n",
    "        affected_devices = all_lotl_events.select(\"DeviceName\").distinct().count()\n",
    "        \n",
    "        print(f\"   Affected Users: {affected_users}\")\n",
    "        print(f\"   Affected Devices: {affected_devices}\")\n",
    "        \n",
    "        print(\"\\n🛡️  LOTL MITIGATION RECOMMENDATIONS:\")\n",
    "        print(\"   1. Implement PowerShell logging and monitoring\")\n",
    "        print(\"   2. Restrict administrative tools usage\")\n",
    "        print(\"   3. Enable application whitelisting\")\n",
    "        print(\"   4. Monitor process creation events\")\n",
    "        print(\"   5. Train users on social engineering tactics\")\n",
    "        \n",
    "    else:\n",
    "        print(\"✅ No Living off the Land techniques detected\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Error analyzing process events: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992e3ea",
   "metadata": {},
   "source": [
    "## 3. Advanced Persistence Detection\n",
    "\n",
    "Detect sophisticated persistence mechanisms used by advanced threats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998f22b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-03T09:35:12.9420821Z",
       "execution_start_time": "2025-09-03T09:33:52.1331303Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "fbc688c6-8c33-4441-9c5f-5bf9fb6438bf",
       "queued_time": "2025-09-03T09:33:52.131733Z",
       "session_id": "39",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "completionTime": "2025-09-03T09:35:10.610GMT",
          "dataRead": 14616,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 121,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 261,
          "stageIds": [
           219,
           218
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:35:10.577GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:35:10.559GMT",
          "dataRead": 2533216,
          "dataWritten": 14616,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 120,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9409,
          "stageIds": [
           217
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:55.199GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:54.912GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "9",
          "jobId": 119,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           215,
           216
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:54.815GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:53.908GMT",
          "dataRead": 14616,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 118,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 261,
          "stageIds": [
           213,
           214
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:53.877GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:53.865GMT",
          "dataRead": 1148095,
          "dataWritten": 14616,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 117,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9409,
          "stageIds": [
           212
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:38.641GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:38.361GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "9",
          "jobId": 116,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           210,
           211
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:38.265GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:37.333GMT",
          "dataRead": 14616,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 115,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 261,
          "stageIds": [
           208,
           209
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:37.300GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:37.286GMT",
          "dataRead": 383936,
          "dataWritten": 14616,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 114,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9409,
          "stageIds": [
           207
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:22.961GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:22.659GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "9",
          "jobId": 113,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           205,
           206
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:22.558GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:21.620GMT",
          "dataRead": 14616,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 112,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 261,
          "stageIds": [
           204,
           203
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:21.572GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:21.556GMT",
          "dataRead": 402381,
          "dataWritten": 14616,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 111,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9409,
          "stageIds": [
           202
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:08.207GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:07.926GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "9",
          "jobId": 110,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           201,
           200
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:07.830GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:06.907GMT",
          "dataRead": 14616,
          "dataWritten": 0,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 109,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 261,
          "numTasks": 262,
          "rowCount": 261,
          "stageIds": [
           198,
           199
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:34:06.872GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:34:06.860GMT",
          "dataRead": 371354,
          "dataWritten": 14616,
          "description": "Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "9",
          "jobId": 108,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 261,
          "numCompletedStages": 1,
          "numCompletedTasks": 261,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 261,
          "rowCount": 9409,
          "stageIds": [
           197
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:53.455GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:33:53.186GMT",
          "dataRead": 1237006,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 9:\n# Advanced persistence detection\ndef detect_persistence_mechanisms(process_events, registry_events=None):\n    \"\"\"\n    Detect various persistence mechanisms\n    \"\"\"\n    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\n\")\n    \n    persistence_indicators = []\n    \n    # 1. Scheduled task creation/modification\n    schtasks_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"schtasks|at\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n    )\n    \n    schtasks_count = schtasks_persistence.count()\n    if schtasks_count > 0:\n        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n    \n    # 2. Service creation/modification\n    service_persistence = process_events.filter(\n        lower(col(\"FileName\")).rlike(\"sc\\.exe|net\\.exe\") &\n        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n    )\n    \n    service_count = service_persistence.count()\n    if service_count > 0:\n        persistence...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "9",
          "jobId": 107,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 18,
          "numTasks": 68,
          "rowCount": 8335,
          "stageIds": [
           195,
           196
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:33:53.086GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 15,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "MSGMedium",
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(MSGMedium, 39, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 DETECTING PERSISTENCE MECHANISMS...\n",
      "\n",
      "✅ No obvious persistence mechanisms detected\n",
      "   Continue monitoring for sophisticated techniques\n",
      "✅ No obvious persistence mechanisms detected\n",
      "   Continue monitoring for sophisticated techniques\n"
     ]
    }
   ],
   "source": [
    "# Advanced persistence detection\n",
    "def detect_persistence_mechanisms(process_events, registry_events=None):\n",
    "    \"\"\"\n",
    "    Detect various persistence mechanisms\n",
    "    \"\"\"\n",
    "    print(\"🔄 DETECTING PERSISTENCE MECHANISMS...\\n\")\n",
    "    \n",
    "    persistence_indicators = []\n",
    "    \n",
    "    # 1. Scheduled task creation/modification\n",
    "    schtasks_persistence = process_events.filter(\n",
    "        lower(col(\"FileName\")).rlike(\"schtasks|at\\\\.exe\") &\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"/create|/change|/run\")\n",
    "    )\n",
    "    \n",
    "    schtasks_count = schtasks_persistence.count()\n",
    "    if schtasks_count > 0:\n",
    "        persistence_indicators.append((\"Scheduled Tasks\", schtasks_count, schtasks_persistence))\n",
    "    \n",
    "    # 2. Service creation/modification\n",
    "    service_persistence = process_events.filter(\n",
    "        lower(col(\"FileName\")).rlike(\"sc\\\\.exe|net\\\\.exe\") &\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"create.*binpath|config.*binpath|start.*auto\")\n",
    "    )\n",
    "    \n",
    "    service_count = service_persistence.count()\n",
    "    if service_count > 0:\n",
    "        persistence_indicators.append((\"Service Persistence\", service_count, service_persistence))\n",
    "    \n",
    "    # 3. Registry-based persistence (RunKey modifications)\n",
    "    run_key_persistence = process_events.filter(\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"reg.*add.*run|reg.*add.*runonce\")\n",
    "    )\n",
    "    \n",
    "    run_key_count = run_key_persistence.count()\n",
    "    if run_key_count > 0:\n",
    "        persistence_indicators.append((\"Registry Run Keys\", run_key_count, run_key_persistence))\n",
    "    \n",
    "    # 4. DLL hijacking indicators (unusual DLL loads)\n",
    "    dll_hijacking = process_events.filter(\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"rundll32|regsvr32\") &\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"appdata|temp|public\")\n",
    "    )\n",
    "    \n",
    "    dll_count = dll_hijacking.count()\n",
    "    if dll_count > 0:\n",
    "        persistence_indicators.append((\"DLL Hijacking\", dll_count, dll_hijacking))\n",
    "    \n",
    "    # 5. WMI event subscription persistence\n",
    "    wmi_persistence = process_events.filter(\n",
    "        lower(col(\"ProcessCommandLine\")).rlike(\"wmic.*eventfilter|wmic.*consumer|wmic.*subscription\")\n",
    "    )\n",
    "    \n",
    "    wmi_count = wmi_persistence.count()\n",
    "    if wmi_count > 0:\n",
    "        persistence_indicators.append((\"WMI Persistence\", wmi_count, wmi_persistence))\n",
    "    \n",
    "    return persistence_indicators\n",
    "\n",
    "# Analyze persistence mechanisms\n",
    "persistence_results = detect_persistence_mechanisms(process_filtered) if 'process_filtered' in locals() else []\n",
    "\n",
    "if persistence_results:\n",
    "    print(f\"🔄 PERSISTENCE MECHANISMS DETECTED: {len(persistence_results)}\\n\")\n",
    "    \n",
    "    for mechanism, mechanism_count, data in persistence_results:\n",
    "        print(f\"🎯 {mechanism}: {mechanism_count} instances\")\n",
    "        \n",
    "        # Show details for each mechanism\n",
    "        mechanism_details = data.groupBy(\n",
    "            \"ProcessCommandLine\", \"AccountName\", \"DeviceName\"\n",
    "        ).agg(\n",
    "            spark_count(\"*\").alias(\"Count\")\n",
    "        ).orderBy(desc(\"Count\"))\n",
    "        \n",
    "        print(\"   Command examples:\")\n",
    "        mechanism_details.show(3, truncate=False)\n",
    "        print()\n",
    "    \n",
    "    # Risk assessment\n",
    "    total_persistence = sum(mechanism_count for _, mechanism_count, _ in persistence_results)\n",
    "    risk_level = \"HIGH\" if total_persistence >= 10 else \"MEDIUM\" if total_persistence >= 5 else \"LOW\"\n",
    "    \n",
    "    print(f\"🛡️  PERSISTENCE RISK LEVEL: {risk_level}\")\n",
    "    print(f\"   Total Indicators: {total_persistence}\")\n",
    "    \n",
    "    if risk_level in [\"HIGH\", \"MEDIUM\"]:\n",
    "        print(\"\\n🚨 IMMEDIATE ACTIONS REQUIRED:\")\n",
    "        print(\"   1. Review all scheduled tasks and services\")\n",
    "        print(\"   2. Check registry run keys for unauthorized entries\")\n",
    "        print(\"   3. Validate DLL authenticity and locations\")\n",
    "        print(\"   4. Audit WMI subscriptions and filters\")\n",
    "        print(\"   5. Implement application allowlisting\")\n",
    "        print(\"   6. Monitor for lateral movement patterns\")\n",
    "\n",
    "else:\n",
    "    print(\"✅ No obvious persistence mechanisms detected\")\n",
    "    print(\"   Continue monitoring for sophisticated techniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36dabf",
   "metadata": {},
   "source": [
    "## 4. Data Exfiltration Pattern Analysis\n",
    "\n",
    "Detect sophisticated data exfiltration patterns combining multiple indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fc7954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-03T09:35:13.5007282Z",
       "execution_start_time": "2025-09-03T09:35:13.2761485Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d6f15ab8-692b-4d2a-b902-73b90181306d",
       "queued_time": "2025-09-03T09:35:13.2746474Z",
       "session_id": "39",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 0,
         "SUCCEEDED": 0,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "MSGMedium",
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(MSGMedium, 39, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 ANALYZING DATA EXFILTRATION PATTERNS...\n",
      "\n",
      "✅ No obvious data exfiltration patterns detected\n",
      "   Continue monitoring for suspicious data movement\n"
     ]
    }
   ],
   "source": [
    "# Advanced data exfiltration detection\n",
    "def detect_data_exfiltration_patterns():\n",
    "    \"\"\"\n",
    "    Detect sophisticated data exfiltration using multiple data sources\n",
    "    \"\"\"\n",
    "    print(\"📤 ANALYZING DATA EXFILTRATION PATTERNS...\\n\")\n",
    "    \n",
    "    exfiltration_indicators = []\n",
    "    \n",
    "    try:\n",
    "        # 1. Large external data transfers - NOTE: Adapted for available columns\n",
    "        if 'network_filtered' in locals() and network_filtered is not None:\n",
    "            # Focus on connection frequency to external IPs as a proxy for data transfers\n",
    "            external_connections = network_filtered.groupBy(\n",
    "                \"DeviceName\", \"RemoteIP\"\n",
    "            ).agg(\n",
    "                spark_count(\"*\").alias(\"ConnectionCount\")\n",
    "            ).filter(col(\"ConnectionCount\") > 100)  # High connection frequency\n",
    "            \n",
    "            high_connection_count = external_connections.count()\n",
    "            if high_connection_count > 0:\n",
    "                exfiltration_indicators.append((\"High External Connection Frequency\", high_connection_count, external_connections))\n",
    "        \n",
    "        # 2. Archive creation before external transfers\n",
    "        if 'process_filtered' in locals():\n",
    "            archive_creation = process_filtered.filter(\n",
    "                lower(col(\"ProcessCommandLine\")).rlike(\n",
    "                    \"7z.*a |winrar.*a |zip.*-r|tar.*-czf|makecab\"\n",
    "                ) &\n",
    "                lower(col(\"ProcessCommandLine\")).rlike(\n",
    "                    \"documents|desktop|users|programdata|temp\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            archive_count = archive_creation.count()\n",
    "            if archive_count > 0:\n",
    "                exfiltration_indicators.append((\"Suspicious Archive Creation\", archive_count, archive_creation))\n",
    "        \n",
    "        # 3. Cloud storage tool usage\n",
    "        if 'process_filtered' in locals():\n",
    "            cloud_tools = process_filtered.filter(\n",
    "                lower(col(\"FileName\")).rlike(\n",
    "                    \"rclone|aws|gsutil|azcopy|dropbox|googledrive\"\n",
    "                ) |\n",
    "                lower(col(\"ProcessCommandLine\")).rlike(\n",
    "                    \"s3.*cp|blob.*upload|drive.*upload|dropbox.*upload\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            cloud_count = cloud_tools.count()\n",
    "            if cloud_count > 0:\n",
    "                exfiltration_indicators.append((\"Cloud Storage Tools\", cloud_count, cloud_tools))\n",
    "        \n",
    "        # 4. Encoded/encrypted data preparation\n",
    "        if 'process_filtered' in locals():\n",
    "            encoding_activities = process_filtered.filter(\n",
    "                lower(col(\"ProcessCommandLine\")).rlike(\n",
    "                    \"base64|certutil.*encode|openssl.*enc|gpg.*encrypt\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            encoding_count = encoding_activities.count()\n",
    "            if encoding_count > 0:\n",
    "                exfiltration_indicators.append((\"Data Encoding/Encryption\", encoding_count, encoding_activities))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error in exfiltration analysis: {str(e)}\")\n",
    "    \n",
    "    return exfiltration_indicators\n",
    "\n",
    "# Detect data exfiltration patterns\n",
    "exfiltration_results = detect_data_exfiltration_patterns()\n",
    "\n",
    "if exfiltration_results:\n",
    "    print(f\"📤 DATA EXFILTRATION INDICATORS DETECTED: {len(exfiltration_results)}\\n\")\n",
    "    \n",
    "    for indicator, count, data in exfiltration_results:\n",
    "        print(f\"🚨 {indicator}: {count} instances\")\n",
    "        \n",
    "        # Show relevant details based on indicator type\n",
    "        if \"Connection\" in indicator:\n",
    "            data.select(\n",
    "                \"DeviceName\", \"RemoteIP\", \"ConnectionCount\"\n",
    "            ).show(5, truncate=False)\n",
    "        else:\n",
    "            summary = data.groupBy(\"ProcessCommandLine\", \"AccountName\").agg(\n",
    "                spark_count(\"*\").alias(\"Count\"),\n",
    "                countDistinct(\"DeviceName\").alias(\"UniqueDevices\")\n",
    "            ).orderBy(desc(\"Count\"))\n",
    "            \n",
    "            summary.show(3, truncate=False)\n",
    "        print()\n",
    "    \n",
    "    # Risk assessment\n",
    "    total_indicators = len(exfiltration_results)\n",
    "    risk_level = \"HIGH\" if total_indicators >= 3 else \"MEDIUM\" if total_indicators >= 2 else \"LOW\"\n",
    "    \n",
    "    print(f\"🎯 EXFILTRATION RISK LEVEL: {risk_level}\")\n",
    "    print(f\"   Total Indicators: {total_indicators}\")\n",
    "    \n",
    "    if risk_level in [\"HIGH\", \"MEDIUM\"]:\n",
    "        print(\"\\n🚨 IMMEDIATE ACTIONS REQUIRED:\")\n",
    "        print(\"   1. Investigate all flagged activities immediately\")\n",
    "        print(\"   2. Review data access logs for affected users\")\n",
    "        print(\"   3. Check for unauthorized cloud storage usage\")\n",
    "        print(\"   4. Validate legitimate business purposes\")\n",
    "        print(\"   5. Consider temporary network restrictions\")\n",
    "        print(\"   6. Implement DLP policies if not already present\")\n",
    "    \n",
    "else:\n",
    "    print(\"✅ No obvious data exfiltration patterns detected\")\n",
    "    print(\"   Continue monitoring for suspicious data movement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0348d9e",
   "metadata": {},
   "source": [
    "## 5. User Behavior Analytics (UBA)\n",
    "\n",
    "Advanced behavioral analysis to detect anomalous user activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a759c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-03T09:35:25.3046302Z",
       "execution_start_time": "2025-09-03T09:35:13.7964153Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "cb8599c2-885e-4172-9fdf-0b0e31397849",
       "queued_time": "2025-09-03T09:35:13.7947247Z",
       "session_id": "39",
       "session_start_time": null,
       "spark_jobs": {
        "jobs": [
         {
          "dataRead": 1556,
          "dataWritten": 103,
          "description": "Job group for statement 11:\n# User Behavior Analytics\ndef perform_user_behavior_analysis():\n    \"\"\"\n    Perform advanced user behavior analysis across multiple data sources\n    \"\"\"\n    print(\"👤 USER BEHAVIOR ANALYTICS...\n\")\n    \n    try:\n        # Load sign-in data for behavioral analysis using smart table checking\n        if SENTINEL_ENVIRONMENT:\n            table_info = safe_table_check(\"SigninLogs\")\n            if table_info['available']:\n                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n                print(f\"✅ Loaded SigninLogs from {table_info['workspace']}\")\n            else:\n                print(f\"❌ SigninLogs not available: {table_info['error']}\")\n                signin_logs = None\n        else:\n            signin_logs = None\n        \n        if signin_logs is None:\n            print(\"⚠️ SigninLogs not available - skipping user behavior analysis\")\n            return\n        \n        # Filter to analysis window\n        signin_filtered = signin_logs.filter(\n            col(\"CreatedD...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "11",
          "jobId": 129,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 14,
          "rowCount": 15,
          "stageIds": [
           231,
           232
          ],
          "status": "RUNNING",
          "submissionTime": "2025-09-03T09:35:17.847GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:35:17.805GMT",
          "dataRead": 83196,
          "dataWritten": 1556,
          "description": "Job group for statement 11:\n# User Behavior Analytics\ndef perform_user_behavior_analysis():\n    \"\"\"\n    Perform advanced user behavior analysis across multiple data sources\n    \"\"\"\n    print(\"👤 USER BEHAVIOR ANALYTICS...\n\")\n    \n    try:\n        # Load sign-in data for behavioral analysis using smart table checking\n        if SENTINEL_ENVIRONMENT:\n            table_info = safe_table_check(\"SigninLogs\")\n            if table_info['available']:\n                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n                print(f\"✅ Loaded SigninLogs from {table_info['workspace']}\")\n            else:\n                print(f\"❌ SigninLogs not available: {table_info['error']}\")\n                signin_logs = None\n        else:\n            signin_logs = None\n        \n        if signin_logs is None:\n            print(\"⚠️ SigninLogs not available - skipping user behavior analysis\")\n            return\n        \n        # Filter to analysis window\n        signin_filtered = signin_logs.filter(\n            col(\"CreatedD...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "11",
          "jobId": 128,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 13,
          "numCompletedStages": 1,
          "numCompletedTasks": 13,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 13,
          "rowCount": 52,
          "stageIds": [
           230
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:35:17.519GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:35:17.721GMT",
          "dataRead": 181956,
          "dataWritten": 1556,
          "description": "Job group for statement 11:\n# User Behavior Analytics\ndef perform_user_behavior_analysis():\n    \"\"\"\n    Perform advanced user behavior analysis across multiple data sources\n    \"\"\"\n    print(\"👤 USER BEHAVIOR ANALYTICS...\n\")\n    \n    try:\n        # Load sign-in data for behavioral analysis using smart table checking\n        if SENTINEL_ENVIRONMENT:\n            table_info = safe_table_check(\"SigninLogs\")\n            if table_info['available']:\n                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n                print(f\"✅ Loaded SigninLogs from {table_info['workspace']}\")\n            else:\n                print(f\"❌ SigninLogs not available: {table_info['error']}\")\n                signin_logs = None\n        else:\n            signin_logs = None\n        \n        if signin_logs is None:\n            print(\"⚠️ SigninLogs not available - skipping user behavior analysis\")\n            return\n        \n        # Filter to analysis window\n        signin_filtered = signin_logs.filter(\n            col(\"CreatedD...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "11",
          "jobId": 127,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 13,
          "numCompletedStages": 1,
          "numCompletedTasks": 13,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 13,
          "rowCount": 52,
          "stageIds": [
           229
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:35:17.461GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:35:17.267GMT",
          "dataRead": 43704,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 11:\n# User Behavior Analytics\ndef perform_user_behavior_analysis():\n    \"\"\"\n    Perform advanced user behavior analysis across multiple data sources\n    \"\"\"\n    print(\"👤 USER BEHAVIOR ANALYTICS...\n\")\n    \n    try:\n        # Load sign-in data for behavioral analysis using smart table checking\n        if SENTINEL_ENVIRONMENT:\n            table_info = safe_table_check(\"SigninLogs\")\n            if table_info['available']:\n                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n                print(f\"✅ Loaded SigninLogs from {table_info['workspace']}\")\n            else:\n                print(f\"❌ SigninLogs not available: {table_info['error']}\")\n                signin_logs = None\n        else:\n            signin_logs = None\n        \n        if signin_logs is None:\n            print(\"⚠️ SigninLogs not available - skipping user behavior analysis\")\n            return\n        \n        # Filter to analysis window\n        signin_filtered = signin_logs.filter(\n            col(\"CreatedD...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "11",
          "jobId": 126,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 14,
          "numTasks": 64,
          "rowCount": 172,
          "stageIds": [
           227,
           228
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:35:17.149GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:35:17.049GMT",
          "dataRead": 43704,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 11:\n# User Behavior Analytics\ndef perform_user_behavior_analysis():\n    \"\"\"\n    Perform advanced user behavior analysis across multiple data sources\n    \"\"\"\n    print(\"👤 USER BEHAVIOR ANALYTICS...\n\")\n    \n    try:\n        # Load sign-in data for behavioral analysis using smart table checking\n        if SENTINEL_ENVIRONMENT:\n            table_info = safe_table_check(\"SigninLogs\")\n            if table_info['available']:\n                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n                print(f\"✅ Loaded SigninLogs from {table_info['workspace']}\")\n            else:\n                print(f\"❌ SigninLogs not available: {table_info['error']}\")\n                signin_logs = None\n        else:\n            signin_logs = None\n        \n        if signin_logs is None:\n            print(\"⚠️ SigninLogs not available - skipping user behavior analysis\")\n            return\n        \n        # Filter to analysis window\n        signin_filtered = signin_logs.filter(\n            col(\"CreatedD...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "11",
          "jobId": 125,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 14,
          "numTasks": 64,
          "rowCount": 172,
          "stageIds": [
           225,
           226
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:35:16.907GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:35:16.828GMT",
          "dataRead": 43704,
          "dataWritten": 0,
          "description": "Delta: Job group for statement 11:\n# User Behavior Analytics\ndef perform_user_behavior_analysis():\n    \"\"\"\n    Perform advanced user behavior analysis across multiple data sources\n    \"\"\"\n    print(\"👤 USER BEHAVIOR ANALYTICS...\n\")\n    \n    try:\n        # Load sign-in data for behavioral analysis using smart table checking\n        if SENTINEL_ENVIRONMENT:\n            table_info = safe_table_check(\"SigninLogs\")\n            if table_info['available']:\n                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n                print(f\"✅ Loaded SigninLogs from {table_info['workspace']}\")\n            else:\n                print(f\"❌ SigninLogs not available: {table_info['error']}\")\n                signin_logs = None\n        else:\n            signin_logs = None\n        \n        if signin_logs is None:\n            print(\"⚠️ SigninLogs not available - skipping user behavior analysis\")\n            return\n        \n        # Filter to analysis window\n        signin_filtered = signin_logs.filter(\n            col(\"CreatedD...: Filtering files for query",
          "displayName": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "jobGroup": "11",
          "jobId": 124,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:111",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 50,
          "numCompletedStages": 1,
          "numCompletedTasks": 50,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 14,
          "numTasks": 64,
          "rowCount": 172,
          "stageIds": [
           223,
           224
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:35:16.670GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:35:15.554GMT",
          "dataRead": 0,
          "dataWritten": 0,
          "description": "Job group for statement 11:\n# User Behavior Analytics\ndef perform_user_behavior_analysis():\n    \"\"\"\n    Perform advanced user behavior analysis across multiple data sources\n    \"\"\"\n    print(\"👤 USER BEHAVIOR ANALYTICS...\n\")\n    \n    try:\n        # Load sign-in data for behavioral analysis using smart table checking\n        if SENTINEL_ENVIRONMENT:\n            table_info = safe_table_check(\"SigninLogs\")\n            if table_info['available']:\n                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n                print(f\"✅ Loaded SigninLogs from {table_info['workspace']}\")\n            else:\n                print(f\"❌ SigninLogs not available: {table_info['error']}\")\n                signin_logs = None\n        else:\n            signin_logs = None\n        \n        if signin_logs is None:\n            print(\"⚠️ SigninLogs not available - skipping user behavior analysis\")\n            return\n        \n        # Filter to analysis window\n        signin_filtered = signin_logs.filter(\n            col(\"CreatedD...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "11",
          "jobId": 123,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 1,
          "numCompletedStages": 1,
          "numCompletedTasks": 1,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 1,
          "numSkippedTasks": 23,
          "numTasks": 24,
          "rowCount": 0,
          "stageIds": [
           222,
           221
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:35:15.531GMT",
          "usageDescription": ""
         },
         {
          "completionTime": "2025-09-03T09:35:15.513GMT",
          "dataRead": 160831,
          "dataWritten": 1184,
          "description": "Job group for statement 11:\n# User Behavior Analytics\ndef perform_user_behavior_analysis():\n    \"\"\"\n    Perform advanced user behavior analysis across multiple data sources\n    \"\"\"\n    print(\"👤 USER BEHAVIOR ANALYTICS...\n\")\n    \n    try:\n        # Load sign-in data for behavioral analysis using smart table checking\n        if SENTINEL_ENVIRONMENT:\n            table_info = safe_table_check(\"SigninLogs\")\n            if table_info['available']:\n                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n                print(f\"✅ Loaded SigninLogs from {table_info['workspace']}\")\n            else:\n                print(f\"❌ SigninLogs not available: {table_info['error']}\")\n                signin_logs = None\n        else:\n            signin_logs = None\n        \n        if signin_logs is None:\n            print(\"⚠️ SigninLogs not available - skipping user behavior analysis\")\n            return\n        \n        # Filter to analysis window\n        signin_filtered = signin_logs.filter(\n            col(\"CreatedD...",
          "displayName": "count at <unknown>:0",
          "jobGroup": "11",
          "jobId": 122,
          "jobTags": [],
          "killedTasksSummary": {},
          "name": "count at <unknown>:0",
          "numActiveStages": 0,
          "numActiveTasks": 0,
          "numCompletedIndices": 23,
          "numCompletedStages": 1,
          "numCompletedTasks": 23,
          "numFailedStages": 0,
          "numFailedTasks": 0,
          "numKilledTasks": 0,
          "numSkippedStages": 0,
          "numSkippedTasks": 0,
          "numTasks": 23,
          "rowCount": 204,
          "stageIds": [
           220
          ],
          "status": "SUCCEEDED",
          "submissionTime": "2025-09-03T09:35:15.263GMT",
          "usageDescription": ""
         }
        ],
        "limit": 20,
        "numbers": {
         "FAILED": 0,
         "RUNNING": 1,
         "SUCCEEDED": 7,
         "UNKNOWN": 0
        },
        "rule": "ALL_DESC"
       },
       "spark_pool": "MSGMedium",
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(MSGMedium, 39, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 USER BEHAVIOR ANALYTICS...\n",
      "\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: SigninLogs\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table SigninLogs\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: SigninLogs\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table SigninLogs\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Loading table: SigninLogs\"}\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table SigninLogs\"}\n",
      "✅ Loaded SigninLogs from ak-SecOps\n",
      "{\"level\": \"INFO\", \"run_id\": \"c538f690-2d90-44c0-92e9-6903ff10ab25\", \"message\": \"Successfully loaded table SigninLogs\"}\n",
      "✅ Loaded SigninLogs from ak-SecOps\n",
      "\n",
      "🌍 NEW COUNTRY SIGN-INS: 1\n",
      "\n",
      "🌍 NEW COUNTRY SIGN-INS: 1\n",
      "+--------------------------------------+-------+--------------------+\n",
      "|UserPrincipalName                     |Country|UserDisplayName     |\n",
      "+--------------------------------------+-------+--------------------+\n",
      "|admin@mngenvmcap183476.onmicrosoft.com|US     |System Administrator|\n",
      "+--------------------------------------+-------+--------------------+\n",
      "\n",
      "+--------------------------------------+-------+--------------------+\n",
      "|UserPrincipalName                     |Country|UserDisplayName     |\n",
      "+--------------------------------------+-------+--------------------+\n",
      "|admin@mngenvmcap183476.onmicrosoft.com|US     |System Administrator|\n",
      "+--------------------------------------+-------+--------------------+\n",
      "\n",
      "\n",
      "🎯 USER BEHAVIOR ANALYSIS SUMMARY:\n",
      "   Total Behavioral Anomalies: 1\n",
      "\n",
      "🔍 INVESTIGATION PRIORITIES:\n",
      "   1. Review users with multiple anomaly types\n",
      "   2. Correlate with recent security events\n",
      "   3. Verify account compromise indicators\n",
      "   4. Check for privilege escalation attempts\n",
      "   5. Monitor for additional suspicious activities\n",
      "\n",
      "🎯 USER BEHAVIOR ANALYSIS SUMMARY:\n",
      "   Total Behavioral Anomalies: 1\n",
      "\n",
      "🔍 INVESTIGATION PRIORITIES:\n",
      "   1. Review users with multiple anomaly types\n",
      "   2. Correlate with recent security events\n",
      "   3. Verify account compromise indicators\n",
      "   4. Check for privilege escalation attempts\n",
      "   5. Monitor for additional suspicious activities\n"
     ]
    }
   ],
   "source": [
    "# User Behavior Analytics\n",
    "def perform_user_behavior_analysis():\n",
    "    \"\"\"\n",
    "    Perform advanced user behavior analysis across multiple data sources\n",
    "    \"\"\"\n",
    "    print(\"👤 USER BEHAVIOR ANALYTICS...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load sign-in data for behavioral analysis using smart table checking\n",
    "        if SENTINEL_ENVIRONMENT:\n",
    "            table_info = safe_table_check(\"SigninLogs\")\n",
    "            if table_info['available']:\n",
    "                signin_logs = data_provider.read_table(\"SigninLogs\", table_info['workspace'])\n",
    "                print(f\"✅ Loaded SigninLogs from {table_info['workspace']}\")\n",
    "            else:\n",
    "                print(f\"❌ SigninLogs not available: {table_info['error']}\")\n",
    "                signin_logs = None\n",
    "        else:\n",
    "            signin_logs = None\n",
    "        \n",
    "        if signin_logs is None:\n",
    "            print(\"⚠️ SigninLogs not available - skipping user behavior analysis\")\n",
    "            return\n",
    "        \n",
    "        # Filter to analysis window\n",
    "        signin_filtered = signin_logs.filter(\n",
    "            col(\"CreatedDateTime\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS*2} HOURS\"))  # Longer window for baseline\n",
    "        )\n",
    "        \n",
    "        # 1. Unusual time-based patterns\n",
    "        user_time_patterns = signin_filtered.withColumn(\n",
    "            \"HourOfDay\", hour(col(\"CreatedDateTime\"))\n",
    "        ).withColumn(\n",
    "            \"DayOfWeek\", dayofweek(col(\"CreatedDateTime\"))\n",
    "        ).groupBy(\n",
    "            \"UserPrincipalName\", \"HourOfDay\", \"DayOfWeek\"\n",
    "        ).agg(\n",
    "            spark_count(\"*\").alias(\"SignInCount\")\n",
    "        )\n",
    "        \n",
    "        # Calculate user's normal hours (hours with >10% of their activity)\n",
    "        user_total_signins = user_time_patterns.groupBy(\"UserPrincipalName\").agg(\n",
    "            spark_sum(\"SignInCount\").alias(\"TotalSignIns\")\n",
    "        )\n",
    "        \n",
    "        user_normal_hours = user_time_patterns.join(\n",
    "            user_total_signins, \"UserPrincipalName\"\n",
    "        ).withColumn(\n",
    "            \"ActivityPercentage\", col(\"SignInCount\") / col(\"TotalSignIns\")\n",
    "        ).filter(\n",
    "            col(\"ActivityPercentage\") > 0.05  # Hours with >5% of activity\n",
    "        )\n",
    "        \n",
    "        # Find recent sign-ins outside normal patterns\n",
    "        recent_signins = signin_filtered.filter(\n",
    "            col(\"CreatedDateTime\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "        ).withColumn(\n",
    "            \"HourOfDay\", hour(col(\"CreatedDateTime\"))\n",
    "        ).withColumn(\n",
    "            \"DayOfWeek\", dayofweek(col(\"CreatedDateTime\"))\n",
    "        )\n",
    "        \n",
    "        # Anti-join to find sign-ins outside normal patterns\n",
    "        anomalous_time_signins = recent_signins.join(\n",
    "            user_normal_hours.select(\"UserPrincipalName\", \"HourOfDay\", \"DayOfWeek\"),\n",
    "            [\"UserPrincipalName\", \"HourOfDay\", \"DayOfWeek\"],\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        \n",
    "        anomalous_time_count = anomalous_time_signins.count()\n",
    "        \n",
    "        if anomalous_time_count > 0:\n",
    "            print(f\"⏰ ANOMALOUS TIME-BASED SIGN-INS: {anomalous_time_count}\")\n",
    "            \n",
    "            time_anomalies = anomalous_time_signins.groupBy(\n",
    "                \"UserPrincipalName\", \"UserDisplayName\"\n",
    "            ).agg(\n",
    "                spark_count(\"*\").alias(\"AnomalousSignIns\"),\n",
    "                countDistinct(\"IPAddress\").alias(\"UniqueIPs\")\n",
    "            ).orderBy(desc(\"AnomalousSignIns\"))\n",
    "            \n",
    "            time_anomalies.show(10, truncate=False)\n",
    "        \n",
    "        # 2. Geographic anomalies\n",
    "        if signin_filtered.filter(col(\"LocationDetails\").isNotNull()).count() > 0:\n",
    "            location_schema = StructType([\n",
    "                StructField(\"countryOrRegion\", StringType(), True)\n",
    "            ])\n",
    "            \n",
    "            user_locations = signin_filtered.filter(\n",
    "                col(\"LocationDetails\").isNotNull()\n",
    "            ).withColumn(\n",
    "                \"Country\", from_json(col(\"LocationDetails\"), location_schema).getField(\"countryOrRegion\")\n",
    "            )\n",
    "            \n",
    "            # Baseline countries for each user\n",
    "            baseline_countries = user_locations.filter(\n",
    "                col(\"CreatedDateTime\") < (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "            ).select(\"UserPrincipalName\", \"Country\", \"UserDisplayName\").distinct()\n",
    "            \n",
    "            # Recent countries\n",
    "            recent_countries = user_locations.filter(\n",
    "                col(\"CreatedDateTime\") >= (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "            ).select(\"UserPrincipalName\", \"Country\", \"UserDisplayName\").distinct()\n",
    "            \n",
    "            new_country_signins = recent_countries.join(\n",
    "                baseline_countries,\n",
    "                [\"UserPrincipalName\", \"Country\"],\n",
    "                \"left_anti\"\n",
    "            )\n",
    "            \n",
    "            new_country_count = new_country_signins.count()\n",
    "            \n",
    "            if new_country_count > 0:\n",
    "                print(f\"\\n🌍 NEW COUNTRY SIGN-INS: {new_country_count}\")\n",
    "                new_country_signins.show(10, truncate=False)\n",
    "        \n",
    "        # 3. Application usage anomalies\n",
    "        app_anomalies = recent_signins.filter(\n",
    "            col(\"AppDisplayName\").isNotNull()\n",
    "        ).groupBy(\n",
    "            \"UserPrincipalName\", \"AppDisplayName\"\n",
    "        ).agg(\n",
    "            spark_count(\"*\").alias(\"RecentUsage\")\n",
    "        )\n",
    "        \n",
    "        # Find apps used in baseline period\n",
    "        baseline_app_usage = signin_filtered.filter(\n",
    "            col(\"CreatedDateTime\") < (current_timestamp() - expr(f\"INTERVAL {ANALYSIS_HOURS} HOURS\"))\n",
    "        ).filter(\n",
    "            col(\"AppDisplayName\").isNotNull()\n",
    "        ).groupBy(\n",
    "            \"UserPrincipalName\", \"AppDisplayName\"\n",
    "        ).agg(\n",
    "            spark_count(\"*\").alias(\"BaselineUsage\")\n",
    "        )\n",
    "        \n",
    "        app_usage_anomalies = app_anomalies.join(\n",
    "            baseline_app_usage,\n",
    "            [\"UserPrincipalName\", \"AppDisplayName\"],\n",
    "            \"left\"\n",
    "        ).filter(\n",
    "            col(\"RecentUsage\") > (col(\"BaselineUsage\") * 3)  # 3x normal usage\n",
    "        ).filter(\n",
    "            col(\"RecentUsage\") > 5  # At least 5 sign-ins\n",
    "        )\n",
    "        \n",
    "        app_anomaly_count = app_usage_anomalies.count()\n",
    "        \n",
    "        if app_anomaly_count > 0:\n",
    "            print(f\"\\n📱 APPLICATION USAGE ANOMALIES: {app_anomaly_count}\")\n",
    "            app_usage_anomalies.orderBy(desc(\"RecentUsage\")).show(10, truncate=False)\n",
    "        \n",
    "        # Summary\n",
    "        total_anomalies = sum([\n",
    "            anomalous_time_count if 'anomalous_time_count' in locals() else 0,\n",
    "            new_country_count if 'new_country_count' in locals() else 0,\n",
    "            app_anomaly_count if 'app_anomaly_count' in locals() else 0\n",
    "        ])\n",
    "        \n",
    "        if total_anomalies > 0:\n",
    "            print(f\"\\n🎯 USER BEHAVIOR ANALYSIS SUMMARY:\")\n",
    "            print(f\"   Total Behavioral Anomalies: {total_anomalies}\")\n",
    "            print(\"\\n🔍 INVESTIGATION PRIORITIES:\")\n",
    "            print(\"   1. Review users with multiple anomaly types\")\n",
    "            print(\"   2. Correlate with recent security events\")\n",
    "            print(\"   3. Verify account compromise indicators\")\n",
    "            print(\"   4. Check for privilege escalation attempts\")\n",
    "            print(\"   5. Monitor for additional suspicious activities\")\n",
    "        else:\n",
    "            print(\"✅ No significant user behavior anomalies detected\")\n",
    "        \n",
    "        # Return total anomalies for summary\n",
    "        return total_anomalies\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error in user behavior analysis: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# Perform user behavior analysis\n",
    "total_anomalies = perform_user_behavior_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940c568",
   "metadata": {},
   "source": [
    "## 6. Advanced Threat Hunting Summary\n",
    "\n",
    "Comprehensive summary and threat assessment based on all analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e026ad9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-09-03T09:35:25.9119556Z",
       "execution_start_time": "2025-09-03T09:35:25.6772533Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "ed602593-2a0f-4981-bfc3-5786ef60d809",
       "queued_time": "2025-09-03T09:35:25.676054Z",
       "session_id": "39",
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": "MSGMedium",
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(MSGMedium, 39, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 ADVANCED THREAT HUNTING SUMMARY\n",
      "==================================================\n",
      "📊 THREAT HUNTING RESULTS (24-hour analysis):\n",
      "   🚨 C2 Beacons: 1\n",
      "   🚨 Living off the Land: 1\n",
      "   ✅ Persistence Mechanisms: 0\n",
      "   ✅ Data Exfiltration Indicators: 0\n",
      "   🚨 Behavioral Anomalies: 1\n",
      "\n",
      "🟠 OVERALL THREAT LEVEL: HIGH (Score: 40/100)\n",
      "\n",
      "🎯 THREAT-SPECIFIC RECOMMENDATIONS:\n",
      "   ⚠️  ELEVATED THREAT - IMMEDIATE INVESTIGATION\n",
      "   1. Begin detailed investigation of all findings\n",
      "   2. Implement enhanced monitoring\n",
      "   3. Consider network segmentation\n",
      "   4. Review and update security policies\n",
      "   5. Increase security team alertness\n",
      "   6. Prepare for potential escalation\n",
      "\n",
      "🔄 CONTINUOUS IMPROVEMENT:\n",
      "   📊 Update baselines with new data\n",
      "   🎓 Train analysts on identified techniques\n",
      "   🔧 Tune detection rules based on findings\n",
      "   📝 Document lessons learned\n",
      "   🕒 Schedule regular threat hunting cycles\n",
      "\n",
      "📅 Analysis completed: 2025-09-03 09:35:25\n",
      "🎯 Next recommended analysis: 2025-09-10\n",
      "✨ Advanced threat hunting cycle complete!\n",
      "\n",
      "📋 EXECUTIVE SUMMARY FOR STAKEHOLDERS:\n",
      "   • Threat Level: HIGH\n",
      "   • Critical Findings: 1\n",
      "   • Analysis Period: 24 hours\n",
      "   • Total Indicators: 3\n",
      "   • Immediate action required for security posture\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive threat hunting summary\n",
    "print(\"🎯 ADVANCED THREAT HUNTING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all findings\n",
    "threat_findings = {}\n",
    "\n",
    "if 'beacon_count' in locals():\n",
    "    threat_findings['C2 Beacons'] = beacon_count\n",
    "if 'lotl_results' in locals():\n",
    "    threat_findings['Living off the Land'] = len(lotl_results)\n",
    "if 'persistence_results' in locals():\n",
    "    threat_findings['Persistence Mechanisms'] = len(persistence_results)\n",
    "if 'exfiltration_results' in locals():\n",
    "    threat_findings['Data Exfiltration Indicators'] = len(exfiltration_results)\n",
    "if 'total_anomalies' in locals():\n",
    "    threat_findings['Behavioral Anomalies'] = total_anomalies\n",
    "\n",
    "# Display findings\n",
    "print(f\"📊 THREAT HUNTING RESULTS ({ANALYSIS_HOURS}-hour analysis):\")\n",
    "for finding, count in threat_findings.items():\n",
    "    status = \"🚨\" if count > 0 else \"✅\"\n",
    "    print(f\"   {status} {finding}: {count}\")\n",
    "\n",
    "# Calculate overall threat score\n",
    "threat_score = 0\n",
    "critical_findings = 0\n",
    "\n",
    "# Weight different finding types\n",
    "if threat_findings.get('C2 Beacons', 0) > 0:\n",
    "    threat_score += 40\n",
    "    critical_findings += 1\n",
    "if threat_findings.get('Data Exfiltration Indicators', 0) >= 2:\n",
    "    threat_score += 30\n",
    "    critical_findings += 1\n",
    "if threat_findings.get('Living off the Land', 0) >= 3:\n",
    "    threat_score += 20\n",
    "if threat_findings.get('Persistence Mechanisms', 0) >= 2:\n",
    "    threat_score += 15\n",
    "if threat_findings.get('Behavioral Anomalies', 0) >= 5:\n",
    "    threat_score += 10\n",
    "\n",
    "# Determine threat level\n",
    "if threat_score >= 50:\n",
    "    threat_level = \"CRITICAL\"\n",
    "    color = \"🔴\"\n",
    "elif threat_score >= 30:\n",
    "    threat_level = \"HIGH\"\n",
    "    color = \"🟠\"\n",
    "elif threat_score >= 15:\n",
    "    threat_level = \"MEDIUM\"\n",
    "    color = \"🟡\"\n",
    "else:\n",
    "    threat_level = \"LOW\"\n",
    "    color = \"🟢\"\n",
    "\n",
    "print(f\"\\n{color} OVERALL THREAT LEVEL: {threat_level} (Score: {threat_score}/100)\")\n",
    "\n",
    "# Provide specific recommendations based on findings\n",
    "print(f\"\\n🎯 THREAT-SPECIFIC RECOMMENDATIONS:\")\n",
    "\n",
    "if threat_level == \"CRITICAL\":\n",
    "    print(\"   🚨 IMMEDIATE INCIDENT RESPONSE REQUIRED\")\n",
    "    print(\"   1. Activate incident response team\")\n",
    "    print(\"   2. Isolate affected systems immediately\")\n",
    "    print(\"   3. Preserve evidence for forensic analysis\")\n",
    "    print(\"   4. Reset credentials for affected accounts\")\n",
    "    print(\"   5. Implement emergency containment measures\")\n",
    "    print(\"   6. Contact legal and compliance teams\")\n",
    "    print(\"   7. Prepare external communications if needed\")\n",
    "\n",
    "elif threat_level == \"HIGH\":\n",
    "    print(\"   ⚠️  ELEVATED THREAT - IMMEDIATE INVESTIGATION\")\n",
    "    print(\"   1. Begin detailed investigation of all findings\")\n",
    "    print(\"   2. Implement enhanced monitoring\")\n",
    "    print(\"   3. Consider network segmentation\")\n",
    "    print(\"   4. Review and update security policies\")\n",
    "    print(\"   5. Increase security team alertness\")\n",
    "    print(\"   6. Prepare for potential escalation\")\n",
    "\n",
    "elif threat_level == \"MEDIUM\":\n",
    "    print(\"   🔍 ACTIVE MONITORING AND INVESTIGATION\")\n",
    "    print(\"   1. Investigate flagged activities systematically\")\n",
    "    print(\"   2. Validate findings with additional context\")\n",
    "    print(\"   3. Implement targeted monitoring\")\n",
    "    print(\"   4. Review security controls effectiveness\")\n",
    "    print(\"   5. Update threat hunting playbooks\")\n",
    "\n",
    "else:\n",
    "    print(\"   ✅ BASELINE SECURITY POSTURE MAINTAINED\")\n",
    "    print(\"   1. Continue regular monitoring\")\n",
    "    print(\"   2. Maintain current security controls\")\n",
    "    print(\"   3. Schedule next threat hunting cycle\")\n",
    "    print(\"   4. Review and update hunting queries\")\n",
    "    print(\"   5. Train team on new techniques\")\n",
    "\n",
    "# Next steps and continuous improvement\n",
    "print(f\"\\n🔄 CONTINUOUS IMPROVEMENT:\")\n",
    "print(\"   📊 Update baselines with new data\")\n",
    "print(\"   🎓 Train analysts on identified techniques\")\n",
    "print(\"   🔧 Tune detection rules based on findings\")\n",
    "print(\"   📝 Document lessons learned\")\n",
    "print(\"   🕒 Schedule regular threat hunting cycles\")\n",
    "\n",
    "print(f\"\\n📅 Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"🎯 Next recommended analysis: {(datetime.now() + timedelta(days=7)).strftime('%Y-%m-%d')}\")\n",
    "print(\"✨ Advanced threat hunting cycle complete!\")\n",
    "\n",
    "# Generate hunting report summary\n",
    "if threat_level in [\"CRITICAL\", \"HIGH\"]:\n",
    "    print(\"\\n📋 EXECUTIVE SUMMARY FOR STAKEHOLDERS:\")\n",
    "    print(f\"   • Threat Level: {threat_level}\")\n",
    "    print(f\"   • Critical Findings: {critical_findings}\")\n",
    "    print(f\"   • Analysis Period: {ANALYSIS_HOURS} hours\")\n",
    "    print(f\"   • Total Indicators: {sum(threat_findings.values())}\")\n",
    "    print(\"   • Immediate action required for security posture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medium pool (8 vCores) [04_Advanced_Threat_Hunting]",
   "language": "Python",
   "name": "MSGMedium"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
