{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f6deba",
   "metadata": {},
   "source": [
    "# üîê Service Principal Sign-In & Risk Analysis - Microsoft Sentinel Data Lake\n",
    "\n",
    "Actionable analytics and security hunting over Azure AD (Entra ID) service principal activity using `AADServicePrincipalSignInLogs` and optional risk enrichment from `AADRiskyServicePrincipals`.\n",
    "\n",
    "## üéØ Objectives\n",
    "- Understand service principal authentication patterns\n",
    "- Detect abnormal spikes, failures, geographic or resource anomalies\n",
    "- Identify risky / high-impact service principals\n",
    "\n",
    "## üöÄ Zero-Config Loading\n",
    "This notebook attempts to load required tables automatically (no `PRIMARY_WORKSPACE` variable). It will:\n",
    "1. Try `data_provider.read_table(table)` directly\n",
    "2. If that fails, try common fallback workspace names (e.g. `default`, `ak-SecOps`)\n",
    "3. Gracefully continue if a table is missing\n",
    "\n",
    "You can later introduce explicit workspace mapping if needed for multi-workspace deployments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb60208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports & Initialization (Zero Manual Workspace Config)\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "from pyspark.sql.functions import (\n",
    "    col, lower, upper, count as spark_count, countDistinct, expr, when, avg, stddev,\n",
    "    date_trunc, hour, dayofweek, to_timestamp, current_timestamp, lit, percentile_approx\n",
    ")\n",
    "from pyspark.sql import DataFrame\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import math, statistics, json, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "print('‚úÖ Environment initialized (no workspace variables required)')\n",
    "\n",
    "TARGET_HOURS = 24  # Analysis window\n",
    "FALLBACK_WORKSPACES = ['default', 'ak-SecOps']  # Attempted only if direct load fails\n",
    "SERVICE_PRINCIPAL_TABLE_CANDIDATES = [\n",
    "    'AADServicePrincipalSignInLogs',\n",
    "    'ServicePrincipalSignInLogs',\n",
    "    'AADServicePrincipalSignIns'\n",
    "]\n",
    "TABLES_OPTIONAL = ['AADRiskyServicePrincipals']\n",
    "\n",
    "\n",
    "def try_read(table_name: str, workspace: str | None):\n",
    "    \"\"\"Attempt a read with optional workspace (positional second arg).\"\"\"\n",
    "    if workspace:\n",
    "        return data_provider.read_table(table_name, workspace)\n",
    "    return data_provider.read_table(table_name)\n",
    "\n",
    "\n",
    "def smart_load(table_name: str):\n",
    "    \"\"\"Attempt to load table with minimal assumptions.\n",
    "    Strategy:\n",
    "      1. Direct read (implicit/default binding)\n",
    "      2. Fallback workspaces (positional second argument)\n",
    "      3. Return (df, source_workspace, error)\n",
    "    \"\"\"\n",
    "    last_error = None\n",
    "    # 1. Direct\n",
    "    try:\n",
    "        df = try_read(table_name, None)\n",
    "        return df, 'auto', None\n",
    "    except Exception as e:\n",
    "        last_error = str(e)\n",
    "    # 2. Workspaces\n",
    "    for ws in FALLBACK_WORKSPACES:\n",
    "        try:\n",
    "            df = try_read(table_name, ws)\n",
    "            return df, ws, None\n",
    "        except Exception as e2:\n",
    "            last_error = str(e2)\n",
    "    return None, None, last_error\n",
    "\n",
    "loaded = {}\n",
    "\n",
    "# Try all candidate service principal tables until one loads\n",
    "sp_df = None\n",
    "sp_workspace = None\n",
    "sp_error_chain = []\n",
    "for candidate in SERVICE_PRINCIPAL_TABLE_CANDIDATES:\n",
    "    df, ws, err = smart_load(candidate)\n",
    "    if df is not None:\n",
    "        sp_df = df\n",
    "        sp_workspace = ws\n",
    "        print(f\"‚úÖ Loaded service principal sign-ins from '{candidate}' (workspace={ws})\")\n",
    "        break\n",
    "    else:\n",
    "        sp_error_chain.append(f\"{candidate}: {err}\")\n",
    "\n",
    "if sp_df is None:\n",
    "    print('‚ö†Ô∏è No dedicated service principal sign-in table found. Attempting to derive from SigninLogs...')\n",
    "    signin_df, signin_ws, signin_err = smart_load('SigninLogs')\n",
    "    if signin_df is not None:\n",
    "        print(f\"üîÑ Deriving service principal activity from SigninLogs (workspace={signin_ws})\")\n",
    "        # Heuristic filters (schema dependent). We keep flexible checks.\n",
    "        candidate_cols = signin_df.columns\n",
    "        derived = signin_df\n",
    "        if 'ServicePrincipalId' in candidate_cols:\n",
    "            derived = derived.filter(col('ServicePrincipalId').isNotNull())\n",
    "        elif 'AppId' in candidate_cols and 'UserId' in candidate_cols:\n",
    "            # Filter where UserId null but AppId present (often service principal)\n",
    "            derived = derived.filter(col('AppId').isNotNull() & col('UserId').isNull())\n",
    "        elif 'AppId' in candidate_cols:\n",
    "            derived = derived.filter(col('AppId').isNotNull())\n",
    "        sp_df = derived\n",
    "        sp_workspace = signin_ws if signin_ws else 'derived'\n",
    "    else:\n",
    "        print('‚ùå Could not derive from SigninLogs either.')\n",
    "        for e in sp_error_chain:\n",
    "            print('   ‚Ä¢ ' + e)\n",
    "\n",
    "# Optional risk table\n",
    "risk_df, risk_ws, risk_err = smart_load('AADRiskyServicePrincipals') if sp_df is not None else (None, None, None)\n",
    "if risk_df is not None:\n",
    "    print(f\"üß© Loaded AADRiskyServicePrincipals (workspace={risk_ws})\")\n",
    "else:\n",
    "    if risk_err:\n",
    "        print(f\"‚ÑπÔ∏è Risk enrichment table not available: {risk_err}\")\n",
    "    else:\n",
    "        print('‚ÑπÔ∏è Risk enrichment skipped (service principal data missing)')\n",
    "\n",
    "# If still no service principal sign-ins, stop early\n",
    "if sp_df is None:\n",
    "    raise RuntimeError('No service principal sign-in data available from candidates or SigninLogs.')\n",
    "\n",
    "service_principal_signins = sp_df\n",
    "risky_principals = risk_df\n",
    "\n",
    "# Filter to analysis window if a recognizable time column exists\n",
    "TIME_COLUMN_CANDIDATES = ['CreatedDateTime', 'TimeGenerated', 'Timestamp']\n",
    "actual_time_col = None\n",
    "for c in TIME_COLUMN_CANDIDATES:\n",
    "    if c in service_principal_signins.columns:\n",
    "        actual_time_col = c\n",
    "        break\n",
    "\n",
    "if actual_time_col:\n",
    "    cutoff = current_timestamp() - expr(f'INTERVAL {TARGET_HOURS} HOURS')\n",
    "    service_principal_signins = service_principal_signins.filter(col(actual_time_col) >= cutoff)\n",
    "    filtered_count = service_principal_signins.count()\n",
    "    print(f\"‚è±Ô∏è Filtered sign-ins to last {TARGET_HOURS}h using column '{actual_time_col}': {filtered_count:,} rows\")\n",
    "else:\n",
    "    print('‚ö†Ô∏è No recognizable time column found; skipping time window filter')\n",
    "\n",
    "if risky_principals is not None:\n",
    "    print('üß© Risk enrichment table available (AADRiskyServicePrincipals)')\n",
    "else:\n",
    "    print('‚ÑπÔ∏è Risk enrichment table not available (optional)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200aa9c9",
   "metadata": {},
   "source": [
    "## 1. Data Exploration & Profiling\n",
    "\n",
    "Understand schema, volume, temporal coverage, and key distributions for service principal sign-ins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß≠ COMPACT PROFILE (security-focused essentials only)\n",
    "if service_principal_signins is None:\n",
    "    print('‚ùå Required sign-in data missing')\n",
    "else:\n",
    "    total_rows = service_principal_signins.count()\n",
    "    time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "    if time_col:\n",
    "        tb = service_principal_signins.selectExpr(f'min({time_col}) as start', f'max({time_col}) as end').collect()[0]\n",
    "        print(f'üóÇÔ∏è Window: {tb.start} ‚Üí {tb.end}')\n",
    "    print(f'üßÆ Rows: {total_rows:,}')\n",
    "    # Distinct core entities\n",
    "    app_cnt = service_principal_signins.select('AppId').distinct().count() if 'AppId' in service_principal_signins.columns else None\n",
    "    ip_cnt = service_principal_signins.select('IpAddress').distinct().count() if 'IpAddress' in service_principal_signins.columns else None\n",
    "    print('üî¢ Distinct:', end=' ')\n",
    "    if app_cnt is not None: print(f'Apps={app_cnt}', end=' ')\n",
    "    if ip_cnt is not None: print(f'IPs={ip_cnt}', end=' ')\n",
    "    print('')\n",
    "    \n",
    "    if app_cnt and 'AppId' in service_principal_signins.columns:\n",
    "        name_col = 'AppDisplayName' if 'AppDisplayName' in service_principal_signins.columns else None\n",
    "        selected_cols = ([] if name_col is None else [name_col]) + ['AppId']\n",
    "        sample_apps_df = (\n",
    "            service_principal_signins\n",
    "            .select(*selected_cols)\n",
    "            .dropDuplicates()\n",
    "            .orderBy(col(name_col) if name_col else col('AppId'))\n",
    "            .limit(10)\n",
    "        )\n",
    "        samples = sample_apps_df.collect()\n",
    "        if samples:\n",
    "            formatted = []\n",
    "            for row in samples:\n",
    "                app_id = row['AppId'] if 'AppId' in row else None\n",
    "                display = row[name_col] if name_col else None\n",
    "                if display and str(display).strip():\n",
    "                    formatted.append(f\"{display} ({app_id})\" if app_id else str(display))\n",
    "                elif app_id:\n",
    "                    formatted.append(app_id)\n",
    "            if formatted:\n",
    "                extras = max(app_cnt - len(samples), 0)\n",
    "                suffix = f\" ‚Ä¶ +{extras}\" if extras > 0 else ''\n",
    "                print(f\"   ‚Üí Sample apps: {', '.join(formatted)}{suffix}\")\n",
    "    \n",
    "    # Quick null signal for status/app\n",
    "    key_cols = [c for c in ['AppId','AppDisplayName','IpAddress','ResultType','Status'] if c in service_principal_signins.columns]\n",
    "    if key_cols:\n",
    "        null_info = []\n",
    "        for c in key_cols:\n",
    "            n = service_principal_signins.filter(col(c).isNull()).count()\n",
    "            if n > 0:\n",
    "                null_info.append(f\"{c}:{n}\")\n",
    "        if null_info:\n",
    "            print('‚ö†Ô∏è Nulls:', ', '.join(null_info))\n",
    "    # Risk overlap quick stat\n",
    "    if risky_principals is not None:\n",
    "        join_key = None\n",
    "        if 'AppId' in service_principal_signins.columns and 'ServicePrincipalId' in risky_principals.columns:\n",
    "            join_key = ('AppId','ServicePrincipalId')\n",
    "        if join_key:\n",
    "            overlap = service_principal_signins.select(join_key[0]).distinct().join(\n",
    "            risky_principals.select(join_key[1]).distinct(), col(join_key[0])==col(join_key[1]), 'inner'\n",
    "            ).count()\n",
    "            print(f'üß© Risk overlap principals: {overlap}')\n",
    "        else:\n",
    "            print('‚ÑπÔ∏è Risk table loaded but no join key matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà HOURLY & FAILURE RATE\n",
    "if service_principal_signins is not None:\n",
    "    time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "    status_col = next((c for c in ['ResultType','Status','status','ResultType'.lower()] if c in service_principal_signins.columns), None)\n",
    "    if time_col:\n",
    "        base = service_principal_signins.withColumn('HourBucket', date_trunc('hour', col(time_col)))\n",
    "        hourly = base.groupBy('HourBucket').agg(spark_count('*').alias('Events'))\n",
    "        if status_col:\n",
    "            # Assume 0 = success style codes; adapt if textual\n",
    "            success_expr = when(col(status_col).isin(['0',0,'Success','success']), 1).otherwise(0)\n",
    "            fail_expr = when(col(status_col).isin(['0',0,'Success','success']), 0).otherwise(1)\n",
    "            fr = base.withColumn('Success', success_expr).withColumn('Failure', fail_expr) \\\n",
    "                     .groupBy('HourBucket') \\\n",
    "                     .agg(spark_count('*').alias('Events'),\n",
    "                          expr('sum(Failure) as Failures'),\n",
    "                          expr('sum(Success) as Successes')) \\\n",
    "                     .withColumn('FailureRate', col('Failures')/col('Events')) \\\n",
    "                     .orderBy('HourBucket')\n",
    "            pdf = fr.toPandas()\n",
    "            if not pdf.empty:\n",
    "                plt.figure(figsize=(10,4))\n",
    "                sns.lineplot(data=pdf, x='HourBucket', y='Events', label='Events')\n",
    "                ax2 = plt.twinx()\n",
    "                sns.lineplot(data=pdf, x='HourBucket', y='FailureRate', color='red', label='FailureRate', ax=ax2)\n",
    "                plt.title('Hourly Volume & Failure Rate')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                ax2.set_ylabel('Failure Rate')\n",
    "                plt.tight_layout(); plt.show()\n",
    "            else:\n",
    "                print('‚ö†Ô∏è No hourly data')\n",
    "        else:\n",
    "            pdf = hourly.orderBy('HourBucket').toPandas()\n",
    "            if not pdf.empty:\n",
    "                plt.figure(figsize=(10,4))\n",
    "                sns.lineplot(data=pdf, x='HourBucket', y='Events', marker='o')\n",
    "                plt.title('Service Principal Sign-Ins per Hour')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout(); plt.show()\n",
    "            else:\n",
    "                print('‚ö†Ô∏è No hourly data')\n",
    "    else:\n",
    "        print('‚ö†Ô∏è No time column for temporal analysis')\n",
    "else:\n",
    "    print('‚ùå Dataset not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ STATUS, APPLICATION & IP INSIGHTS\n",
    "if service_principal_signins is not None:\n",
    "    status_col = next((c for c in ['ResultType','Status','status'] if c in service_principal_signins.columns), None)\n",
    "    if status_col:\n",
    "        print(f'üîé Using status column: {status_col}')\n",
    "        breakdown = (service_principal_signins\n",
    "                     .groupBy(status_col)\n",
    "                     .agg(spark_count('*').alias('Count'))\n",
    "                     .orderBy(col('Count').desc()))\n",
    "        breakdown.show(10, truncate=False)\n",
    "    # App failure ranking\n",
    "    app_col = 'AppId' if 'AppId' in service_principal_signins.columns else None\n",
    "    if app_col and status_col:\n",
    "        success_expr = when(col(status_col).isin(['0',0,'Success','success']), 1).otherwise(0)\n",
    "        fail_expr = when(col(status_col).isin(['0',0,'Success','success']), 0).otherwise(1)\n",
    "        app_fail = (service_principal_signins\n",
    "                    .withColumn('Fail', fail_expr)\n",
    "                    .groupBy(app_col)\n",
    "                    .agg(spark_count('*').alias('Events'), expr('sum(Fail) as Failures'))\n",
    "                    .withColumn('FailureRate', col('Failures')/col('Events'))\n",
    "                    .filter(col('Events') >= 3)\n",
    "                    .orderBy(col('FailureRate').desc(), col('Events').desc())\n",
    "                    .limit(15))\n",
    "        print('\\nüî• Apps by Failure Rate (>=3 events)')\n",
    "        app_fail.show(truncate=False)\n",
    "    if 'IpAddress' in service_principal_signins.columns:\n",
    "        # Top IPs\n",
    "        top_ips = (service_principal_signins\n",
    "                   .groupBy('IpAddress')\n",
    "                   .agg(spark_count('*').alias('Events'))\n",
    "                   .orderBy(col('Events').desc())\n",
    "                   .limit(10))\n",
    "        print('\\nüåê Top Source IPs:')\n",
    "        top_ips.show(truncate=False)\n",
    "        # New IPs (first seen within this window) only if time column present\n",
    "        time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "        if time_col:\n",
    "            first_seen = (service_principal_signins\n",
    "                          .groupBy('IpAddress')\n",
    "                          .agg(expr(f'min({time_col}) as FirstSeen'), expr(f'max({time_col}) as LastSeen'), spark_count('*').alias('Events'))\n",
    "                          .orderBy('FirstSeen'))\n",
    "            # Heuristic: new if lifespan < 1 hour\n",
    "            new_ips = first_seen.withColumn('LifespanMinutes', (expr('unix_timestamp(LastSeen)') - expr('unix_timestamp(FirstSeen)'))/60.0) \\\n",
    "                                  .filter(col('LifespanMinutes') < 60)\n",
    "            if new_ips.count() > 0:\n",
    "                print('\\n\udd95 Newly Observed IPs (<60m lifespan):')\n",
    "                new_ips.show(10, truncate=False)\n",
    "    # Risk overlay simple metric\n",
    "    if risky_principals is not None and 'ServicePrincipalId' in risky_principals.columns and 'AppId' in service_principal_signins.columns:\n",
    "        risky_join = (service_principal_signins.select('AppId')\n",
    "                      .distinct()\n",
    "                      .join(risky_principals.select('ServicePrincipalId').distinct(), col('AppId')==col('ServicePrincipalId'), 'inner')\n",
    "                      .count())\n",
    "        print(f'üß© Risk-tagged principals in window: {risky_join}')\n",
    "else:\n",
    "    print('‚ùå Dataset not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8a088",
   "metadata": {},
   "source": [
    "## 2. Security Detections & Anomalies\n",
    "\n",
    "Focused heuristics highlighting suspicious service principal behavior. Thresholds are intentionally simple and can be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• BURST & SPIKE DETECTION (Events per Hour > mean+3*std)\n",
    "if service_principal_signins is None:\n",
    "    print('‚ùå No data for detection phase')\n",
    "else:\n",
    "    time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "    if not time_col:\n",
    "        print('‚ö†Ô∏è No time column; skipping burst detection')\n",
    "    else:\n",
    "        hourly = (service_principal_signins\n",
    "                  .withColumn('HourBucket', date_trunc('hour', col(time_col)))\n",
    "                  .groupBy('HourBucket')\n",
    "                  .agg(spark_count('*').alias('Events'))\n",
    "                  .orderBy('HourBucket'))\n",
    "        stats = hourly.agg(avg('Events').alias('mean'), stddev('Events').alias('std')).collect()[0]\n",
    "        mean_v = stats['mean'] or 0\n",
    "        std_v = stats['std'] or 0\n",
    "        threshold = mean_v + (3 * std_v)\n",
    "        spikes = hourly.filter(col('Events') > threshold)\n",
    "        print(f'Mean={mean_v:.2f} Std={std_v:.2f} Threshold={threshold:.2f}')\n",
    "        cnt = spikes.count()\n",
    "        if cnt > 0:\n",
    "            print(f'üö® {cnt} spike hour(s) detected (>{threshold:.2f})')\n",
    "            spikes.show(truncate=False)\n",
    "        else:\n",
    "            print('‚úÖ No abnormal bursts above threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99121a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ RARE / NEW APPLICATION DETECTION (Apps <=2 events or first seen this window)\n",
    "if service_principal_signins is not None and 'AppId' in service_principal_signins.columns:\n",
    "    time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "    usage = (service_principal_signins\n",
    "             .groupBy('AppId')\n",
    "             .agg(spark_count('*').alias('Events')))\n",
    "    rare = usage.filter(col('Events') <= 2)\n",
    "    rc = rare.count()\n",
    "    if rc > 0:\n",
    "        print(f'üïµÔ∏è Rare apps (<=2 events): {rc}')\n",
    "        rare.show(15, truncate=False)\n",
    "    else:\n",
    "        print('‚úÖ No rare low-volume apps')\n",
    "    if time_col:\n",
    "        # Calculate first seen heuristic inside window (all are within window by design but show earliest timestamp)\n",
    "        first_seen = (service_principal_signins\n",
    "                      .groupBy('AppId')\n",
    "                      .agg(expr(f'min({time_col}) as FirstSeen'), spark_count('*').alias('Events'))\n",
    "                      .orderBy('FirstSeen'))\n",
    "        # (Optionally could compare against historical baseline if persisted; here just surface earliest ordering)\n",
    "        print('\\nüïí Earliest observed apps in window (potentially new):')\n",
    "        first_seen.show(10, truncate=False)\n",
    "else:\n",
    "    print('‚ö†Ô∏è AppId column absent; skipping rare app detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ FAILURE ‚Üí SUCCESS PIVOT (Potential credential misuse)\n",
    "if service_principal_signins is not None:\n",
    "    status_col = next((c for c in ['ResultType','Status','status'] if c in service_principal_signins.columns), None)\n",
    "    app_col = 'AppId' if 'AppId' in service_principal_signins.columns else None\n",
    "    time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "    if status_col and app_col and time_col:\n",
    "        labeled = service_principal_signins.select(app_col, time_col, status_col).withColumn(\n",
    "            'IsSuccess', when(col(status_col).isin(['0',0,'Success','success']), 1).otherwise(0)\n",
    "        ).withColumn(\n",
    "            'IsFailure', when(col(status_col).isin(['0',0,'Success','success']), 0).otherwise(1)\n",
    "        )\n",
    "        # Aggregate sequentially at hour level for simplicity\n",
    "        hour_app = labeled.withColumn('HourBucket', date_trunc('hour', col(time_col))) \\\n",
    "                           .groupBy('HourBucket', app_col) \\\n",
    "                           .agg(expr('sum(IsFailure) as Failures'), expr('sum(IsSuccess) as Successes')) \\\n",
    "                           .orderBy('HourBucket')\n",
    "        # Look for pattern: failures in an hour followed by success next hour\n",
    "        from pyspark.sql.window import Window\n",
    "        w = Window.partitionBy(app_col).orderBy('HourBucket')\n",
    "        shifted_success = hour_app.withColumn('NextHourSuccess', expr('lead(Successes,1) over (partition by {0} order by HourBucket)'.format(app_col)))\n",
    "        pivots = shifted_success.filter((col('Failures') > 3) & (col('NextHourSuccess') > 0))\n",
    "        pc = pivots.count()\n",
    "        if pc > 0:\n",
    "            print(f'‚ö†Ô∏è Potential credential pivot events: {pc}')\n",
    "            pivots.select(app_col, 'HourBucket', 'Failures', 'NextHourSuccess').show(15, truncate=False)\n",
    "        else:\n",
    "            print('‚úÖ No failure‚Üísuccess pivot patterns detected (threshold Failures>3 then next-hour success)')\n",
    "    else:\n",
    "        print('‚ö†Ô∏è Missing columns for pivot detection')\n",
    "else:\n",
    "    print('‚ùå Dataset not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåç UNUSUAL / NEW IP ADDRESS ACTIVITY\n",
    "if service_principal_signins is not None and 'IpAddress' in service_principal_signins.columns:\n",
    "    time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "    ip_usage = (service_principal_signins\n",
    "                .groupBy('IpAddress')\n",
    "                .agg(spark_count('*').alias('Events')))\n",
    "    low_freq = ip_usage.filter(col('Events') == 1)\n",
    "    lf_count = low_freq.count()\n",
    "    if lf_count > 0:\n",
    "        print(f'üïµÔ∏è Single-use IPs: {lf_count}')\n",
    "        low_freq.show(10, truncate=False)\n",
    "    else:\n",
    "        print('‚úÖ No single-use IPs')\n",
    "    if time_col:\n",
    "        first_seen = (service_principal_signins\n",
    "                      .groupBy('IpAddress')\n",
    "                      .agg(expr(f'min({time_col}) as FirstSeen'), expr(f'max({time_col}) as LastSeen'), spark_count('*').alias('Events'))\n",
    "                      .orderBy('FirstSeen'))\n",
    "        new_short = first_seen.withColumn('SpanMinutes', (expr('unix_timestamp(LastSeen)') - expr('unix_timestamp(FirstSeen)'))/60.0) \\\n",
    "                               .filter(col('SpanMinutes') < 15)\n",
    "        ns_count = new_short.count()\n",
    "        if ns_count > 0:\n",
    "            print(f'üÜï Very short-lived IP activity (<15m span): {ns_count}')\n",
    "            new_short.show(10, truncate=False)\n",
    "else:\n",
    "    print('‚ö†Ô∏è IP column absent; skipping IP anomaly logic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåô OFF-HOURS ACTIVITY (00:00‚Äì05:00 local bucket)\n",
    "if service_principal_signins is not None:\n",
    "    time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "    if time_col:\n",
    "        off_hours = (service_principal_signins\n",
    "                     .withColumn('Hour', hour(col(time_col)))\n",
    "                     .filter(col('Hour').between(0,5)))\n",
    "        cnt = off_hours.count()\n",
    "        if cnt > 0:\n",
    "            print(f'üåô Off-hours events (00‚Äì05): {cnt}')\n",
    "            # Top apps active off-hours\n",
    "            if 'AppId' in off_hours.columns:\n",
    "                top_off = (off_hours.groupBy('AppId')\n",
    "                           .agg(spark_count('*').alias('Events'))\n",
    "                           .orderBy(col('Events').desc())\n",
    "                           .limit(10))\n",
    "                print('\\nTop off-hours apps:')\n",
    "                top_off.show(truncate=False)\n",
    "        else:\n",
    "            print('‚úÖ No off-hours activity detected')\n",
    "    else:\n",
    "        print('‚ö†Ô∏è No time column; skipping off-hours analysis')\n",
    "else:\n",
    "    print('‚ùå Dataset not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ôªÔ∏è PERSISTENCE PATTERN (Regular hourly presence > N hours, low variance)\n",
    "if service_principal_signins is not None:\n",
    "    time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "    app_col = 'AppId' if 'AppId' in service_principal_signins.columns else None\n",
    "    if time_col and app_col:\n",
    "        hour_app = (service_principal_signins\n",
    "                    .withColumn('HourBucket', date_trunc('hour', col(time_col)))\n",
    "                    .groupBy('HourBucket', app_col)\n",
    "                    .agg(spark_count('*').alias('Events')))\n",
    "        # Count active hours and compute variance of events per app\n",
    "        from pyspark.sql.window import Window\n",
    "        app_stats = (hour_app.groupBy(app_col)\n",
    "                     .agg(\n",
    "                         spark_count('*').alias('ActiveHours'),\n",
    "                         expr('avg(Events) as MeanPerHour'),\n",
    "                         expr('stddev(Events) as StdPerHour')\n",
    "                     )\n",
    "                     .withColumn('StdPerHour', when(col('StdPerHour').isNull(), lit(0)).otherwise(col('StdPerHour'))))\n",
    "        # Persistence heuristic: active >= 8 hours AND low variance (std <= 0.5 * mean)\n",
    "        persistent = app_stats.filter((col('ActiveHours') >= 8) & (col('StdPerHour') <= (col('MeanPerHour') * 0.5)))\n",
    "        pc = persistent.count()\n",
    "        if pc > 0:\n",
    "            print(f'‚ôªÔ∏è Potential persistence (scheduled/automated) apps: {pc}')\n",
    "            persistent.orderBy(col('ActiveHours').desc()).show(15, truncate=False)\n",
    "        else:\n",
    "            print('‚úÖ No persistence patterns meeting heuristic (>=8 active hours & low variance)')\n",
    "    else:\n",
    "        print('‚ö†Ô∏è Missing columns for persistence analysis')\n",
    "else:\n",
    "    print('‚ùå Dataset not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17eb6c",
   "metadata": {},
   "source": [
    "## 3. Risk Enrichment (If Available)\n",
    "\n",
    "Correlates observed service principals with entries in `AADRiskyServicePrincipals` and computes a simple priority score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0117e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© RISK ENRICHMENT JOIN & SCORING\n",
    "if risky_principals is None:\n",
    "    print('‚ÑπÔ∏è Risk dataset not loaded; skipping enrichment')\n",
    "else:\n",
    "    # Identify join keys\n",
    "    join_key = None\n",
    "    if 'AppId' in service_principal_signins.columns and 'ServicePrincipalId' in risky_principals.columns:\n",
    "        join_key = ('AppId','ServicePrincipalId')\n",
    "    elif 'ServicePrincipalId' in service_principal_signins.columns and 'ServicePrincipalId' in risky_principals.columns:\n",
    "        join_key = ('ServicePrincipalId','ServicePrincipalId')\n",
    "\n",
    "    if not join_key:\n",
    "        print('‚ö†Ô∏è No compatible join key between sign-ins and risk table')\n",
    "    else:\n",
    "        left_key, right_key = join_key\n",
    "        # Compactify risk table columns for robustness\n",
    "        risk_cols = [c for c in risky_principals.columns if c.lower() in (\n",
    "            'serviceprincipalid','risklevel','riskscore','riskstate','detectionid','riskdetections','riskdetail'\n",
    "        )]\n",
    "        risk_view = risky_principals.select(*risk_cols).dropDuplicates([right_key])\n",
    "        enriched = service_principal_signins.join(risk_view, col(left_key)==col(right_key), 'left')\n",
    "\n",
    "        # Simple scoring heuristic\n",
    "        def risk_case(colname):\n",
    "            return (\n",
    "                when(col(colname) == 'high', 3)\n",
    "                .when(col(colname) == 'medium', 2)\n",
    "                .when(col(colname) == 'low', 1)\n",
    "                .otherwise(0)\n",
    "            )\n",
    "        score_col = None\n",
    "        if 'riskLevel' in risk_view.columns:\n",
    "            score_col = risk_case('riskLevel')\n",
    "        elif 'risklevel' in risk_view.columns:\n",
    "            score_col = risk_case('risklevel')\n",
    "\n",
    "        if score_col is not None:\n",
    "            enriched = enriched.withColumn('RiskScore', score_col)\n",
    "        else:\n",
    "            enriched = enriched.withColumn('RiskScore', lit(0))\n",
    "\n",
    "        # Aggregate per principal\n",
    "        principal_col = left_key\n",
    "        agg = (enriched.groupBy(principal_col)\n",
    "               .agg(\n",
    "                    spark_count('*').alias('Events'),\n",
    "                    expr('max(RiskScore) as MaxRiskScore'),\n",
    "                    expr('sum(RiskScore) as TotalRiskScore')\n",
    "                )\n",
    "               .withColumn('PriorityScore', col('Events')*0.4 + col('MaxRiskScore')*2 + col('TotalRiskScore')*0.6)\n",
    "               .orderBy(col('PriorityScore').desc()))\n",
    "\n",
    "        print('üî• PRIORITIZED PRINCIPALS (Top 15):')\n",
    "        agg.limit(15).show(truncate=False)\n",
    "\n",
    "        # Persist small result for later summary\n",
    "        agg_cached = agg.cache()\n",
    "        agg_cached.count()  # trigger\n",
    "        globals()['risk_enriched_principals'] = agg_cached\n",
    "        print('‚úÖ Risk enrichment complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e275ee",
   "metadata": {},
   "source": [
    "## 4. Executive Summary & Recommended Actions\n",
    "\n",
    "Quick situational snapshot suitable for incident review or daily security brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1effad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã EXECUTIVE SUMMARY\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "summary = {}\n",
    "\n",
    "if 'service_principal_signins' in globals() and service_principal_signins is not None:\n",
    "    total = service_principal_signins.count()\n",
    "    summary['total_events'] = total\n",
    "    # Off-hours proportion\n",
    "    time_col = next((c for c in ['CreatedDateTime','TimeGenerated','Timestamp'] if c in service_principal_signins.columns), None)\n",
    "    if time_col:\n",
    "        off_hours = service_principal_signins.withColumn('Hour', hour(col(time_col))).filter(col('Hour').between(0,5)).count()\n",
    "        summary['off_hours_events'] = off_hours\n",
    "        summary['off_hours_pct'] = (off_hours/total*100) if total else 0\n",
    "    # Distinct apps\n",
    "    if 'AppId' in service_principal_signins.columns:\n",
    "        summary['distinct_apps'] = service_principal_signins.select('AppId').distinct().count()\n",
    "else:\n",
    "    print('‚ö†Ô∏è Sign-in dataset missing; summary limited')\n",
    "\n",
    "# Risk enrichment snapshot\n",
    "if 'risk_enriched_principals' in globals():\n",
    "    rep = risk_enriched_principals\n",
    "    total_principals = rep.count()\n",
    "    high_prior = rep.orderBy(col('PriorityScore').desc()).limit(5)\n",
    "    summary['risk_tracked_principals'] = total_principals\n",
    "else:\n",
    "    high_prior = None\n",
    "\n",
    "print('=== SERVICE PRINCIPAL ACTIVITY SUMMARY ===')\n",
    "for k,v in summary.items():\n",
    "    print(f'{k}: {v}')\n",
    "\n",
    "if high_prior is not None:\n",
    "    print('\\nTop Priority Principals:')\n",
    "    high_prior.show(truncate=False)\n",
    "\n",
    "# Recommended actions heuristics\n",
    "actions = []\n",
    "if summary.get('off_hours_pct',0) > 30:\n",
    "    actions.append('Investigate high off-hours activity for automation drift or misuse')\n",
    "if high_prior is not None and high_prior.count() > 0:\n",
    "    actions.append('Review top priority principals for least privilege & rotation needs')\n",
    "if not actions:\n",
    "    actions.append('Maintain monitoring; no immediate high-risk signals')\n",
    "\n",
    "print('\\n=== RECOMMENDED ACTIONS ===')\n",
    "for a in actions:\n",
    "    print('- ' + a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "large pool (16 vCores) [06_ServicePrincipal_SignIn_Analysis]",
   "language": "Python",
   "name": "MSGLarge"
  },
  "language_info": {
   "codemirror_mode": "ipython",
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
